% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{book}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage[]{natbib}
\bibliographystyle{apalike}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Quantitive Methods 2, ZHAW},
  pdfauthor={Jürgen Degenfellner},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Quantitive Methods 2, ZHAW}
\author{Jürgen Degenfellner}
\date{2025-01-14}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Introduction}\label{intro0}

This script is a continuation of
\href{https://jdegenfellner.github.io/Script_QM1_ZHAW/}{the first one} for Quantitative Methods 1
at ZHAW.

In the first part, we learned about the basics of probability theory,
descriptive statistics, Bayesian statistics, and hypothesis testing.

In this script, we will dive into the basics of statistical modeling -
a world of aesthetic wonder and surprises.

This script is a work in progress and will be updated as we go along.

Please feel free to send me suggestions for improvements or corrections.

As the first one, this \textbf{should be a collaborative effort} and will (hopefully)
never be finished as our insight grows over time.

For the working setup with R, please
see \href{https://jdegenfellner.github.io/Script_QM1_ZHAW/index.html\#section}{this}
and the following sections in the first script.

\section{Books we will heavily borrow from are:}\label{books-we-will-heavily-borrow-from-are}

\begin{itemize}
\tightlist
\item
  (Free) \href{https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf}{Statistical Rethinking}, YouTube-Playlist: \href{https://youtu.be/FdnMWdICdRs?si=q2py5QVY_L299hEa}{Statistical Rethinking 2023}
\item
  (Free) \href{https://vdoc.pub/documents/understanding-regression-analysis-a-conditional-distribution-approach-84oqjr8sqva0}{Understanding Regression Analysis: A Conditional Distribution Approach}
\item
  \href{http://www.stat.columbia.edu/~gelman/arm/}{Data Analysis Using Regression and Multilevel/Hierarchical Models}
\item
  (Free) \href{https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf}{Doing Bayesian Data Analysis}
\end{itemize}

\chapter{Introduction}\label{intro}

\section{What is statistical modeling and what do we need this for?}\label{what-is-statistical-modeling-and-what-do-we-need-this-for}

Typically, one simplifies the complex reality (and loses information) in order to make it better
understandable, mathematically treatable and to make predictions.

Underlying our models, there are theories which should be \href{https://en.wikipedia.org/wiki/Falsifiability}{falsifiable}
and testable.
For instance, I would be really surprised if I pull up my multimeter and measure the voltage (V) and
electric current (I) at a resistence (R) in a circuit and find that \href{https://en.wikipedia.org/wiki/Ohm\%27s_law}{Ohm's law} \(V = IR\) is not true.
This \href{https://en.wikipedia.org/wiki/Scientific_law}{\textbf{law}}
can be tested over and over again and if one would find a single valid counterexample,
the law would be falsified. It is also true that the law is probably not 100\% accularate,
but an extremely good approximation of reality. Real-world measurements carry
measurement errors and when plotting the data, one would see that the data points
might not lie exactly on a straight line. This is not a problem.

A \href{https://en.wikipedia.org/wiki/Statistical_model}{statistical model}
is a mathematical framework that represents the
relationships between variables, helping us understand, infer, and
predict patterns in data. It acts as a bridge between observed data
and the real-world processes that generated them. In health research,
where variability and uncertainty are inherent, statistical models are
valuable tools for making sense of complex phenomena.
You can watch \href{https://www.youtube.com/watch?v=3d5ivs_8amQ&ab_channel=VeryNormal}{this} as short intro.

Depending on the task at hand, we would use different models.
In any case, logical reasoning and critical thinking comes first,
then comes the model. \textbf{It makes no sense to estimate statistical models just for the sake of it}.

\textbf{All models are wrong, but some are useful}.
Or to quote \href{https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949}{George Box}:

\begin{quote}
``Since all models are wrong the scientist cannot obtain
a `correct' one by excessive elaboration. On the contrary
following William of Occam he should seek an economical
description of natural phenomena. Just as the ability to
devise simple but evocative models is the signature of the
great scientist so overelaboration and overparameterization is often the mark of mediocrity.''
\end{quote}

In my opinion, statistical model is an art form: difficult and beautiful.

\textbf{One goal of this course} is to improve interpretation and limitations of statistical models.
They are not magical turning data into truth. Firstly ,the rule gargabe in, garbage out (GABA) applies.
Secondly, statistical models are based on data and their variability and have intrinsict limitations
one cannot overcome even with the most sophisticated models. This is expressed for instance
in the so-called \href{https://en.wikipedia.org/wiki/Bias\%E2\%80\%93variance_tradeoff}{bias-variance trade-off}.
You can't have it all.

\subsection{Explanatory vs.~Predictive Models}\label{explanatory-vs.-predictive-models}

I can recommend reading \href{https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full}{this}
article by Shmueli et al.~(2010) on this topic.

Statistical models serve different purposes depending on the research question. Two primary goals are \textbf{explanation}
and \textbf{prediction}, and each requires a different approach:

\textbf{Explanatory Models} focus on understanding causal relationships.
These models aim to uncover mechanisms and answer \textbf{``why''}
questions. For example:

\begin{itemize}
\tightlist
\item
  Does smoking increase the risk of lung cancer? \textbf{Yes}. (If you want to see what a large effect-size looks like, check out \href{https://bmjopen.bmj.com/content/bmjopen/8/10/e021611.full.pdf}{this study}.)
\item
  How large is the ``effect'' of smoking on lung cancer? \textbf{Large}.
\item
  Does pain education and graded sensorimotor relearning improve disability (a question we ask in
  our \href{https://data.snf.ch/grants/grant/220585}{Resolve Swiss project})?
\end{itemize}

Explanatory models are \textbf{theory-driven}, designed to test hypotheses. Here, one wants to understand the underlying
mechanisms and the relationships between variables and hence often uses (parsimonious) models that are more interpretable,
like linear regression.

\textbf{Predictive Models} prioritize forecasting future outcomes based on patterns in the data.
These models aim to answer \textbf{``what will happen?''} For instance:

\begin{itemize}
\tightlist
\item
  \href{https://www.tandfonline.com/doi/abs/10.1080/03091902.2020.1822940}{Gait analysis} using Machine Learning (ML)?
\item
  \href{https://jamanetwork.com/journals/jamadermatology/fullarticle/2756346}{Skin cancer detection} using neural networks?
\end{itemize}

Predictive models are \textbf{data-driven}, often using complex algorithms to achieve high accuracy.
Their success is measured using metrics like \href{https://computersciencewiki.org/index.php/Root-mean-square_error_(RMSE)}{Root Means Square Error}
(RMSE), \href{https://en.wikipedia.org/wiki/Receiver_operating_characteristic\#:~:text=The\%20area\%20under\%20the\%20curve,ranks\%20higher\%20than\%20'negative'}{Area Unter the Curve}
(AUC), or \textbf{prediction error on new, unseen data}.
Any amount of model complexity is allowed. One could for instance estimate a
\href{https://en.wikipedia.org/wiki/Neural_network_(machine_learning)}{neural network} (``just'' another statistical model)
with many hidden layers and neurons in order to improve prediction quality. Interpretability of the model weights is not a priority here.

While explanatory and predictive goals often complement each other,
their differences highlight the importance of clearly defining the purpose
of your analysis. In applied health research, explanatory models help identify
causal mechanisms, while predictive models can guide real-world decisions by
providing actionable forecasts. Together, they enhance both our understanding
of phenomena and our ability to make informed decisions in complex environments.

\subsection{Individual vs.~Population Prediction}\label{individual-vs.-population-prediction}

Another important distinction is between \textbf{individual vs.~population} prediction.
In the smoking example above, we can be very sure about the mean effects that smoking has on lung cancer.
On an individual level, it is \href{https://www.liebertpub.com/doi/10.1089/rej.2019.2298}{harder to predict the outcome}.
Nevertheless, individual predictions will be (notably) better than random guessing. We will discuss this in greater detail.

\subsection{Practical Use of Statistical Models}\label{practical-use-of-statistical-models}

In my optinion, we should never be afraid to test our statistical models (as honestly as possible) against reality.
We could for instance ask ourselves:

\begin{itemize}
\item
  ``How much better does this model classify than the arithmetic mean?
  (i.e., the linear model with just an intercept)''
\item
  ``How much better does this model classify than random guessing?''
\item
  Is it worth the effort to collect data and estimate this model by using hundreds of hours of our time?
\end{itemize}

In some cases, these questions can be answered straightforwardly.

\begin{itemize}
\item
  In advertising (Google, Facebook, \ldots), a couple of percentage points in prediction quality might make a differences of millions
  of dollars in revenue offsetting the statistitians salary.
\item
  Increased forecasts of a few percentage points in the stock market or just being \href{https://en.wikipedia.org/wiki/Jim_Simons}{slightly better}
  than the average, will make you faboulously rich.
\item
  Increased cancer forecasting might save lives, money and pain and is not only measured in money.
\end{itemize}

\subsection{Start at the beginning}\label{start-at-the-beginning}

What do we actually want to do in general? Very broadly speaking we want to:
\textbf{describe} the association of variables to each other that carry variability.
Hence, the relationship is not deterministic like \[y = 2x + 3\] but rather we need
to ``loosen up'' the relationship to account for variability.
So, the values \(2\) and \(3\) are not fixed but aflicted with uncertainty.
Depending on your philosophical view, you might say you want to find
the ``true'' but unknown relationship between variables.
This is what we do in simulation studies all the time: We know the true relationship,
simulate data with variability and then try to estimate the true relationship we assumed in the first place.
For some practical applications, we can get a really nice and complete answer to our question
(for instance sample size for proportions).

So we are looking for a function \(f\) such that

\[ Y = f(X) \]

where \(Y\) is the ``outcome'', \(X\) are the ``predictors''. Since this relationship is not deterministic, we have to account for variability.
\(X\) can be a single Variable \(x\) or many variables \(x_1, x_2, \ldots, x_p\).

Elaborate on X, Y and F, independent, depenent\ldots\ldots.

The simplest statistical model would be the mean model. Let's use the example in
\href{https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf}{Statistical rethinking}
starting on page 78.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rethinking)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Howell1"}\NormalTok{)}
\NormalTok{d }\OtherTok{\textless{}{-}}\NormalTok{ Howell1}
\FunctionTok{str}\NormalTok{(d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    544 obs. of  4 variables:
##  $ height: num  152 140 137 157 145 ...
##  $ weight: num  47.8 36.5 31.9 53 41.3 ...
##  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...
##  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...
\end{verbatim}

\chapter{Literature}\label{literature}

Here is a review of existing methods.

\chapter{Methods}\label{methods}

We describe our methods in this chapter.

Math can be added in body using usual syntax like this

\section{math example}\label{math-example}

\(p\) is unknown but expected to be around 1/3. Standard error will be approximated

\[
SE = \sqrt{\frac{p(1-p)}{n}} \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
\]

You can also use math in footnotes like this\footnote{where we mention \(p = \frac{a}{b}\)}.

We will approximate standard error to 0.027\footnote{\(p\) is unknown but expected to be around 1/3. Standard error will be approximated

  \[
  SE = \sqrt{\frac{p(1-p)}{n}} \approx \sqrt{\frac{1/3 (1 - 1/3)} {300}} = 0.027
  \]}

\chapter{Applications}\label{applications}

Some \emph{significant} applications are demonstrated in this chapter.

\section{Example one}\label{example-one}

\section{Example two}\label{example-two}

\chapter{Final Words}\label{final-words}

We have finished a nice book.

\bibliography{book.bib,packages.bib}

\end{document}
