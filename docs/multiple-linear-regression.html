<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Multiple Linear Regression | Quantitative Methods 2, ZHAW" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jürgen Degenfellner" />


<meta name="date" content="2025-02-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="simple-linear-regression.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#books-we-will-heavily-borrow-from-are"><i class="fa fa-check"></i><b>1.1</b> Books we will heavily borrow from are:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#if-you-need-a-good-reason-to-buy-great-books"><i class="fa fa-check"></i><b>1.2</b> If you need a good reason to buy great books…</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-statistical-modeling-and-what-do-we-need-this-for"><i class="fa fa-check"></i><b>2.1</b> What is statistical modeling and what do we need this for?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#explanatory-vs.-predictive-models"><i class="fa fa-check"></i><b>2.1.1</b> Explanatory vs. Predictive Models</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#individual-vs.-population-prediction"><i class="fa fa-check"></i><b>2.1.2</b> Individual vs. Population Prediction</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#practical-use-of-statistical-models"><i class="fa fa-check"></i><b>2.1.3</b> Practical Use of Statistical Models</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#start-at-the-beginning"><i class="fa fa-check"></i><b>2.1.4</b> Start at the beginning</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#a-simple-model-for-adult-body-heights-in-the-bayesian-framework"><i class="fa fa-check"></i><b>2.2</b> A (simple) model for adult body heights in the Bayesian framework</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#classical-approach-for-the-simplest-model"><i class="fa fa-check"></i><b>2.3</b> Classical approach for the simplest model</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#exercise1_Intro"><i class="fa fa-check"></i><b>2.4.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#exercise2_Intro"><i class="fa fa-check"></i><b>2.4.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#exercise3_Intro"><i class="fa fa-check"></i><b>2.4.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#exercise4_Intro"><i class="fa fa-check"></i><b>2.4.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro.html"><a href="intro.html#exercise5_Intro"><i class="fa fa-check"></i><b>2.4.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="2.4.6" data-path="intro.html"><a href="intro.html#exercise6_Intro"><i class="fa fa-check"></i><b>2.4.6</b> [M] Exercise 6</a></li>
<li class="chapter" data-level="2.4.7" data-path="intro.html"><a href="intro.html#exercise7_Intro"><i class="fa fa-check"></i><b>2.4.7</b> [H] Exercise 7</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#addendum"><i class="fa fa-check"></i><b>2.5</b> Addendum</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#bivariate_normal"><i class="fa fa-check"></i><b>2.5.1</b> The bivariate normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple_lin_reg_bayes"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition"><i class="fa fa-check"></i><b>3.1.1</b> Model definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>3.1.2</b> Priors</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit-model"><i class="fa fa-check"></i><b>3.1.3</b> Fit model</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#result"><i class="fa fa-check"></i><b>3.1.4</b> Result</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#credible-bands"><i class="fa fa-check"></i><b>3.1.5</b> Credible bands</a></li>
<li class="chapter" data-level="3.1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition-1"><i class="fa fa-check"></i><b>3.2.1</b> Model definition</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic"><i class="fa fa-check"></i><b>3.2.2</b> Fit the model</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence_intervals_frequentist"><i class="fa fa-check"></i><b>3.2.3</b> Confidence Intervals of coefficients (frequentist)</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analysis_of_variance"><i class="fa fa-check"></i><b>3.2.4</b> ANOVA (Analysis of Variance)</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r2---coefficient-of-determination"><i class="fa fa-check"></i><b>3.2.5</b> <span class="math inline">\(R^2\)</span> - Coefficient of Determination</a></li>
<li class="chapter" data-level="3.2.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#check-regression-assumptions"><i class="fa fa-check"></i><b>3.2.6</b> Check regression assumptions</a></li>
<li class="chapter" data-level="3.2.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bootstrap-fit"><i class="fa fa-check"></i><b>3.2.7</b> Bootstrap fit</a></li>
<li class="chapter" data-level="3.2.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-towards-the-mean"><i class="fa fa-check"></i><b>3.2.8</b> Regression towards the mean</a></li>
<li class="chapter" data-level="3.2.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#random-x-vs.-fixed-x"><i class="fa fa-check"></i><b>3.2.9</b> Random X vs. fixed X</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise1_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="3.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise2_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="3.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise3_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="3.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise4_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="3.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise5_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="3.3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise6_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.6</b> [H] Exercise 6</a></li>
<li class="chapter" data-level="3.3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise7_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.7</b> [M] Exercise 7</a></li>
<li class="chapter" data-level="3.3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise8_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.8</b> [E] Exercise 8</a></li>
<li class="chapter" data-level="3.3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise9_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.9</b> [M] Exercise 9</a></li>
<li class="chapter" data-level="3.3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise10_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.10</b> [M] Exercise 10</a></li>
<li class="chapter" data-level="3.3.11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise11_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.11</b> [E] Exercise 11</a></li>
<li class="chapter" data-level="3.3.12" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise12_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.12</b> [M] Exercise 12</a></li>
<li class="chapter" data-level="3.3.13" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise13_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.13</b> [M] Exercise 13</a></li>
<li class="chapter" data-level="3.3.14" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise14_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.14</b> [M] Exercise 14</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-bayesian-framework"><i class="fa fa-check"></i><b>4.1</b> Linear Regression with 2 predictors in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#meaning-of-linear"><i class="fa fa-check"></i><b>4.1.1</b> Meaning of “linear”</a></li>
<li class="chapter" data-level="4.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor"><i class="fa fa-check"></i><b>4.1.2</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.1.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_bayes"><i class="fa fa-check"></i><b>4.1.3</b> Adding another predictor to the model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-frequentist-framework"><i class="fa fa-check"></i><b>4.2</b> Linear regression with 2 predictors in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor_freq"><i class="fa fa-check"></i><b>4.2.1</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_freq"><i class="fa fa-check"></i><b>4.2.2</b> Adding another predictor to the model</a></li>
<li class="chapter" data-level="4.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_term"><i class="fa fa-check"></i><b>4.2.3</b> Interaction Term <span class="math inline">\(X_1 \times X_2\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_plot"><i class="fa fa-check"></i><b>4.2.4</b> Using an interaction plot to see a potential interaction</a></li>
<li class="chapter" data-level="4.2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simpsons_paradox"><i class="fa fa-check"></i><b>4.2.5</b> Simpsons Paradox</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#throwing_variables"><i class="fa fa-check"></i><b>4.3</b> What happens when you just throw variables into multiple regression?</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#pipe"><i class="fa fa-check"></i><b>4.3.1</b> Pipe</a></li>
<li class="chapter" data-level="4.3.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#fork"><i class="fa fa-check"></i><b>4.3.2</b> Fork</a></li>
<li class="chapter" data-level="4.3.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#collider"><i class="fa fa-check"></i><b>4.3.3</b> Collider</a></li>
<li class="chapter" data-level="4.3.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#multicollinearity"><i class="fa fa-check"></i><b>4.3.4</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#more-than-2-predictors"><i class="fa fa-check"></i><b>4.4</b> More than 2 predictors</a></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>4.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise1_multiple_regression"><i class="fa fa-check"></i><b>4.5.1</b> [M] Exercise 1</a></li>
<li class="chapter" data-level="4.5.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise2_multiple_regression"><i class="fa fa-check"></i><b>4.5.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="4.5.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise3_multiple_regression"><i class="fa fa-check"></i><b>4.5.3</b> [H] Exercise 3</a></li>
<li class="chapter" data-level="4.5.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise4_multiple_regression"><i class="fa fa-check"></i><b>4.5.4</b> [E] Exercise 4</a></li>
<li class="chapter" data-level="4.5.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise5_multiple_regression"><i class="fa fa-check"></i><b>4.5.5</b> [E] Exercise 5</a></li>
<li class="chapter" data-level="4.5.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise6_multiple_regression"><i class="fa fa-check"></i><b>4.5.6</b> [M] Exercise 6</a></li>
<li class="chapter" data-level="4.5.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise7_multiple_regression"><i class="fa fa-check"></i><b>4.5.7</b> [E] Exercise 7</a></li>
<li class="chapter" data-level="4.5.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise8_multiple_regression"><i class="fa fa-check"></i><b>4.5.8</b> [M] Exercise 8</a></li>
<li class="chapter" data-level="4.5.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise9_multiple_regression"><i class="fa fa-check"></i><b>4.5.9</b> [M] Exercise 9</a></li>
<li class="chapter" data-level="4.5.10" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise10_multiple_regression"><i class="fa fa-check"></i><b>4.5.10</b> [E] Exercise 10</a></li>
<li class="chapter" data-level="4.5.11" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise11_multiple_regression"><i class="fa fa-check"></i><b>4.5.11</b> [E] Exercise 11</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#todos"><i class="fa fa-check"></i><b>4.6</b> TODOS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods 2, ZHAW</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Multiple Linear Regression<a href="multiple-linear-regression.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>So far, we have dealt with the simple mean model and the model with one predictor
in the Bayesian and Frequentist framework.
We will now add another predictor and subsequently an interaction term to the model.
Finally, we will add more than two predictors to the model.</p>
<p>If you feel confused at any point: As Richard McElreath repeatedly says:
This is normal, it means you are paying attention. I also refer to the
great <a href="https://www.youtube.com/watch?v=lytxafTXg6c&amp;ab_channel=sdfhsfh">Richard Feynman</a>.</p>
<div id="linear-regression-with-2-predictors-in-the-bayesian-framework" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Linear Regression with 2 predictors in the Bayesian Framework<a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-bayesian-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="meaning-of-linear" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Meaning of “linear”<a href="multiple-linear-regression.html#meaning-of-linear" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What is a linear model? The term “linear” refers to the relationship of the predictors
with the dependent variable (or outcome). The following model is also linear:</p>
<p><span class="math display">\[height_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2\]</span></p>
<p>The model is linear in the parameters <span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span> but not in the predictors <span class="math inline">\(x_i\)</span>.
The term <span class="math inline">\(x_i^2\)</span> is ok, since the heights are just sums of multiples of the predictors (which can be nonlinear).
This model is not a linear model anymore:</p>
<p><span class="math display">\[height_i = \beta_0 + \beta_1 x_i + e^{\beta_2 x_i^2}\]</span></p>
<p><span class="math inline">\(\beta_2\)</span> is now is the exponent of <span class="math inline">\(e\)</span>. It would also not be linear,
if the coefficients are in a square root or in the denominator of a fraction,
or in a sine or in a logarithm. You get the idea.</p>
<p>Here is an easy way to check if the model is linear: If I change the predictor-value
(i.e.,the value of <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x_i^2\)</span> or whatever your predictor is) by one unit,
the change in the (expected value of the) dependent variable is the coefficient in front of
the predictor (<span class="math inline">\(\beta_i\)</span>).</p>
</div>
<div id="adding_transformed_predictor" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Adding a transformed predictor to the model<a href="multiple-linear-regression.html#adding_transformed_predictor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Around 4.5. in the book <a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">Statistical Rethinking</a>
there is are lineare regression using a quadratic term for weight.
It is a principle, called the “<strong>variable inclusion principle</strong>”, that we always include the lower order terms when fitting a model
with higher order terms. See <a href="https://vdoc.pub/documents/understanding-regression-analysis-a-conditional-distribution-approach-84oqjr8sqva0">Westfall</a>,
p. 213. If we do not include the lower order terms, the coefficient does not measure what
we want it to meausure (curvature in our case). For instance, if we want to model a quadratic relationship (parabola) between
weight and height, we also have to include the linear term for weight (<span class="math inline">\(x_i\)</span>).
Since we do not assume the relationship between weight and height to be linear but
quadratic (which is a polynomial of degree 2), we call this a
<a href="https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=In%20statistics%2C%20polynomial%20regression%20is,nth%20degree%20polynomial%20in%20x.">polynomial regression</a>.
This <a href="https://www.youtube.com/watch?v=QptI-vDle8Y&amp;ab_channel=MikeXCohen">video</a> could be instructive.
One has to be careful with fitting polynomials to data points since the regression coefficients
can become quite large. Using a polynomial of high degree implies to have a lot of parameters
to estimate. Increasing the degree of the polynomial increases <span class="math inline">\(R^2\)</span> but also the risk of overfitting.
(see Statistical Rethinking p. 200). So this is - of course - not the final solution to regression problems.</p>
<p>This time, lets look at the whole age range of data from the !Kung San people.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="multiple-linear-regression.html#cb107-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb107-2"><a href="multiple-linear-regression.html#cb107-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb107-3"><a href="multiple-linear-regression.html#cb107-3" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb107-4"><a href="multiple-linear-regression.html#cb107-4" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb107-5"><a href="multiple-linear-regression.html#cb107-5" tabindex="-1"></a>d <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> weight, <span class="at">y =</span> height)) <span class="sc">+</span></span>
<span id="cb107-6"><a href="multiple-linear-regression.html#cb107-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb107-7"><a href="multiple-linear-regression.html#cb107-7" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb107-8"><a href="multiple-linear-regression.html#cb107-8" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<p>It would not be a good idea to fit a linear trend through this data,
because we would not caupture the relationship adequately.
The red line is a <a href="https://en.wikipedia.org/wiki/Local_regression">loess smothing</a> line
which is often used to capture non-linear relationships.
The blue line is the usual line from classic linear regression (from the previous chapter).
Which one describes the data more accurately?
In this case it is obvious, a non-linear relationship is present and it might be a good idea
to model it. Modeling the relationship with a linear trend leads to bad residuals with structure.
We will demonstrate this in the freuqentist setting.
Unfortunately, in more complex settings, with more predictors, it is not always so easy to see.</p>
<p>This time, we use the mean for the prior from the book (<span class="math inline">\(178 cm\)</span>).
The model equations are (see <a href="multiple-linear-regression.html#exercise2_multiple_regression">exercise 2</a>):</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp;=&amp; \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha &amp;\sim&amp; \text{Normal}(178, 20) \\
\beta_1 &amp;\sim&amp; \text{Log-Normal}(0, 1) \\
\beta_2 &amp;\sim&amp; \text{Normal}(0, 1) \\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>The prior for <span class="math inline">\(\beta_1\)</span> is log-normal, because we can reasonably assume
the the overall linear trend is positive. The prior for <span class="math inline">\(\beta_2\)</span> is normal, because
we are not so sure. If we thought back to our school days to the topic of
“curve discussion” or parabolas, we could probably also assume that <span class="math inline">\(\beta_2\)</span> is negative.
But, data will show.</p>
<p>How can we interpret the model equations?
The model assumes that the <strong>expected</strong> height <span class="math inline">\(\mu_i\)</span> of a person <span class="math inline">\(i\)</span>
depends non-linearly (quadratically) on the weight <span class="math inline">\(x_i\)</span> of the person.
We are in the business of mean-modeling.
The prior for <span class="math inline">\(\sigma\)</span> is uniform as before.
The prior for <span class="math inline">\(\alpha\)</span> is normal with mean <span class="math inline">\(178\)</span> and standard deviation <span class="math inline">\(20\)</span>
because this is what we can expect from body heights in our experience.</p>
<p>Let’s <strong>fit the model</strong>:</p>
<p>We standardize the weight again and add the squared weights to the data set.
Standardizing the the predictors is a good idea, especially in polynomial regression
since squares and cubes of large numbers can get huge and cause numerical problems.</p>
<p>Let’s fit the model with the quadratic term for weight:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="multiple-linear-regression.html#cb109-1" tabindex="-1"></a><span class="co"># Standardize weight</span></span>
<span id="cb109-2"><a href="multiple-linear-regression.html#cb109-2" tabindex="-1"></a>d<span class="sc">$</span>weight_s <span class="ot">&lt;-</span> (d<span class="sc">$</span>weight <span class="sc">-</span> <span class="fu">mean</span>(d<span class="sc">$</span>weight)) <span class="sc">/</span> <span class="fu">sd</span>(d<span class="sc">$</span>weight)</span>
<span id="cb109-3"><a href="multiple-linear-regression.html#cb109-3" tabindex="-1"></a><span class="co"># Square of standardized weight</span></span>
<span id="cb109-4"><a href="multiple-linear-regression.html#cb109-4" tabindex="-1"></a>d<span class="sc">$</span>weight_s2 <span class="ot">&lt;-</span> d<span class="sc">$</span>weight_s<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb109-5"><a href="multiple-linear-regression.html#cb109-5" tabindex="-1"></a>m4<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb109-6"><a href="multiple-linear-regression.html#cb109-6" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb109-7"><a href="multiple-linear-regression.html#cb109-7" tabindex="-1"></a>    height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb109-8"><a href="multiple-linear-regression.html#cb109-8" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b1<span class="sc">*</span>weight_s <span class="sc">+</span> b2<span class="sc">*</span>weight_s<span class="sc">^</span><span class="dv">2</span>,</span>
<span id="cb109-9"><a href="multiple-linear-regression.html#cb109-9" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">178</span>, <span class="dv">20</span>),</span>
<span id="cb109-10"><a href="multiple-linear-regression.html#cb109-10" tabindex="-1"></a>    b1 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb109-11"><a href="multiple-linear-regression.html#cb109-11" tabindex="-1"></a>    b2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb109-12"><a href="multiple-linear-regression.html#cb109-12" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb109-13"><a href="multiple-linear-regression.html#cb109-13" tabindex="-1"></a>  ), <span class="at">data =</span> d)</span>
<span id="cb109-14"><a href="multiple-linear-regression.html#cb109-14" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##             mean        sd       5.5%      94.5%
## a     146.672739 0.3736465 146.075580 147.269898
## b1     21.397637 0.2898827  20.934348  21.860925
## b2     -8.419933 0.2813308  -8.869554  -7.970312
## sigma   5.750550 0.1743749   5.471865   6.029235</code></pre>
<p><span class="math inline">\(\beta_2\)</span> is indeed negative.
We get our <strong><a href="https://en.wikipedia.org/wiki/Joint_probability_distribution">joint distribution</a></strong>
of the <strong>four model parameters</strong>.
Let’s look at the fit using the mean estimates of the posterior distribution:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="multiple-linear-regression.html#cb111-1" tabindex="-1"></a><span class="co"># Summarize the model parameters</span></span>
<span id="cb111-2"><a href="multiple-linear-regression.html#cb111-2" tabindex="-1"></a>model_summary <span class="ot">&lt;-</span> <span class="fu">precis</span>(m4<span class="fl">.1</span>)</span>
<span id="cb111-3"><a href="multiple-linear-regression.html#cb111-3" tabindex="-1"></a>params <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(model_summary)</span>
<span id="cb111-4"><a href="multiple-linear-regression.html#cb111-4" tabindex="-1"></a></span>
<span id="cb111-5"><a href="multiple-linear-regression.html#cb111-5" tabindex="-1"></a><span class="co"># Extract parameter values</span></span>
<span id="cb111-6"><a href="multiple-linear-regression.html#cb111-6" tabindex="-1"></a>a <span class="ot">&lt;-</span> params[<span class="st">&quot;a&quot;</span>, <span class="st">&quot;mean&quot;</span>]       <span class="co"># Intercept</span></span>
<span id="cb111-7"><a href="multiple-linear-regression.html#cb111-7" tabindex="-1"></a>b1 <span class="ot">&lt;-</span> params[<span class="st">&quot;b1&quot;</span>, <span class="st">&quot;mean&quot;</span>]     <span class="co"># Coefficient for standardized weight</span></span>
<span id="cb111-8"><a href="multiple-linear-regression.html#cb111-8" tabindex="-1"></a>b2 <span class="ot">&lt;-</span> params[<span class="st">&quot;b2&quot;</span>, <span class="st">&quot;mean&quot;</span>]     <span class="co"># Coefficient for squared standardized weight</span></span>
<span id="cb111-9"><a href="multiple-linear-regression.html#cb111-9" tabindex="-1"></a></span>
<span id="cb111-10"><a href="multiple-linear-regression.html#cb111-10" tabindex="-1"></a><span class="co"># Generate a sequence of standardized weights for the fitted curve</span></span>
<span id="cb111-11"><a href="multiple-linear-regression.html#cb111-11" tabindex="-1"></a>weight_seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>weight_s), <span class="fu">max</span>(d<span class="sc">$</span>weight_s), <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb111-12"><a href="multiple-linear-regression.html#cb111-12" tabindex="-1"></a></span>
<span id="cb111-13"><a href="multiple-linear-regression.html#cb111-13" tabindex="-1"></a><span class="co"># Calculate the fitted values using the quadratic equation</span></span>
<span id="cb111-14"><a href="multiple-linear-regression.html#cb111-14" tabindex="-1"></a>height_fitted <span class="ot">&lt;-</span> a <span class="sc">+</span> b1 <span class="sc">*</span> weight_seq <span class="sc">+</span> b2 <span class="sc">*</span> weight_seq<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb111-15"><a href="multiple-linear-regression.html#cb111-15" tabindex="-1"></a></span>
<span id="cb111-16"><a href="multiple-linear-regression.html#cb111-16" tabindex="-1"></a><span class="co"># Plot the scatterplot</span></span>
<span id="cb111-17"><a href="multiple-linear-regression.html#cb111-17" tabindex="-1"></a><span class="fu">plot</span>(d<span class="sc">$</span>weight_s, d<span class="sc">$</span>height, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb111-18"><a href="multiple-linear-regression.html#cb111-18" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Standardized Weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Height (cm)&quot;</span>,</span>
<span id="cb111-19"><a href="multiple-linear-regression.html#cb111-19" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Scatterplot with Fitted Curve (Standardized Weight)&quot;</span>)</span>
<span id="cb111-20"><a href="multiple-linear-regression.html#cb111-20" tabindex="-1"></a></span>
<span id="cb111-21"><a href="multiple-linear-regression.html#cb111-21" tabindex="-1"></a><span class="co"># Add the fitted curve</span></span>
<span id="cb111-22"><a href="multiple-linear-regression.html#cb111-22" tabindex="-1"></a><span class="fu">lines</span>(weight_seq, height_fitted, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb111-23"><a href="multiple-linear-regression.html#cb111-23" tabindex="-1"></a></span>
<span id="cb111-24"><a href="multiple-linear-regression.html#cb111-24" tabindex="-1"></a><span class="co"># Add a legend</span></span>
<span id="cb111-25"><a href="multiple-linear-regression.html#cb111-25" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Observed data&quot;</span>, <span class="st">&quot;Fitted curve&quot;</span>),</span>
<span id="cb111-26"><a href="multiple-linear-regression.html#cb111-26" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="cn">NA</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="dv">1</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>This fits much better than the linear model without the quadratic term. In the book,
there is also a polynomial regression with a cubic term for weight.
Maybe this fits even better (see <a href="multiple-linear-regression.html#exercise1_multiple_regression">exercise 1</a>).</p>
</div>
<div id="adding_predictor_bayes" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Adding another predictor to the model<a href="multiple-linear-regression.html#adding_predictor_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the !Kung San data set has already such a high <span class="math inline">\(R^2\)</span> with the quadratic term
(and possibly higher with the cubic term), we will use the created data set from
<a href="multiple-linear-regression.html#adding_predictor_freq">below</a> in the frequentist setting
to estimate the coefficients of the model with two predictors.</p>
<p>We use rather uniformative priors and fit the model using <code>quap</code>:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="multiple-linear-regression.html#cb112-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb112-2"><a href="multiple-linear-regression.html#cb112-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb112-3"><a href="multiple-linear-regression.html#cb112-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb112-4"><a href="multiple-linear-regression.html#cb112-4" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb112-5"><a href="multiple-linear-regression.html#cb112-5" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb112-6"><a href="multiple-linear-regression.html#cb112-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">2</span>) <span class="co"># true model</span></span>
<span id="cb112-7"><a href="multiple-linear-regression.html#cb112-7" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb112-8"><a href="multiple-linear-regression.html#cb112-8" tabindex="-1"></a></span>
<span id="cb112-9"><a href="multiple-linear-regression.html#cb112-9" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb112-10"><a href="multiple-linear-regression.html#cb112-10" tabindex="-1"></a>m4<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb112-11"><a href="multiple-linear-regression.html#cb112-11" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb112-12"><a href="multiple-linear-regression.html#cb112-12" tabindex="-1"></a>    Y <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb112-13"><a href="multiple-linear-regression.html#cb112-13" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b1<span class="sc">*</span>X1 <span class="sc">+</span> b2<span class="sc">*</span>X2,</span>
<span id="cb112-14"><a href="multiple-linear-regression.html#cb112-14" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">10</span>, <span class="dv">10</span>),</span>
<span id="cb112-15"><a href="multiple-linear-regression.html#cb112-15" tabindex="-1"></a>    b1 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb112-16"><a href="multiple-linear-regression.html#cb112-16" tabindex="-1"></a>    b2 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb112-17"><a href="multiple-linear-regression.html#cb112-17" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb112-18"><a href="multiple-linear-regression.html#cb112-18" tabindex="-1"></a>  ), <span class="at">data =</span> df)</span>
<span id="cb112-19"><a href="multiple-linear-regression.html#cb112-19" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>##             mean         sd      5.5%      94.5%
## a     10.2700227 0.18934442 9.9674137 10.5726316
## b1     0.4467278 0.04131434 0.3806995  0.5127561
## b2     1.0095082 0.03899992 0.9471788  1.0718377
## sigma  1.8738833 0.13250803 1.6621099  2.0856567</code></pre>
<div id="check_model_bayes" class="section level4 hasAnchor" number="4.1.3.1">
<h4><span class="header-section-number">4.1.3.1</span> Checking model assumptions<a href="multiple-linear-regression.html#check_model_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Andrew Gelman mentions in some of his talks (see <a href="https://sites.stat.columbia.edu/gelman/research/published/philosophy_chapter.pdf">here</a> for more details)
that many Bayesians he met do not check their models, since they reflect subjective probability.
As I said in the introduction, one should not be afraid to check model predictions against the observed
and probably new data. If a model for predicting BMI performs much worse on a new data set,
we can probably conclude that the model does not reflect the general relationship between the predictors
and the dependent variable. We <strong>do not ask</strong> the question if a model is true or false, but if it is useful or
how badly the model assumptions are violated.</p>
<p>For further, more detailed information on model checking, refer to chapter 6 of
<a href="https://sites.stat.columbia.edu/gelman/book/BDA3.pdf">Gelman’s book</a>.</p>
<p>Anyhow, we plot two posterior predictive checks here.
We test the model within the same data set.
In order to do this, we create new observations
by drawing from the posterior distribution and compare these with the acutally
observed values. This is called <strong>posterior predictive checks</strong>.</p>
<p><strong>First, we plot the observed <span class="math inline">\(Y\)</span> values against the predicted <span class="math inline">\(Y\)</span> values (<span class="math inline">\(=\hat{Y}\)</span>)</strong>
from the model (as in Statistical rethinking, Chapter 5).
Although these practically never lie on the line <span class="math inline">\(y=x\)</span>, they should be sufficiently
close to it. We could also compare these two plots with the mean-model (see <a href="multiple-linear-regression.html#exercise6_multiple_regression">exercise 6</a>).</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="multiple-linear-regression.html#cb114-1" tabindex="-1"></a><span class="co"># 1) Posterior predictive checks Y vs Y_hat</span></span>
<span id="cb114-2"><a href="multiple-linear-regression.html#cb114-2" tabindex="-1"></a><span class="co"># see Statstical Rethinking p 138.</span></span>
<span id="cb114-3"><a href="multiple-linear-regression.html#cb114-3" tabindex="-1"></a><span class="co"># call link without specifying new data</span></span>
<span id="cb114-4"><a href="multiple-linear-regression.html#cb114-4" tabindex="-1"></a><span class="co"># so it uses the original data</span></span>
<span id="cb114-5"><a href="multiple-linear-regression.html#cb114-5" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">link</span>(m4<span class="fl">.2</span>)</span>
<span id="cb114-6"><a href="multiple-linear-regression.html#cb114-6" tabindex="-1"></a></span>
<span id="cb114-7"><a href="multiple-linear-regression.html#cb114-7" tabindex="-1"></a><span class="co"># summarize samples accross cases</span></span>
<span id="cb114-8"><a href="multiple-linear-regression.html#cb114-8" tabindex="-1"></a>mu_mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, mean)</span>
<span id="cb114-9"><a href="multiple-linear-regression.html#cb114-9" tabindex="-1"></a>mu_PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb114-10"><a href="multiple-linear-regression.html#cb114-10" tabindex="-1"></a></span>
<span id="cb114-11"><a href="multiple-linear-regression.html#cb114-11" tabindex="-1"></a><span class="co"># simulate observations</span></span>
<span id="cb114-12"><a href="multiple-linear-regression.html#cb114-12" tabindex="-1"></a><span class="co"># again, no new data, so uses original data</span></span>
<span id="cb114-13"><a href="multiple-linear-regression.html#cb114-13" tabindex="-1"></a>D_sim <span class="ot">&lt;-</span> <span class="fu">sim</span>(m4<span class="fl">.2</span>, <span class="at">n =</span> <span class="fl">1e4</span>)</span>
<span id="cb114-14"><a href="multiple-linear-regression.html#cb114-14" tabindex="-1"></a>D_PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(D_sim, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb114-15"><a href="multiple-linear-regression.html#cb114-15" tabindex="-1"></a></span>
<span id="cb114-16"><a href="multiple-linear-regression.html#cb114-16" tabindex="-1"></a><span class="fu">plot</span>(mu_mean <span class="sc">~</span> df<span class="sc">$</span>Y, <span class="at">col =</span> rangi2, <span class="at">ylim =</span> <span class="fu">range</span>(mu_PI), </span>
<span id="cb114-17"><a href="multiple-linear-regression.html#cb114-17" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Observed Y&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Model-Predicted Y&quot;</span>)</span>
<span id="cb114-18"><a href="multiple-linear-regression.html#cb114-18" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a =</span> <span class="dv">0</span>, <span class="at">b =</span> <span class="dv">1</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb114-19"><a href="multiple-linear-regression.html#cb114-19" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df)) <span class="fu">lines</span>(<span class="fu">rep</span>(df<span class="sc">$</span>Y[i], <span class="dv">2</span>), mu_PI[,i], <span class="at">col =</span> rangi2)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-62-1.png" width="672" /></p>
<p>As we can see, the model fits the data quite well. The points are close to the dashed line (<span class="math inline">\(y=x\)</span>).
No under- or overestimation is visible. The model seems to capture the relationship between the predictors <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
and the dependent variable <span class="math inline">\(Y\)</span> quite well - at least in a predictive sense. If there were patches of data points above or below the dashed line,
we would probably have to reconsider the model definition and think about why these points are not captured by the model.</p>
<p><strong>Next, we plot the posterior predictive plots</strong> analog to the upper left in the <code>check_model</code> output.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="multiple-linear-regression.html#cb115-1" tabindex="-1"></a><span class="fu">library</span>(scales)  <span class="co"># For the alpha function to adjust transparency</span></span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;scales&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     discard</code></pre>
<pre><code>## The following object is masked from &#39;package:readr&#39;:
## 
##     col_factor</code></pre>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="multiple-linear-regression.html#cb119-1" tabindex="-1"></a><span class="co"># 2) Posterior predictive densities</span></span>
<span id="cb119-2"><a href="multiple-linear-regression.html#cb119-2" tabindex="-1"></a><span class="co"># Simulate observations using the posterior predictive distribution</span></span>
<span id="cb119-3"><a href="multiple-linear-regression.html#cb119-3" tabindex="-1"></a>D_sim <span class="ot">&lt;-</span> <span class="fu">sim</span>(m4<span class="fl">.2</span>, <span class="at">n =</span> <span class="fl">1e4</span>)  <span class="co"># Generate 10,000 simulated datasets</span></span>
<span id="cb119-4"><a href="multiple-linear-regression.html#cb119-4" tabindex="-1"></a></span>
<span id="cb119-5"><a href="multiple-linear-regression.html#cb119-5" tabindex="-1"></a><span class="co"># Calculate densities for all samples</span></span>
<span id="cb119-6"><a href="multiple-linear-regression.html#cb119-6" tabindex="-1"></a>densities <span class="ot">&lt;-</span> <span class="fu">apply</span>(D_sim, <span class="dv">1</span>, density)</span>
<span id="cb119-7"><a href="multiple-linear-regression.html#cb119-7" tabindex="-1"></a></span>
<span id="cb119-8"><a href="multiple-linear-regression.html#cb119-8" tabindex="-1"></a><span class="co"># Find the maximum density value for setting the y-axis limits</span></span>
<span id="cb119-9"><a href="multiple-linear-regression.html#cb119-9" tabindex="-1"></a>max_density <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fu">sapply</span>(densities, <span class="cf">function</span>(d) <span class="fu">max</span>(d<span class="sc">$</span>y)))</span>
<span id="cb119-10"><a href="multiple-linear-regression.html#cb119-10" tabindex="-1"></a></span>
<span id="cb119-11"><a href="multiple-linear-regression.html#cb119-11" tabindex="-1"></a><span class="co"># Create the density plot with predefined ylim</span></span>
<span id="cb119-12"><a href="multiple-linear-regression.html#cb119-12" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(df<span class="sc">$</span>Y), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, max_density),</span>
<span id="cb119-13"><a href="multiple-linear-regression.html#cb119-13" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Y&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Density&quot;</span>,</span>
<span id="cb119-14"><a href="multiple-linear-regression.html#cb119-14" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Comparison of Observed and Predicted Densities&quot;</span>)</span>
<span id="cb119-15"><a href="multiple-linear-regression.html#cb119-15" tabindex="-1"></a></span>
<span id="cb119-16"><a href="multiple-linear-regression.html#cb119-16" tabindex="-1"></a><span class="co"># Add 100 posterior predictive density lines</span></span>
<span id="cb119-17"><a href="multiple-linear-regression.html#cb119-17" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)  <span class="co"># For reproducibility</span></span>
<span id="cb119-18"><a href="multiple-linear-regression.html#cb119-18" tabindex="-1"></a>n_lines <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb119-19"><a href="multiple-linear-regression.html#cb119-19" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fl">1e4</span>, n_lines)  <span class="co"># Randomly sample 100 posterior predictive datasets</span></span>
<span id="cb119-20"><a href="multiple-linear-regression.html#cb119-20" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> samples) {</span>
<span id="cb119-21"><a href="multiple-linear-regression.html#cb119-21" tabindex="-1"></a>  <span class="fu">lines</span>(<span class="fu">density</span>(D_sim[s, ]), <span class="at">col =</span> <span class="fu">alpha</span>(<span class="st">&quot;lightblue&quot;</span>, <span class="fl">0.3</span>), <span class="at">lwd =</span> <span class="dv">1</span>)</span>
<span id="cb119-22"><a href="multiple-linear-regression.html#cb119-22" tabindex="-1"></a>}</span>
<span id="cb119-23"><a href="multiple-linear-regression.html#cb119-23" tabindex="-1"></a></span>
<span id="cb119-24"><a href="multiple-linear-regression.html#cb119-24" tabindex="-1"></a><span class="co"># Add the density line for the observed Y values</span></span>
<span id="cb119-25"><a href="multiple-linear-regression.html#cb119-25" tabindex="-1"></a>obs_density <span class="ot">&lt;-</span> <span class="fu">density</span>(df<span class="sc">$</span>Y)</span>
<span id="cb119-26"><a href="multiple-linear-regression.html#cb119-26" tabindex="-1"></a><span class="fu">lines</span>(obs_density<span class="sc">$</span>x, obs_density<span class="sc">$</span>y, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb119-27"><a href="multiple-linear-regression.html#cb119-27" tabindex="-1"></a></span>
<span id="cb119-28"><a href="multiple-linear-regression.html#cb119-28" tabindex="-1"></a><span class="co"># Add legend</span></span>
<span id="cb119-29"><a href="multiple-linear-regression.html#cb119-29" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Posterior Predictive Densities&quot;</span>, <span class="st">&quot;Observed Density&quot;</span>),</span>
<span id="cb119-30"><a href="multiple-linear-regression.html#cb119-30" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;lightblue&quot;</span>, <span class="st">&quot;green&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>The light blue lines show distributions of model predicted <span class="math inline">\(Y\)</span> values.
The green line shows the distribution of the observed <span class="math inline">\(Y\)</span> values.
As we can see, there seem to be no systematic differences between the observed and predicted values.
The model seems to capture the relationship well.
If we see systematic deviations here, we need to reconsider the model definition.</p>
<p><strong>Example</strong>: If you want to predict pain (<span class="math inline">\(Y\)</span> variable) and you have a lot of zeros (pain-free participants)
you will probably see a discrepancy between the observed and predicted values in this plot.
What could you do? You could use a two step process (model the probability that a person is pain-free
and then model the pain intensity for the people who have pain) or use a different model (like a zero-inflated model).</p>
<p>Note that we did not explicitely assume normally distributed errors in the model definition above,
so we won’t check this here but in the Frequentist framework below.</p>
</div>
</div>
</div>
<div id="linear-regression-with-2-predictors-in-the-frequentist-framework" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Linear regression with 2 predictors in the Frequentist Framework<a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-frequentist-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To reiterate from the last chapter:
In full, the <strong>classical linear regression model</strong> can be written as
(see p. 21-22 in Westfall):</p>
<p><span class="math display">\[ Y_i|X_i = x_i \sim_{independent} N(\beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik},\sigma^2)\]</span>
for <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
<p>The <span class="math inline">\(Y_i\)</span> are independently normally distributed <em>conditioned</em> on the predictors
having the values <span class="math inline">\(X_i = x_i\)</span>. Each conditional distribution has an expected value (<span class="math inline">\(\mu\)</span>)
that is a linear function of the predictors and a constant variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>If the assumptions of the classical linear regression model are met, the least squares
estimators (OLS) are the best (smallest variance) linear unbiased (on average correct) estimators
(so-called: BLUE) of the parameters.</p>
<div id="adding_transformed_predictor_freq" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Adding a transformed predictor to the model<a href="multiple-linear-regression.html#adding_transformed_predictor_freq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>No, let’s fit the same model as above in the Frequentist framework.</p>
<p>The model is:</p>
<p><span class="math display">\[height_i = \alpha + \beta_1 weight_i + \beta_2 weight_i^2 + \varepsilon_i\]</span>
whereas
<span class="math display">\[\varepsilon_i \sim N(0, \sigma)\]</span></p>
<p>And if you build the expectation on both sides for fixed <span class="math inline">\(weight_i\)</span>, you get:</p>
<p><span class="math display">\[\mathbb{E}(height_i|weight_i) = \alpha + \beta_1 weight_i + \beta_2 weight_i^2\]</span></p>
<p>The last line means, the expected height of a person given a certain weight depends
quadratically on the weight. The error term <span class="math inline">\(\varepsilon_i\)</span> is on average zero, hence it goes away here.
Remember the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law">law of large numbers</a>:
The sample mean <span class="math inline">\(\bar{\varepsilon_i}\)</span> approaches the expected value <span class="math inline">\(\mathbb{E}(\varepsilon_i)=0\)</span>
as the sample size increases. If you drew many samples and average over the error terms,
the average will approach zero.
The weights are considered fixed and therefore do not change when building the expectation.</p>
<p>We are looking for fixed, but unknown, parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span> and <span class="math inline">\(\sigma\)</span>.
The fixed <span class="math inline">\(\sigma\)</span> indicates that the observations <em>wiggle</em> around the expected value
equally strong not matter which weight we have. This is called <strong>homoscedasticity</strong>.</p>
<p>Let’s fit the model using the <code>lm</code> function in R which uses
<a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> to estimate the parameters.
At this point I could torture you with <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)">matrix algebra</a>
and show you the <a href="https://en.wikipedia.org/wiki/Linear_least_squares">normal equations</a> for linear regression,
but I will spare you for now.
Note that the least squares algorithm for fitting the curve works for all
kinds of functional forms. For example, we could also fit an exponential curve using the same
technique (see <a href="multiple-linear-regression.html#exercise9_multiple_regression">exercise 9</a>).</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="multiple-linear-regression.html#cb120-1" tabindex="-1"></a><span class="co"># scale weight</span></span>
<span id="cb120-2"><a href="multiple-linear-regression.html#cb120-2" tabindex="-1"></a>d<span class="sc">$</span>weight_s <span class="ot">&lt;-</span> <span class="fu">scale</span>(d<span class="sc">$</span>weight)</span>
<span id="cb120-3"><a href="multiple-linear-regression.html#cb120-3" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb120-4"><a href="multiple-linear-regression.html#cb120-4" tabindex="-1"></a>m4<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight_s <span class="sc">+</span> <span class="fu">I</span>(weight_s<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> d)</span>
<span id="cb120-5"><a href="multiple-linear-regression.html#cb120-5" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = height ~ weight_s + I(weight_s^2), data = d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.9689  -3.9794   0.2364   3.9262  19.5182 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   146.6604     0.3748  391.30   &lt;2e-16 ***
## weight_s       21.4149     0.2908   73.64   &lt;2e-16 ***
## I(weight_s^2)  -8.4123     0.2822  -29.80   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.766 on 541 degrees of freedom
## Multiple R-squared:  0.9565, Adjusted R-squared:  0.9564 
## F-statistic:  5952 on 2 and 541 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="multiple-linear-regression.html#cb122-1" tabindex="-1"></a><span class="fu">mean</span>(d<span class="sc">$</span>height)</span></code></pre></div>
<pre><code>## [1] 138.2636</code></pre>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="multiple-linear-regression.html#cb124-1" tabindex="-1"></a><span class="fu">confint</span>(m4<span class="fl">.2</span>, <span class="at">level =</span> <span class="fl">0.94</span>)</span></code></pre></div>
<pre><code>##                      3 %       97 %
## (Intercept)   145.954018 147.366836
## weight_s       20.866788  21.962979
## I(weight_s^2)  -8.944251  -7.880337</code></pre>
<p>See <code>?I</code> in R. This command is used so that R knows that it should
treat the “^2” as “square” and not as formula syntax.
We could also create a new variable as before. Whatever you prefer.</p>
<div id="interpretation-of-output-and-coefficients" class="section level4 hasAnchor" number="4.2.1.1">
<h4><span class="header-section-number">4.2.1.1</span> Interpretation of output and coefficients<a href="multiple-linear-regression.html#interpretation-of-output-and-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The intercept <span class="math inline">\(\alpha\)</span> is the <strong>model-predicted height</strong> of a person of <strong>average weight</strong>
(<span class="math inline">\(weight_s=0\)</span> for a person of average weight).
Note that this is not equal to the average height (<span class="math inline">\(138.2636 cm\)</span>) of the people in the data set.</li>
<li>The residuals have range from <span class="math inline">\(-19.97\)</span> to <span class="math inline">\(19.51\)</span>. So, the model maximally
overestimates the heights by <span class="math inline">\(19.97\)</span> cm and underestimates by <span class="math inline">\(19.51\)</span> cm.
These numbers are plausible when you look at the scatterplot with the fitted
curve.</li>
<li>The coefficients <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> agree with the Bayes estimates.
Specifically, <span class="math inline">\(\beta_2\)</span> is non-zero indicating curvature. You cannot directly interpret the coefficients
as in the non-quadratic case since, for instance, you cannot change <span class="math inline">\(weight^2\)</span> by one unit
and hold <span class="math inline">\(weight\)</span> constant at the same time. Refer to Peter Westfall’s book section 9.1. for all the details.</li>
<li>If you like <span class="math inline">\(p\)</span>-values: All the hypotheses that the coefficients are zero
are rejected. The <span class="math inline">\(p\)</span>-values are very small. The values of the test statistics can not be explained
by chance alone. On the other hand, for at least <span class="math inline">\(\beta_1\)</span> and
and the global test this is not a surprise when you look at the scatterplot.
After having fit many models, you would have guessed that all three parameters
are solidly non-zero. The intercept is not zero since a person of average weight probably
has non-zero height. <span class="math inline">\(\beta_1\)</span> is non-zero since you can easily imagine a linear
trend line with positive slope going through the data, and <span class="math inline">\(\beta_2\)</span> is non-zero
since there is clearly (non-trivial) curvature in the scatterplot.</li>
<li>The <span class="math inline">\(R^2\)</span> is a whopping <span class="math inline">\(0.96\)</span> which could be a sign of overfitting, but
in this case we conclude that the true relationship is caputured rather well.
<a href="https://en.wikipedia.org/wiki/Overfitting">Overfitting</a> would occur if
our curve would wiggle around the data points,
so we would fit the data too much to the noise in the data than
the underying trend.</li>
</ul>
</div>
<div id="checking-model-assumptions" class="section level4 hasAnchor" number="4.2.1.2">
<h4><span class="header-section-number">4.2.1.2</span> Checking model assumptions<a href="multiple-linear-regression.html#checking-model-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="multiple-linear-regression.html#cb126-1" tabindex="-1"></a><span class="fu">check_model</span>(m4<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>## Some of the variables were in matrix-format - probably you used
##   `scale()` on your data?
##   If so, and you get an error, please try `datawizard::standardize()` to
##   standardize your data.
## Some of the variables were in matrix-format - probably you used
##   `scale()` on your data?
##   If so, and you get an error, please try `datawizard::standardize()` to
##   standardize your data.</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<p>If we want to be perfectionists, we could remark that (upper right plot)
in the lower fitted values the residuals are more negative,
meaning that the model overestimates the heights in this region.
In the middle region the model underestimates a bit and we can see
a positive tendency in the residuals. Apart from that,
the diagnostic plots look excellent.</p>
</div>
</div>
<div id="adding_predictor_freq" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Adding another predictor to the model<a href="multiple-linear-regression.html#adding_predictor_freq" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, we add another predictor to the model. We use <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>
<strong>simultaneously</strong> to predict <span class="math inline">\(Y\)</span>. We are now in the lucky situation that
we can still visualize the situation in 3D. The regression line from simple
linear regression
becomes a <a href="https://stackoverflow.com/questions/47344850/scatterplot3d-regression-plane-with-residuals">plane</a>.
The vertical distances between the data points and the plane are the residuals.
See <a href="https://rpubs.com/pjozefek/576206">here</a> or
<a href="https://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization">here</a>
at the end for examples.
Minimizing the sum of the squared errors gives again
the estimates for the coefficients.</p>
<p>For demonstration purposes, we can <strong>create data ourselves</strong> with known
coefficients. This is the same as <a href="multiple-linear-regression.html#adding_predictor_bayes">above</a>.
This is the true model, which we usually do not know:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, \sigma^2)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>for example:</p>
<p><span class="math display">\[ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, 5)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>According to the model, the conditional expected value of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(X_1 = x_1\)</span> and <span class="math inline">\(X_2 = x_2\)</span>
is a linear function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. Note, that small letters are realized values
of random variables. Also note, that in the expectation the error term goes away, since
<span class="math inline">\(\mathbb{E}(\varepsilon_i) = 0\)</span>.</p>
<ul>
<li>If <span class="math inline">\(X_1\)</span> increases by one unit, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(0.5\)</span> units on average (in expectation).</li>
<li>If <span class="math inline">\(X_2\)</span> increases by one unit, <span class="math inline">\(Y\)</span> increases by <span class="math inline">\(1\)</span> unit on average (in expectation).</li>
<li>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are zero, <span class="math inline">\(Y\)</span> is <span class="math inline">\(10\)</span> on average (in expectation).</li>
</ul>
<p>Why in expectation? Because there is still the error term which makes the whole thing random!
We can see that an increase in <span class="math inline">\(X_1\)</span> does not influence the relationship between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y\)</span>.
Hence, there is <strong>no interaction</strong> between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with respect to <span class="math inline">\(Y\)</span>.</p>
<p>Now lets’s draw 100 points from this model, fit the model and add the plane:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="multiple-linear-regression.html#cb128-1" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb128-2"><a href="multiple-linear-regression.html#cb128-2" tabindex="-1"></a></span>
<span id="cb128-3"><a href="multiple-linear-regression.html#cb128-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb128-4"><a href="multiple-linear-regression.html#cb128-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb128-5"><a href="multiple-linear-regression.html#cb128-5" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb128-6"><a href="multiple-linear-regression.html#cb128-6" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb128-7"><a href="multiple-linear-regression.html#cb128-7" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb128-8"><a href="multiple-linear-regression.html#cb128-8" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb128-9"><a href="multiple-linear-regression.html#cb128-9" tabindex="-1"></a></span>
<span id="cb128-10"><a href="multiple-linear-regression.html#cb128-10" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb128-11"><a href="multiple-linear-regression.html#cb128-11" tabindex="-1"></a>m4<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2, <span class="at">data =</span> d)</span>
<span id="cb128-12"><a href="multiple-linear-regression.html#cb128-12" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7460 -1.3215 -0.2489  1.2427  4.1597 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.27013    0.19228   53.41   &lt;2e-16 ***
## X1           0.44673    0.04195   10.65   &lt;2e-16 ***
## X2           1.00952    0.03960   25.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.903 on 97 degrees of freedom
## Multiple R-squared:  0.8839, Adjusted R-squared:  0.8815 
## F-statistic: 369.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="multiple-linear-regression.html#cb130-1" tabindex="-1"></a><span class="co"># Create a grid for the plane</span></span>
<span id="cb130-2"><a href="multiple-linear-regression.html#cb130-2" tabindex="-1"></a>X1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X1), <span class="fu">max</span>(d<span class="sc">$</span>X1), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb130-3"><a href="multiple-linear-regression.html#cb130-3" tabindex="-1"></a>X2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X2), <span class="fu">max</span>(d<span class="sc">$</span>X2), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb130-4"><a href="multiple-linear-regression.html#cb130-4" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> X1_grid, <span class="at">X2 =</span> X2_grid)</span>
<span id="cb130-5"><a href="multiple-linear-regression.html#cb130-5" tabindex="-1"></a></span>
<span id="cb130-6"><a href="multiple-linear-regression.html#cb130-6" tabindex="-1"></a><span class="co"># Predict the values for the grid</span></span>
<span id="cb130-7"><a href="multiple-linear-regression.html#cb130-7" tabindex="-1"></a>grid<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">predict</span>(m4<span class="fl">.3</span>, <span class="at">newdata =</span> grid)</span>
<span id="cb130-8"><a href="multiple-linear-regression.html#cb130-8" tabindex="-1"></a></span>
<span id="cb130-9"><a href="multiple-linear-regression.html#cb130-9" tabindex="-1"></a><span class="co"># Convert the grid into a matrix for the plane</span></span>
<span id="cb130-10"><a href="multiple-linear-regression.html#cb130-10" tabindex="-1"></a>plane_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(grid<span class="sc">$</span>Y, <span class="at">nrow =</span> <span class="fu">length</span>(X1_grid), <span class="at">ncol =</span> <span class="fu">length</span>(X2_grid))</span>
<span id="cb130-11"><a href="multiple-linear-regression.html#cb130-11" tabindex="-1"></a></span>
<span id="cb130-12"><a href="multiple-linear-regression.html#cb130-12" tabindex="-1"></a><span class="co"># Create the interactive 3D plot</span></span>
<span id="cb130-13"><a href="multiple-linear-regression.html#cb130-13" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb130-14"><a href="multiple-linear-regression.html#cb130-14" tabindex="-1"></a>  <span class="fu">add_markers</span>(</span>
<span id="cb130-15"><a href="multiple-linear-regression.html#cb130-15" tabindex="-1"></a>    <span class="at">x =</span> d<span class="sc">$</span>X2, <span class="at">y =</span> d<span class="sc">$</span>X1, <span class="at">z =</span> d<span class="sc">$</span>Y,</span>
<span id="cb130-16"><a href="multiple-linear-regression.html#cb130-16" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb130-17"><a href="multiple-linear-regression.html#cb130-17" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span></span>
<span id="cb130-18"><a href="multiple-linear-regression.html#cb130-18" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb130-19"><a href="multiple-linear-regression.html#cb130-19" tabindex="-1"></a>  <span class="fu">add_surface</span>(</span>
<span id="cb130-20"><a href="multiple-linear-regression.html#cb130-20" tabindex="-1"></a>    <span class="at">x =</span> X1_grid, <span class="at">y =</span> X2_grid, <span class="at">z =</span> plane_matrix,</span>
<span id="cb130-21"><a href="multiple-linear-regression.html#cb130-21" tabindex="-1"></a>    <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;pink&quot;</span>)),</span>
<span id="cb130-22"><a href="multiple-linear-regression.html#cb130-22" tabindex="-1"></a>    <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb130-23"><a href="multiple-linear-regression.html#cb130-23" tabindex="-1"></a>    <span class="at">opacity =</span> <span class="fl">0.7</span>,</span>
<span id="cb130-24"><a href="multiple-linear-regression.html#cb130-24" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Fitted Plane&quot;</span></span>
<span id="cb130-25"><a href="multiple-linear-regression.html#cb130-25" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb130-26"><a href="multiple-linear-regression.html#cb130-26" tabindex="-1"></a>  plotly<span class="sc">::</span><span class="fu">layout</span>(</span>
<span id="cb130-27"><a href="multiple-linear-regression.html#cb130-27" tabindex="-1"></a>    <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb130-28"><a href="multiple-linear-regression.html#cb130-28" tabindex="-1"></a>      <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb130-29"><a href="multiple-linear-regression.html#cb130-29" tabindex="-1"></a>      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb130-30"><a href="multiple-linear-regression.html#cb130-30" tabindex="-1"></a>      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb130-31"><a href="multiple-linear-regression.html#cb130-31" tabindex="-1"></a>    ),</span>
<span id="cb130-32"><a href="multiple-linear-regression.html#cb130-32" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Interactive 3D Scatterplot with Fitted Plane&quot;</span></span>
<span id="cb130-33"><a href="multiple-linear-regression.html#cb130-33" tabindex="-1"></a>  )</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-58cece611d4f1b241602" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-58cece611d4f1b241602">{"x":{"visdat":{"a35a72442b98":["function () ","plotlyVisDat"]},"cur_data":"a35a72442b98","attrs":{"a35a72442b98":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[9.4443987628904011,13.333800774941469,12.133021279668236,9.5249461000369529,4.7364466057395678,13.110030053932444,5.6505624919871416,-2.6915973043760397,9.6835502035007526,13.372771869883055,10.421960153411863,14.426731037966759,5.3774668420099854,8.966769300350542,9.2224188820480499,19.324443517647648,10.890680732507244,0.43379512701927547,5.0323217859747178,1.1279460830200065,6.7707267619375422,5.9536612767633095,7.2018979385879955,8.3124876734520878,16.92939726234739,2.6430170894358751,11.862207545300455,9.3388007192618208,4.1146754847733282,10.746811714849734,22.199502776793707,11.339202369983692,12.873056415835688,8.6057941476553346,0.63893921686459931,14.744254437710621,3.7157430024744071,14.382923087793795,19.429219875060383,0.26628371850054355,10.194910287864269,7.1648219212529405,1.9679096650446017,5.5744444031060336,4.6541209391291138,8.342419574497967,1.482060216115193,9.5532640731783047,21.120918627921032,4.327344911454289,13.820784779023231,12.650091589533226,10.866002283265379,8.560615962929548,12.03532704499019,12.212069610944848,11.10466464782988,10.860848824872161,14.967222752027357,5.6011456272436657,15.170421399691325,2.5185654276194671,2.9605141825589425,26.259158572277943,9.8223919406710856,15.34512167918176,14.037070888084919,4.2005526471672674,14.112419163123903,17.148448797139881,9.3855455547850664,6.4785989154930874,13.712628901296251,16.078708889025989,6.2725840687341741,6.1908326560139839,9.8266148785503837,8.6497118209268162,13.492209623864802,7.4102948959991402,1.3628295997224362,18.75211881281016,8.0971511174505544,6.7520738074737681,8.503674772841002,10.111652728783406,18.733737918563573,14.79333252002546,12.517339204227589,10.711689728035747,15.892753933951489,11.85592488806984,13.360273199407175,2.79848201789475,10.852588915270818,18.618819697732381,22.205580290317254,4.8733643852768296,6.3968866915551779,4.0063764681849268],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5},"name":"Data Points","inherit":true},"a35a72442b98.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[-5.2517772599504369,-3.845273924496897,-2.4387705890433566,-1.0322672535898163,0.37423608186372337,1.7807394173172628,3.1872427527708043,4.5937460882243437,6.0002494236778832,7.4067527591314226,8.813256094584963,10.219759430038504,11.626262765492045,13.032766100945583,14.439269436399124,15.845772771852662,17.252276107306205,18.658779442759744,20.065282778213284,21.471786113666823],[-4.7231644904891299,-3.31666115503559,-1.9101578195820497,-0.50365448412850922,0.90284885132503034,2.30935218677857,3.7158555222321112,5.1223588576856507,6.5288621931391901,7.9353655285927296,9.341868864046269,10.74837219949981,12.154875534953351,13.561378870406889,14.96788220586043,16.37438554131397,17.780888876767509,19.187392212221052,20.593895547674588,22.000398883128131],[-4.194551721027822,-2.7880483855742821,-1.3815450501207418,0.024958285332798636,1.4314616207863382,2.8379649562398779,4.2444682916934191,5.6509716271469586,7.057474962600498,8.4639782980540375,9.8704816335075769,11.276984968961118,12.683488304414659,14.089991639868197,15.496494975321738,16.902998310775278,18.309501646228817,19.71600498168236,21.122508317135896,22.529011652589439],[-3.6659389515665137,-2.2594356161129743,-0.85293228065943394,0.55357105479410651,1.9600743902476461,3.3665777257011857,4.773081061154727,6.1795843966082664,7.5860877320618059,8.9925910675153453,10.399094402968885,11.805597738422426,13.212101073875967,14.618604409329505,16.025107744783046,17.431611080236586,18.838114415690125,20.244617751143668,21.651121086597204,23.057624422050747],[-3.1373261821052059,-1.7308228466516666,-0.32431951119812608,1.0821838242554143,2.4886871597089537,3.8951904951624936,5.3016938306160348,6.7081971660695743,8.1147005015231137,9.5212038369766532,10.927707172430193,12.334210507883734,13.740713843337275,15.147217178790813,16.553720514244354,17.960223849697893,19.366727185151433,20.773230520604976,22.179733856058512,23.586237191512055],[-2.6087134126438989,-1.2022100771903597,0.2042932582631809,1.6107965937167212,3.0172999291702607,4.4238032646238006,5.8303066000773418,7.2368099355308813,8.6433132709844216,10.049816606437961,11.4563199418915,12.862823277345042,14.269326612798583,15.675829948252121,17.082333283705662,18.488836619159201,19.895339954612741,21.301843290066284,22.70834662551982,24.114849960973363],[-2.080100643182591,-0.67359730772905169,0.73290602772448876,2.1394093631780291,3.5459126986315685,4.9524160340851084,6.3589193695386497,7.7654227049921891,9.1719260404457295,10.578429375899269,11.984932711352808,13.39143604680635,14.797939382259891,16.204442717713427,17.61094605316697,19.017449388620509,20.423952724074049,21.830456059527592,23.236959394981128,24.643462730434671],[-1.5514878737212834,-0.14498453826774382,1.2615187971857966,2.668022132639337,4.0745254680928769,5.4810288035464163,6.8875321389999575,8.2940354744534979,9.7005388099070373,11.107042145360577,12.513545480814116,13.920048816267657,15.326552151721199,16.733055487174735,18.139558822628278,19.546062158081817,20.952565493535356,22.359068828988899,23.765572164442435,25.172075499895978],[-1.0228751042599755,0.38362823119356404,1.7901315666471045,3.1966349021006448,4.6031382375541847,6.0096415730077242,7.4161449084612654,8.8226482439148057,10.229151579368345,11.635654914821885,13.042158250275424,14.448661585728965,15.855164921182507,17.261668256636042,18.668171592089585,20.074674927543125,21.481178262996664,22.887681598450207,24.294184933903743,25.700688269357286],[-0.49426233479866943,0.91224100065487013,2.3187443361084106,3.7252476715619509,5.1317510070154908,6.5382543424690303,7.9447576779225715,9.3512610133761118,10.757764348829651,12.164267684283191,13.57077101973673,14.977274355190271,16.383777690643811,17.79028102609735,19.19678436155089,20.603287697004429,22.009791032457972,23.416294367911512,24.822797703365051,26.229301038818591],[0.034350434662638436,1.4408537701161779,2.8473571055697184,4.2538604410232592,5.6603637764767987,7.0668671119303381,8.4733704473838785,9.8798737828374197,11.286377118290959,12.692880453744499,14.099383789198038,15.505887124651579,16.912390460105119,18.318893795558658,19.725397131012198,21.131900466465737,22.53840380191928,23.94490713737282,25.351410472826359,26.757913808279898],[0.56296320412394629,1.9694665395774857,3.3759698750310263,4.7824732104845671,6.1889765459381065,7.595479881391646,9.0019832168451863,10.408486552298728,11.814989887752267,13.221493223205806,14.627996558659346,16.034499894112887,17.441003229566427,18.847506565019966,20.254009900473505,21.660513235927045,23.067016571380588,24.473519906834127,25.880023242287667,27.286526577741206],[1.0915759735852542,2.4980793090387938,3.9045826444923342,5.3110859799458749,6.7175893153994144,8.124092650852953,9.5305959863064942,10.937099321760035,12.343602657213575,13.750105992667114,15.156609328120654,16.563112663574195,17.969615999027734,19.376119334481274,20.782622669934813,22.189126005388353,23.595629340841896,25.002132676295435,26.408636011748975,27.815139347202514],[1.620188743046562,3.0266920785001017,4.4331954139536416,5.8396987494071828,7.2462020848607223,8.6527054203142608,10.059208755767802,11.465712091221343,12.872215426674883,14.278718762128422,15.685222097581962,17.091725433035503,18.498228768489042,19.904732103942582,21.311235439396121,22.717738774849661,24.124242110303204,25.530745445756743,26.937248781210283,28.343752116663822],[2.1488015125078701,3.5553048479614096,4.9618081834149494,6.3683115188684907,7.7748148543220301,9.1813181897755687,10.58782152522911,11.994324860682651,13.400828196136191,14.80733153158973,16.213834867043268,17.620338202496811,19.02684153795035,20.43334487340389,21.839848208857429,23.246351544310969,24.652854879764512,26.059358215218051,27.46586155067159,28.87236488612513],[2.6774142819691762,4.0839176174227152,5.4904209528762555,6.8969242883297968,8.3034276237833353,9.7099309592368748,11.116434294690416,12.522937630143957,13.929440965597497,15.335944301051036,16.742447636504576,18.148950971958115,19.555454307411658,20.961957642865194,22.368460978318737,23.774964313772276,25.181467649225816,26.587970984679359,27.994474320132895,29.400977655586438],[3.2060270514304841,4.6125303868840231,6.0190337223375634,7.4255370577911046,8.8320403932446432,10.238543728698183,11.645047064151724,13.051550399605265,14.458053735058805,15.864557070512344,17.271060405965883,18.677563741419423,20.084067076872966,21.490570412326502,22.897073747780045,24.303577083233584,25.710080418687124,27.116583754140667,28.523087089594203,29.929590425047746],[3.7346398208917919,5.1411431563453309,6.5476464917988713,7.9541498272524125,9.360653162705951,10.76715649815949,12.173659833613032,13.580163169066573,14.986666504520112,16.393169839973652,17.799673175427191,19.206176510880731,20.612679846334274,22.01918318178781,23.425686517241353,24.832189852694892,26.238693188148432,27.645196523601975,29.05169985905551,30.458203194509053],[4.2632525903530993,5.6697559258066388,7.0762592612601791,8.4827625967137195,9.8892659321672589,11.295769267620798,12.70227260307434,14.108775938527881,15.51527927398142,16.92178260943496,18.328285944888499,19.734789280342039,21.141292615795582,22.547795951249118,23.954299286702661,25.3608026221562,26.767305957609739,28.173809293063282,29.580312628516818,30.986815963970361],[4.7918653598144072,6.1983686952679466,7.604872030721487,9.0113753661750273,10.417878701628567,11.824382037082106,13.230885372535647,14.637388707989189,16.043892043442728,17.450395378896268,18.856898714349807,20.263402049803346,21.669905385256889,23.076408720710425,24.482912056163968,25.889415391617508,27.295918727071047,28.70242206252459,30.108925397978126,31.515428733431669]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"colorscale":[[0,1],["red","pink"]],"showscale":false,"opacity":0.69999999999999996,"name":"Fitted Plane","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"title":"Interactive 3D Scatterplot with Fitted Plane","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[9.4443987628904011,13.333800774941469,12.133021279668236,9.5249461000369529,4.7364466057395678,13.110030053932444,5.6505624919871416,-2.6915973043760397,9.6835502035007526,13.372771869883055,10.421960153411863,14.426731037966759,5.3774668420099854,8.966769300350542,9.2224188820480499,19.324443517647648,10.890680732507244,0.43379512701927547,5.0323217859747178,1.1279460830200065,6.7707267619375422,5.9536612767633095,7.2018979385879955,8.3124876734520878,16.92939726234739,2.6430170894358751,11.862207545300455,9.3388007192618208,4.1146754847733282,10.746811714849734,22.199502776793707,11.339202369983692,12.873056415835688,8.6057941476553346,0.63893921686459931,14.744254437710621,3.7157430024744071,14.382923087793795,19.429219875060383,0.26628371850054355,10.194910287864269,7.1648219212529405,1.9679096650446017,5.5744444031060336,4.6541209391291138,8.342419574497967,1.482060216115193,9.5532640731783047,21.120918627921032,4.327344911454289,13.820784779023231,12.650091589533226,10.866002283265379,8.560615962929548,12.03532704499019,12.212069610944848,11.10466464782988,10.860848824872161,14.967222752027357,5.6011456272436657,15.170421399691325,2.5185654276194671,2.9605141825589425,26.259158572277943,9.8223919406710856,15.34512167918176,14.037070888084919,4.2005526471672674,14.112419163123903,17.148448797139881,9.3855455547850664,6.4785989154930874,13.712628901296251,16.078708889025989,6.2725840687341741,6.1908326560139839,9.8266148785503837,8.6497118209268162,13.492209623864802,7.4102948959991402,1.3628295997224362,18.75211881281016,8.0971511174505544,6.7520738074737681,8.503674772841002,10.111652728783406,18.733737918563573,14.79333252002546,12.517339204227589,10.711689728035747,15.892753933951489,11.85592488806984,13.360273199407175,2.79848201789475,10.852588915270818,18.618819697732381,22.205580290317254,4.8733643852768296,6.3968866915551779,4.0063764681849268],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"pink"]],"showscale":false,"z":[[-5.2517772599504369,-3.845273924496897,-2.4387705890433566,-1.0322672535898163,0.37423608186372337,1.7807394173172628,3.1872427527708043,4.5937460882243437,6.0002494236778832,7.4067527591314226,8.813256094584963,10.219759430038504,11.626262765492045,13.032766100945583,14.439269436399124,15.845772771852662,17.252276107306205,18.658779442759744,20.065282778213284,21.471786113666823],[-4.7231644904891299,-3.31666115503559,-1.9101578195820497,-0.50365448412850922,0.90284885132503034,2.30935218677857,3.7158555222321112,5.1223588576856507,6.5288621931391901,7.9353655285927296,9.341868864046269,10.74837219949981,12.154875534953351,13.561378870406889,14.96788220586043,16.37438554131397,17.780888876767509,19.187392212221052,20.593895547674588,22.000398883128131],[-4.194551721027822,-2.7880483855742821,-1.3815450501207418,0.024958285332798636,1.4314616207863382,2.8379649562398779,4.2444682916934191,5.6509716271469586,7.057474962600498,8.4639782980540375,9.8704816335075769,11.276984968961118,12.683488304414659,14.089991639868197,15.496494975321738,16.902998310775278,18.309501646228817,19.71600498168236,21.122508317135896,22.529011652589439],[-3.6659389515665137,-2.2594356161129743,-0.85293228065943394,0.55357105479410651,1.9600743902476461,3.3665777257011857,4.773081061154727,6.1795843966082664,7.5860877320618059,8.9925910675153453,10.399094402968885,11.805597738422426,13.212101073875967,14.618604409329505,16.025107744783046,17.431611080236586,18.838114415690125,20.244617751143668,21.651121086597204,23.057624422050747],[-3.1373261821052059,-1.7308228466516666,-0.32431951119812608,1.0821838242554143,2.4886871597089537,3.8951904951624936,5.3016938306160348,6.7081971660695743,8.1147005015231137,9.5212038369766532,10.927707172430193,12.334210507883734,13.740713843337275,15.147217178790813,16.553720514244354,17.960223849697893,19.366727185151433,20.773230520604976,22.179733856058512,23.586237191512055],[-2.6087134126438989,-1.2022100771903597,0.2042932582631809,1.6107965937167212,3.0172999291702607,4.4238032646238006,5.8303066000773418,7.2368099355308813,8.6433132709844216,10.049816606437961,11.4563199418915,12.862823277345042,14.269326612798583,15.675829948252121,17.082333283705662,18.488836619159201,19.895339954612741,21.301843290066284,22.70834662551982,24.114849960973363],[-2.080100643182591,-0.67359730772905169,0.73290602772448876,2.1394093631780291,3.5459126986315685,4.9524160340851084,6.3589193695386497,7.7654227049921891,9.1719260404457295,10.578429375899269,11.984932711352808,13.39143604680635,14.797939382259891,16.204442717713427,17.61094605316697,19.017449388620509,20.423952724074049,21.830456059527592,23.236959394981128,24.643462730434671],[-1.5514878737212834,-0.14498453826774382,1.2615187971857966,2.668022132639337,4.0745254680928769,5.4810288035464163,6.8875321389999575,8.2940354744534979,9.7005388099070373,11.107042145360577,12.513545480814116,13.920048816267657,15.326552151721199,16.733055487174735,18.139558822628278,19.546062158081817,20.952565493535356,22.359068828988899,23.765572164442435,25.172075499895978],[-1.0228751042599755,0.38362823119356404,1.7901315666471045,3.1966349021006448,4.6031382375541847,6.0096415730077242,7.4161449084612654,8.8226482439148057,10.229151579368345,11.635654914821885,13.042158250275424,14.448661585728965,15.855164921182507,17.261668256636042,18.668171592089585,20.074674927543125,21.481178262996664,22.887681598450207,24.294184933903743,25.700688269357286],[-0.49426233479866943,0.91224100065487013,2.3187443361084106,3.7252476715619509,5.1317510070154908,6.5382543424690303,7.9447576779225715,9.3512610133761118,10.757764348829651,12.164267684283191,13.57077101973673,14.977274355190271,16.383777690643811,17.79028102609735,19.19678436155089,20.603287697004429,22.009791032457972,23.416294367911512,24.822797703365051,26.229301038818591],[0.034350434662638436,1.4408537701161779,2.8473571055697184,4.2538604410232592,5.6603637764767987,7.0668671119303381,8.4733704473838785,9.8798737828374197,11.286377118290959,12.692880453744499,14.099383789198038,15.505887124651579,16.912390460105119,18.318893795558658,19.725397131012198,21.131900466465737,22.53840380191928,23.94490713737282,25.351410472826359,26.757913808279898],[0.56296320412394629,1.9694665395774857,3.3759698750310263,4.7824732104845671,6.1889765459381065,7.595479881391646,9.0019832168451863,10.408486552298728,11.814989887752267,13.221493223205806,14.627996558659346,16.034499894112887,17.441003229566427,18.847506565019966,20.254009900473505,21.660513235927045,23.067016571380588,24.473519906834127,25.880023242287667,27.286526577741206],[1.0915759735852542,2.4980793090387938,3.9045826444923342,5.3110859799458749,6.7175893153994144,8.124092650852953,9.5305959863064942,10.937099321760035,12.343602657213575,13.750105992667114,15.156609328120654,16.563112663574195,17.969615999027734,19.376119334481274,20.782622669934813,22.189126005388353,23.595629340841896,25.002132676295435,26.408636011748975,27.815139347202514],[1.620188743046562,3.0266920785001017,4.4331954139536416,5.8396987494071828,7.2462020848607223,8.6527054203142608,10.059208755767802,11.465712091221343,12.872215426674883,14.278718762128422,15.685222097581962,17.091725433035503,18.498228768489042,19.904732103942582,21.311235439396121,22.717738774849661,24.124242110303204,25.530745445756743,26.937248781210283,28.343752116663822],[2.1488015125078701,3.5553048479614096,4.9618081834149494,6.3683115188684907,7.7748148543220301,9.1813181897755687,10.58782152522911,11.994324860682651,13.400828196136191,14.80733153158973,16.213834867043268,17.620338202496811,19.02684153795035,20.43334487340389,21.839848208857429,23.246351544310969,24.652854879764512,26.059358215218051,27.46586155067159,28.87236488612513],[2.6774142819691762,4.0839176174227152,5.4904209528762555,6.8969242883297968,8.3034276237833353,9.7099309592368748,11.116434294690416,12.522937630143957,13.929440965597497,15.335944301051036,16.742447636504576,18.148950971958115,19.555454307411658,20.961957642865194,22.368460978318737,23.774964313772276,25.181467649225816,26.587970984679359,27.994474320132895,29.400977655586438],[3.2060270514304841,4.6125303868840231,6.0190337223375634,7.4255370577911046,8.8320403932446432,10.238543728698183,11.645047064151724,13.051550399605265,14.458053735058805,15.864557070512344,17.271060405965883,18.677563741419423,20.084067076872966,21.490570412326502,22.897073747780045,24.303577083233584,25.710080418687124,27.116583754140667,28.523087089594203,29.929590425047746],[3.7346398208917919,5.1411431563453309,6.5476464917988713,7.9541498272524125,9.360653162705951,10.76715649815949,12.173659833613032,13.580163169066573,14.986666504520112,16.393169839973652,17.799673175427191,19.206176510880731,20.612679846334274,22.01918318178781,23.425686517241353,24.832189852694892,26.238693188148432,27.645196523601975,29.05169985905551,30.458203194509053],[4.2632525903530993,5.6697559258066388,7.0762592612601791,8.4827625967137195,9.8892659321672589,11.295769267620798,12.70227260307434,14.108775938527881,15.51527927398142,16.92178260943496,18.328285944888499,19.734789280342039,21.141292615795582,22.547795951249118,23.954299286702661,25.3608026221562,26.767305957609739,28.173809293063282,29.580312628516818,30.986815963970361],[4.7918653598144072,6.1983686952679466,7.604872030721487,9.0113753661750273,10.417878701628567,11.824382037082106,13.230885372535647,14.637388707989189,16.043892043442728,17.450395378896268,18.856898714349807,20.263402049803346,21.669905385256889,23.076408720710425,24.482912056163968,25.889415391617508,27.295918727071047,28.70242206252459,30.108925397978126,31.515428733431669]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"opacity":0.69999999999999996,"name":"Fitted Plane","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>This is, of course, a very idealized situation. There is no curvature in the plane,
no interaction, no outliers, no heteroscadasticity. It’s the simplest case of multiple regression
with 2 predictors. Reality is - usually - more complicated.</p>
<p>Let’s look at summary output and check model assumptions:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="multiple-linear-regression.html#cb131-1" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2, data = d)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7460 -1.3215 -0.2489  1.2427  4.1597 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.27013    0.19228   53.41   &lt;2e-16 ***
## X1           0.44673    0.04195   10.65   &lt;2e-16 ***
## X2           1.00952    0.03960   25.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.903 on 97 degrees of freedom
## Multiple R-squared:  0.8839, Adjusted R-squared:  0.8815 
## F-statistic: 369.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="multiple-linear-regression.html#cb133-1" tabindex="-1"></a><span class="fu">check_model</span>(m4<span class="fl">.3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<p>We could repeat this simulation to get a feeling for the variability.
The posterior predictive checks look nice. In this case, we <em>know</em> that the model is true
and with this knowledge we can assess the diagnostic plots in front of us.</p>
<div id="adding-variables-to-the-model-and-why" class="section level4 hasAnchor" number="4.2.2.1">
<h4><span class="header-section-number">4.2.2.1</span> Adding variables to the model and why<a href="multiple-linear-regression.html#adding-variables-to-the-model-and-why" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This is a very complex question. We will go into it in later chapters and the next course (Methodenvertiefung).
At this point we can say this:
Depending on the goal at hand (prediction or explanation), we add variables to the model and probably use
other models apart from linear regression.
Prediction seems to be easier than explanation. For instance, within linear models and
just a handful of predictors, one can even brute force the problem by searching through
all subsets of predictors. If that is not possible, one could use clever algortithms, like
<a href="https://www.sthda.com/english/articles/37-model-selection-essentials-in-r/155-best-subsets-regression-essentials-in-r/">best subest selection</a>.</p>
<ul>
<li>What is <strong>not</strong> a good idea is to throw all variables into the model and hope for the best.</li>
<li>What is also not a good idea is to select variables depending on the <span class="math inline">\(p\)</span>-values of the coefficients.</li>
<li><strong>Leaving variables out</strong>, that are important, can lead to biased estimates of the coefficients
(<a href="https://en.wikipedia.org/wiki/Omitted-variable_bias">omitted variable bias</a>).</li>
<li>Importantly, also <strong>adding variables</strong> can hurt conclusions from the model (see Statistical Rethinking 6.2).</li>
</ul>
</div>
</div>
<div id="interaction_term" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Interaction Term <span class="math inline">\(X_1 \times X_2\)</span><a href="multiple-linear-regression.html#interaction_term" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>I recommend reading the excellent explanations about interactions
in John Kruschke’s book <a href="https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf">Doing Bayesian Data Analysis</a>,
15.2.2 und 15.2.3. Peter Westfall also has a nice explanation in his <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">book</a>
in section 9.3.</p>
<p>Our statistical model is now:</p>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \mathbf{\beta_3 X_{1i} \times X_{2i}} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, \sigma^2)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1} \times x_{2}\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>for example:</p>
<p><span class="math display">\[ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + 0.89 \cdot X_{1i} \times X_{2i} + \varepsilon_i\]</span>
<span class="math display">\[ \varepsilon_i \sim N(0, 5)\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2 + 0.89 x_1 \times x_2\]</span>
<span class="math display">\[ i = 1 \ldots n\]</span></p>
<p>The second equation states that the conditional expectation of <span class="math inline">\(Y_i\)</span> given <span class="math inline">\(X_1=x_1\)</span> and <span class="math inline">\(X_2=x_2\)</span>
is a function of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> and their interaction <span class="math inline">\(x_1 \times x_2\)</span>. We are in a different situation now.
Set for instance <span class="math inline">\(x_2\)</span> to a certain value, say <span class="math inline">\(x_2 = 7\)</span>. Then the relationship (in expectation)
between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> is:</p>
<p><span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + 0.5 x_1 + 1 \cdot 7 + 0.89 x_1 \cdot 7\]</span>
<span class="math display">\[ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + (0.5 + 0.89 \cdot \mathbf{7}) \cdot x_1 + 1 \cdot 7\]</span></p>
<p>Depending on the value of <span class="math inline">\(x_2\)</span>, the <em>effect</em> of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(Y\)</span> changes.
Hence, <span class="math inline">\(X_2\)</span> <strong>modifies</strong> the relationship between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span>, or stated otherwise,
<span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> <strong>interact</strong> with respect to <span class="math inline">\(Y\)</span>. Remember, the word <em>effect</em> is
used in a strictly technical/statistical sense and <strong>not in a causal</strong> sense.
It does not mean that if we <em>do</em> change <span class="math inline">\(X_1\)</span> by one unit,
<span class="math inline">\(Y\)</span> will also change in an experiment. We are purely describing the relationship
in an associative way. We will probably touch causality in a later chapter.
Bayesian statistics and causal inference are gaining popularity. Hence, we should try to keep up.</p>
<p>Let’s draw 100 points from this model, fit the model and add the plane (see also <a href="multiple-linear-regression.html#exercise3_multiple_regression">exercise 4</a>):</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="multiple-linear-regression.html#cb134-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb134-2"><a href="multiple-linear-regression.html#cb134-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb134-3"><a href="multiple-linear-regression.html#cb134-3" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb134-4"><a href="multiple-linear-regression.html#cb134-4" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb134-5"><a href="multiple-linear-regression.html#cb134-5" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fl">0.89</span> <span class="sc">*</span> X1 <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb134-6"><a href="multiple-linear-regression.html#cb134-6" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb134-7"><a href="multiple-linear-regression.html#cb134-7" tabindex="-1"></a></span>
<span id="cb134-8"><a href="multiple-linear-regression.html#cb134-8" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb134-9"><a href="multiple-linear-regression.html#cb134-9" tabindex="-1"></a>m4<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">*</span> X2, <span class="at">data =</span> d)</span>
<span id="cb134-10"><a href="multiple-linear-regression.html#cb134-10" tabindex="-1"></a><span class="fu">summary</span>(m4<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 * X2, data = d)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -9.360 -3.389 -0.543  2.949 11.583 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.70491    0.47888  22.354  &lt; 2e-16 ***
## X1           0.40719    0.10834   3.759 0.000293 ***
## X2           1.03434    0.09881  10.468  &lt; 2e-16 ***
## X1:X2        0.92182    0.02290  40.257  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.734 on 96 degrees of freedom
## Multiple R-squared:  0.9476, Adjusted R-squared:  0.9459 
## F-statistic: 578.1 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="multiple-linear-regression.html#cb136-1" tabindex="-1"></a><span class="co"># Create a grid for the plane</span></span>
<span id="cb136-2"><a href="multiple-linear-regression.html#cb136-2" tabindex="-1"></a>X1_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X1), <span class="fu">max</span>(d<span class="sc">$</span>X1), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb136-3"><a href="multiple-linear-regression.html#cb136-3" tabindex="-1"></a>X2_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(d<span class="sc">$</span>X2), <span class="fu">max</span>(d<span class="sc">$</span>X2), <span class="at">length.out =</span> <span class="dv">20</span>)</span>
<span id="cb136-4"><a href="multiple-linear-regression.html#cb136-4" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> X1_grid, <span class="at">X2 =</span> X2_grid)</span>
<span id="cb136-5"><a href="multiple-linear-regression.html#cb136-5" tabindex="-1"></a></span>
<span id="cb136-6"><a href="multiple-linear-regression.html#cb136-6" tabindex="-1"></a><span class="co"># Predict the values for the grid</span></span>
<span id="cb136-7"><a href="multiple-linear-regression.html#cb136-7" tabindex="-1"></a>grid<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">predict</span>(m4<span class="fl">.4</span>, <span class="at">newdata =</span> grid)</span>
<span id="cb136-8"><a href="multiple-linear-regression.html#cb136-8" tabindex="-1"></a></span>
<span id="cb136-9"><a href="multiple-linear-regression.html#cb136-9" tabindex="-1"></a><span class="co"># Convert the grid into a matrix for the plane</span></span>
<span id="cb136-10"><a href="multiple-linear-regression.html#cb136-10" tabindex="-1"></a>plane_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(grid<span class="sc">$</span>Y, <span class="at">nrow =</span> <span class="fu">length</span>(X1_grid), <span class="at">ncol =</span> <span class="fu">length</span>(X2_grid))</span>
<span id="cb136-11"><a href="multiple-linear-regression.html#cb136-11" tabindex="-1"></a></span>
<span id="cb136-12"><a href="multiple-linear-regression.html#cb136-12" tabindex="-1"></a><span class="co"># Create the interactive 3D plot</span></span>
<span id="cb136-13"><a href="multiple-linear-regression.html#cb136-13" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb136-14"><a href="multiple-linear-regression.html#cb136-14" tabindex="-1"></a>  <span class="fu">add_markers</span>(</span>
<span id="cb136-15"><a href="multiple-linear-regression.html#cb136-15" tabindex="-1"></a>    <span class="at">x =</span> d<span class="sc">$</span>X2, <span class="at">y =</span> d<span class="sc">$</span>X1, <span class="at">z =</span> d<span class="sc">$</span>Y,</span>
<span id="cb136-16"><a href="multiple-linear-regression.html#cb136-16" tabindex="-1"></a>    <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb136-17"><a href="multiple-linear-regression.html#cb136-17" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span></span>
<span id="cb136-18"><a href="multiple-linear-regression.html#cb136-18" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb136-19"><a href="multiple-linear-regression.html#cb136-19" tabindex="-1"></a>  <span class="fu">add_surface</span>(</span>
<span id="cb136-20"><a href="multiple-linear-regression.html#cb136-20" tabindex="-1"></a>    <span class="at">x =</span> X1_grid, <span class="at">y =</span> X2_grid, <span class="at">z =</span> plane_matrix,</span>
<span id="cb136-21"><a href="multiple-linear-regression.html#cb136-21" tabindex="-1"></a>    <span class="at">colorscale =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;pink&quot;</span>)),</span>
<span id="cb136-22"><a href="multiple-linear-regression.html#cb136-22" tabindex="-1"></a>    <span class="at">showscale =</span> <span class="cn">FALSE</span>,</span>
<span id="cb136-23"><a href="multiple-linear-regression.html#cb136-23" tabindex="-1"></a>    <span class="at">opacity =</span> <span class="fl">0.7</span>,</span>
<span id="cb136-24"><a href="multiple-linear-regression.html#cb136-24" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">&quot;Fitted Plane&quot;</span></span>
<span id="cb136-25"><a href="multiple-linear-regression.html#cb136-25" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span></span>
<span id="cb136-26"><a href="multiple-linear-regression.html#cb136-26" tabindex="-1"></a>  plotly<span class="sc">::</span><span class="fu">layout</span>(</span>
<span id="cb136-27"><a href="multiple-linear-regression.html#cb136-27" tabindex="-1"></a>    <span class="at">scene =</span> <span class="fu">list</span>(</span>
<span id="cb136-28"><a href="multiple-linear-regression.html#cb136-28" tabindex="-1"></a>      <span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb136-29"><a href="multiple-linear-regression.html#cb136-29" tabindex="-1"></a>      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb136-30"><a href="multiple-linear-regression.html#cb136-30" tabindex="-1"></a>      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb136-31"><a href="multiple-linear-regression.html#cb136-31" tabindex="-1"></a>    ),</span>
<span id="cb136-32"><a href="multiple-linear-regression.html#cb136-32" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Interactive 3D Scatterplot with Fitted Plane&quot;</span></span>
<span id="cb136-33"><a href="multiple-linear-regression.html#cb136-33" tabindex="-1"></a>  )</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-0ca8fe38b5204c333a18" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-0ca8fe38b5204c333a18">{"x":{"visdat":{"a35a36c06443":["function () ","plotlyVisDat"]},"cur_data":"a35a36c06443","attrs":{"a35a36c06443":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[24.900013922362266,15.955422852644634,2.7820009352490125,10.609299378353359,0.75595120508472347,9.9630225678275242,-4.7647440018698264,42.473134146781916,20.447075259121547,4.097936569996989,-4.8903504444429711,20.025069148008328,-5.3520322201328261,7.2817458867592117,-0.17885311360416445,36.325035818677819,10.737784581218953,26.300118496630901,-11.936277328943131,8.0472331765234948,2.2536317108471851,12.402815512497188,21.730181511740437,14.588512334386017,-9.8043916392185455,27.289246624957791,14.136203165018296,7.4531916776504197,31.126248517654151,5.7107250612018614,41.772480646442276,8.1039657440360511,14.337890281588734,-1.8647306915661703,-38.617953729844096,28.12783274192989,-14.834936799323591,14.620566293554468,7.405587209546626,10.144922605230985,-3.0185977041701286,6.8711931559023345,50.719918674049417,-70.934222069539814,-38.92776895811437,27.316416239935116,14.28258682804462,-1.6689540385652979,55.572371054635738,8.1711171252685162,17.133937726262264,10.475993548160442,9.517373279356951,-21.87437437112408,17.430912111860906,2.4854232187703431,-5.0534153715020311,7.9085559776505816,17.318604241938697,-0.79730899298882818,22.499300554440318,12.775287394482241,12.444597445583575,-43.292913122856767,26.642579663547046,22.001948555227052,19.985915742248498,-1.6395733394024781,23.552321907871388,34.246160195042805,14.273712122329943,6.0114915628604297,15.003211013005902,-21.693482705696752,20.170028293665311,-20.158337233206346,10.111588146619228,0.44040731140359535,16.537646691015635,8.9008212730779004,-3.7759706045968535,31.790247639458684,12.138862504610398,-6.4540663211198979,10.017254758671459,9.0581868933193217,46.484032548424409,20.536364268491919,6.3918067737995958,-1.546502148104872,24.138317936820194,11.056704900089466,17.298469528461155,13.575119078116852,-22.82384617172664,-7.8554119089381178,57.041528424468389,-41.848430890569404,9.6649950298198188,34.829954351043469],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5},"name":"Data Points","inherit":true},"a35a36c06443.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":[[104.65039627750222,91.263006074593648,77.875615871685056,64.488225668776479,51.100835465867902,37.713445262959318,24.326055060050731,10.938664857142152,-2.4487253457664253,-15.836115548675004,-29.223505751583577,-42.610895954492172,-55.998286157400777,-69.385676360309333,-82.773066563217924,-96.160456766126487,-109.54784696903508,-122.93523717194367,-136.32262737485223,-149.71001757776082],[93.933996537108982,82.066320422806172,70.198644308503347,58.330968194200537,46.463292079897727,34.595615965594916,22.727939851292092,10.860263736989284,-1.0074123773135248,-12.875088491616333,-24.742764605919142,-36.610440720221973,-48.478116834524791,-60.345792948827587,-72.213469063130404,-84.0811451774332,-95.948821291736024,-107.81649740603885,-119.68417352034164,-131.55184963464447],[83.217596796715711,72.869634771018667,62.521672745321624,52.17371071962458,41.825748693927537,31.477786668230504,21.129824642533453,10.781862616836415,0.43390059113937662,-9.9140614345576612,-20.262023460254699,-30.609985485951754,-40.957947511648804,-51.305909537345833,-61.653871563042877,-72.001833588739913,-82.349795614436957,-92.697757640134014,-103.04571966583103,-113.39368169152807],[72.501197056322454,63.672949119231184,54.844701182139907,46.016453245048638,37.188205307957368,28.359957370866095,19.531709433774814,10.703461496683545,1.8752135595922754,-6.9530343774989918,-15.781282314590266,-24.609530251681544,-33.437778188772825,-42.266026125864087,-51.094274062955364,-59.922522000046627,-68.750769937137903,-77.579017874229194,-86.407265811320443,-95.235513748411734],[61.784797315929197,54.476263467443694,47.167729618958191,39.859195770472688,32.550661921987185,25.242128073501686,17.933594225016179,10.625060376530678,3.3165265280451779,-3.9920073204403237,-11.300541168925823,-18.609075017411335,-25.917608865896842,-33.226142714382334,-40.534676562867844,-47.843210411353333,-55.151744259838843,-62.46027810832436,-69.768811956809841,-77.077345805295352],[51.06839757553594,45.279577815656204,39.490758055776475,33.701938295896738,27.913118536017009,22.12429877613728,16.335479016257541,10.54665925637781,4.7578394964980779,-1.0309802633816521,-6.819800023261382,-12.608619783141119,-18.397439543020859,-24.186259302900581,-29.975079062780321,-35.763898822660039,-41.552718582539789,-47.341538342419526,-53.13035810229924,-58.919177862178984],[40.35199783514269,36.082892163868728,31.813786492594758,27.544680821320792,23.275575150046834,19.006469478772871,14.737363807498902,10.468258136224941,6.199152464950977,1.9300467936770163,-2.3390588775969454,-6.6081645488709135,-10.877270220144883,-15.146375891418838,-19.415481562692808,-23.684587233966763,-27.953692905240725,-32.222798576514698,-36.491904247788653,-40.76100991906263],[29.635598094749415,26.886206512081227,24.136814929413035,21.387423346744839,18.638031764076647,15.888640181408457,13.139248598740263,10.389857016072071,7.6404654334038788,4.8910738507356877,2.1416822680674952,-0.60770931460069832,-3.3571008972688974,-6.1064924799370832,-8.8558840626052788,-11.60527564527346,-14.354667227941663,-17.104058810609857,-19.853450393278045,-22.602841975946241],[18.919198354356169,17.689520860293744,16.459843366231318,15.230165872168897,14.000488378106473,12.77081088404405,11.541133389981624,10.311455895919202,9.0817784018567789,7.8521009077943562,6.6224234137319327,5.3927459196695091,4.1630684256070829,2.9333909315446629,1.7037134374822367,0.47403594341981381,-0.75564155064260885,-1.9853190447050315,-3.2149965387674579,-4.444674032829881],[8.2027986139629157,8.4928352085062606,8.7828718030496056,9.0729083975929523,9.3629449921362973,9.6529815866796422,9.9430181812229872,10.233054775766334,10.523091370309677,10.813127964853024,11.103164559396369,11.393201153939716,11.683237748483059,11.973274343026405,12.26331093756975,12.553347532113097,12.84338412665644,13.133420721199787,13.423457315743134,13.713493910286475],[-2.5136011264303506,-0.70385044328123481,1.1059002398678821,2.9156509230169991,4.7254016061661153,6.5351522893152314,8.3449029724643484,10.154653655613465,11.964404338762581,13.774155021911698,15.583905705060813,17.393656388209934,19.203407071359049,21.013157754508164,22.822908437657283,24.632659120806398,26.442409803955513,28.252160487104632,30.061911170253747,31.871661853402863],[-13.230000866823602,-9.900536095068718,-6.5710713233138316,-3.2416065515589447,0.087858220195938974,3.4173229919508237,6.7467877637057114,10.076252535460597,13.405717307215481,16.735182078970364,20.06464685072525,23.394111622480139,26.723576394235028,30.053041165989907,33.382505937744796,36.711970709499674,40.041435481254567,43.370900253009452,46.700365024764331,50.029829796519223],[-23.946400607216852,-19.097221746856199,-14.248042886495544,-9.3988640261348877,-4.5496851657742372,0.29949369458641656,5.1486725549470744,9.9978514153077285,14.847030275668381,19.696209136029033,24.545387996389685,29.394566856750345,34.243745717111004,39.092924577471649,43.942103437832309,48.791282298192954,53.640461158553613,58.489640018914272,63.338818879274918,68.187997739635577],[-34.662800347610123,-28.293907398643697,-21.925014449677271,-15.556121500710844,-9.1872285517444201,-2.8183356027779962,3.5505573461884339,9.9194502951548582,16.288343244121283,22.657236193087705,29.026129142054131,35.395022091020557,41.763915039986991,48.13280798895341,54.501700937919836,60.870593886886248,67.239486835852688,73.608379784819121,79.977272733785526,86.346165682751959],[-45.379200088003387,-37.490593050431194,-29.601986012858998,-21.713378975286794,-13.824771937714601,-5.9361649001424066,1.9524421374297949,9.8410491750019897,17.729656212574184,25.618263250146381,33.506870287718577,41.395477325290777,49.284084362862977,57.172691400435163,65.06129843800737,72.949905475579556,80.838512513151755,88.72711955072397,96.615726588296155,104.50433362586836],[-56.09559982839663,-46.687278702218663,-37.278957576040696,-27.870636449862729,-18.46231532368477,-9.0539941975068139,0.35432692867115967,9.7626480548491212,19.170969181027079,28.579290307205042,37.987611433383002,47.395932559560968,56.804253685738942,66.212574811916895,75.620895938094861,85.029217064272814,94.437538190450795,103.84585931662876,113.25418044280671,122.66250156898468],[-66.811999568789901,-55.88396435400616,-44.95592913922242,-34.027893924438679,-23.099858709654956,-12.171823494871223,-1.2437882800874784,9.6842469346962528,20.612282149479984,31.540317364263714,42.468352579047448,53.396387793831195,64.324423008614929,75.252458223398648,86.180493438182395,97.108528652966115,108.03656386774985,118.96459908253361,129.89263429731733,140.82066951210106],[-77.528399309183158,-65.080650005793657,-52.63290070240415,-40.185151399014643,-27.737402095625136,-15.289652792235637,-2.8419034888461203,9.6058458145433843,22.053595117932883,34.501344421322386,46.949093724711886,59.396843028101408,71.844592331490915,84.292341634880401,96.74009093826993,109.18784024165942,121.63558954504893,134.08333884843844,146.53108815182793,158.97883745521744],[-88.2447990495764,-74.277335657581119,-60.309872265585852,-46.342408873590578,-32.374945481595304,-18.407482089600038,-4.440018697604752,9.5274446943905158,23.494908086385781,37.462371478381051,51.429834870376318,65.397298262371606,79.364761654366887,93.33222504636214,107.29968843835742,121.26715183035267,135.23461522234797,149.20207861434326,163.16954200633847,177.13700539833377],[-98.961198789969657,-83.474021309368624,-67.986843828767576,-52.499666348166521,-37.012488867565487,-21.525311386964454,-6.0381339063633934,9.4490435742376455,24.936221054838686,40.423398535439723,55.910576016040757,71.397753496641826,86.884930977242874,102.37210845784391,117.85928593844496,133.34646341904599,148.83364089964704,164.32081838024808,179.8079958608491,195.29517334145018]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"colorscale":[[0,1],["red","pink"]],"showscale":false,"opacity":0.69999999999999996,"name":"Fitted Plane","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"title":"Interactive 3D Scatterplot with Fitted Plane","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[-3.5520328184965049,1.2844185457826476,-1.2334593923118682,-1.737712996988666,-4.7580928363250781,-0.22513862404460133,-3.9245223472853792,-8.3397096829406827,-1.9011326014388121,4.5949830453038301,-2.8767348130419594,3.0398216111251668,-8.0894135414458201,-0.27780982762269718,2.5970360197173115,1.505766810833572,0.52838097074471668,-3.2035300415268821,-4.2485217301679112,-5.1206439530245662,0.58823298550062941,-4.7373730709240105,-2.4527872185033415,-1.2804609609912363,9.2193100261610379,-3.2597495084772925,1.1769328614242847,0.38980424781855383,-4.8092831706506427,-0.35654043061799334,7.2227542921167442,2.2575202653960731,0.20616460996469965,-2.112484161698124,-10.266236107702579,5.6566860670708774,-7.30320035462411,3.6997375543866724,9.5455178460874173,-7.2194658048589968,3.5089216768735545,-1.3109874470123399,-7.8607207957274383,-7.5733382689087572,-8.007680867872967,-2.6545326108515148,-7.3087779249794993,3.4395838648791379,10.500544702628359,-6.4351523801758947,3.9386942373758904,3.8452112050045493,1.6610128947505884,-5.0418830413850388,-0.59726303315329343,-1.4019766758512333,2.8149476661023991,-1.8621937805191453,4.8848669334281034,-1.8729042888350691,5.2635573278966596,-5.2458850333303282,-6.300776223790562,16.205199674712023,-2.0842879408021591,1.4911379577035766,3.1828483701692436,-2.4189031285437181,2.5843102215680451,1.8448226369254299,-1.0769025382084669,0.32646516762657629,-0.17033626869231999,10.642259495080905,-3.7066804813641419,-5.4799813353733198,0.18894199585539409,1.5524037472156851,2.1826173945509146,-2.2918266635555282,-5.3166306698559556,6.3159258804474501,-1.7482519397677736,-4.32756431326687,-1.1813978447054827,-0.98587947174276103,5.5496014485682021,0.42368646098598228,3.7702689259226077,-2.4964600858613037,1.0722265479080064,-1.6234295574541735,0.47291764086785709,-4.4768167898877085,-6.5540076666398592,9.9860669237398323,3.0035441183620879,-6.2563568081247203,-3.0558295834021045,-5.927400422986552],"y":[-2.8023782327610629,-1.1508874474163997,7.7935415707456199,0.35254195712288,0.64643867580473124,8.5753249344164058,2.3045810299460117,-6.3253061730326703,-3.4342642594676303,-2.2283098504997905,6.1204089871973082,1.7990691352868191,2.0038572529702607,0.55341357972559857,-2.7792056737703748,8.9345656840153911,2.4892523911461968,-9.8330857831481904,3.5067795078184276,-2.3639570386396702,-5.3391185299342254,-1.0898745732914756,-5.1300222415361985,-3.6444561464557013,-3.1251963392462834,-8.4334665537120674,4.1889352224726233,0.76686558918257608,-5.6906846850597379,6.2690746053496333,2.132321107384068,-1.4753574149613558,4.4756283052251113,4.3906674376652113,4.1079054081874355,3.4432012705004551,2.769588267687944,-0.3095585528836084,-1.5298133186995839,-1.9023550050619134,-3.4735348946025635,-1.0395863900979938,-6.3269817578413221,10.844779826692562,6.0398099915249528,-5.6155429160167456,-2.0144241764953801,-2.3332767681160944,3.8998255916815894,-0.41684533235914639,1.2665925699737741,-0.14273377674351509,-0.21435228645658044,6.8430114200722878,-1.128854928296338,7.5823530221476982,-7.7437640211511063,2.9230687481803459,0.61927121922306894,1.0797078437198635,1.8981974137994104,-2.5116172655465112,-1.6660369183471004,-5.0928769155354434,-5.3589561323778891,1.5176432070212904,2.2410488931471311,0.26502113365252072,4.6113373393986876,10.250423428135722,-2.4551558302826764,-11.545844378204063,5.0286926223112829,-3.5460038129119633,-3.44004308233679,5.1278568484834945,-1.4238650352550444,-6.1035885612726783,0.90651739874575099,-0.69445681219522315,0.028820929499434666,1.9264020056316524,-1.8533001589620468,3.2218827425941647,-1.1024328090937532,1.6589098195784846,5.4841950657467384,2.1759074541690144,-1.6296579276561338,5.7440380922554688,4.9675192798105972,2.7419847975403497,1.1936586755572058,-3.1395303801968573,6.803262242650038,-3.0012979357356344,10.936664965082883,7.6630531309259462,-1.1785017955023844,-5.1321045015339042],"z":[24.900013922362266,15.955422852644634,2.7820009352490125,10.609299378353359,0.75595120508472347,9.9630225678275242,-4.7647440018698264,42.473134146781916,20.447075259121547,4.097936569996989,-4.8903504444429711,20.025069148008328,-5.3520322201328261,7.2817458867592117,-0.17885311360416445,36.325035818677819,10.737784581218953,26.300118496630901,-11.936277328943131,8.0472331765234948,2.2536317108471851,12.402815512497188,21.730181511740437,14.588512334386017,-9.8043916392185455,27.289246624957791,14.136203165018296,7.4531916776504197,31.126248517654151,5.7107250612018614,41.772480646442276,8.1039657440360511,14.337890281588734,-1.8647306915661703,-38.617953729844096,28.12783274192989,-14.834936799323591,14.620566293554468,7.405587209546626,10.144922605230985,-3.0185977041701286,6.8711931559023345,50.719918674049417,-70.934222069539814,-38.92776895811437,27.316416239935116,14.28258682804462,-1.6689540385652979,55.572371054635738,8.1711171252685162,17.133937726262264,10.475993548160442,9.517373279356951,-21.87437437112408,17.430912111860906,2.4854232187703431,-5.0534153715020311,7.9085559776505816,17.318604241938697,-0.79730899298882818,22.499300554440318,12.775287394482241,12.444597445583575,-43.292913122856767,26.642579663547046,22.001948555227052,19.985915742248498,-1.6395733394024781,23.552321907871388,34.246160195042805,14.273712122329943,6.0114915628604297,15.003211013005902,-21.693482705696752,20.170028293665311,-20.158337233206346,10.111588146619228,0.44040731140359535,16.537646691015635,8.9008212730779004,-3.7759706045968535,31.790247639458684,12.138862504610398,-6.4540663211198979,10.017254758671459,9.0581868933193217,46.484032548424409,20.536364268491919,6.3918067737995958,-1.546502148104872,24.138317936820194,11.056704900089466,17.298469528461155,13.575119078116852,-22.82384617172664,-7.8554119089381178,57.041528424468389,-41.848430890569404,9.6649950298198188,34.829954351043469],"type":"scatter3d","mode":"markers","marker":{"color":"blue","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"","ticklen":2},"colorscale":[[0,"red"],[1,"pink"]],"showscale":false,"z":[[104.65039627750222,91.263006074593648,77.875615871685056,64.488225668776479,51.100835465867902,37.713445262959318,24.326055060050731,10.938664857142152,-2.4487253457664253,-15.836115548675004,-29.223505751583577,-42.610895954492172,-55.998286157400777,-69.385676360309333,-82.773066563217924,-96.160456766126487,-109.54784696903508,-122.93523717194367,-136.32262737485223,-149.71001757776082],[93.933996537108982,82.066320422806172,70.198644308503347,58.330968194200537,46.463292079897727,34.595615965594916,22.727939851292092,10.860263736989284,-1.0074123773135248,-12.875088491616333,-24.742764605919142,-36.610440720221973,-48.478116834524791,-60.345792948827587,-72.213469063130404,-84.0811451774332,-95.948821291736024,-107.81649740603885,-119.68417352034164,-131.55184963464447],[83.217596796715711,72.869634771018667,62.521672745321624,52.17371071962458,41.825748693927537,31.477786668230504,21.129824642533453,10.781862616836415,0.43390059113937662,-9.9140614345576612,-20.262023460254699,-30.609985485951754,-40.957947511648804,-51.305909537345833,-61.653871563042877,-72.001833588739913,-82.349795614436957,-92.697757640134014,-103.04571966583103,-113.39368169152807],[72.501197056322454,63.672949119231184,54.844701182139907,46.016453245048638,37.188205307957368,28.359957370866095,19.531709433774814,10.703461496683545,1.8752135595922754,-6.9530343774989918,-15.781282314590266,-24.609530251681544,-33.437778188772825,-42.266026125864087,-51.094274062955364,-59.922522000046627,-68.750769937137903,-77.579017874229194,-86.407265811320443,-95.235513748411734],[61.784797315929197,54.476263467443694,47.167729618958191,39.859195770472688,32.550661921987185,25.242128073501686,17.933594225016179,10.625060376530678,3.3165265280451779,-3.9920073204403237,-11.300541168925823,-18.609075017411335,-25.917608865896842,-33.226142714382334,-40.534676562867844,-47.843210411353333,-55.151744259838843,-62.46027810832436,-69.768811956809841,-77.077345805295352],[51.06839757553594,45.279577815656204,39.490758055776475,33.701938295896738,27.913118536017009,22.12429877613728,16.335479016257541,10.54665925637781,4.7578394964980779,-1.0309802633816521,-6.819800023261382,-12.608619783141119,-18.397439543020859,-24.186259302900581,-29.975079062780321,-35.763898822660039,-41.552718582539789,-47.341538342419526,-53.13035810229924,-58.919177862178984],[40.35199783514269,36.082892163868728,31.813786492594758,27.544680821320792,23.275575150046834,19.006469478772871,14.737363807498902,10.468258136224941,6.199152464950977,1.9300467936770163,-2.3390588775969454,-6.6081645488709135,-10.877270220144883,-15.146375891418838,-19.415481562692808,-23.684587233966763,-27.953692905240725,-32.222798576514698,-36.491904247788653,-40.76100991906263],[29.635598094749415,26.886206512081227,24.136814929413035,21.387423346744839,18.638031764076647,15.888640181408457,13.139248598740263,10.389857016072071,7.6404654334038788,4.8910738507356877,2.1416822680674952,-0.60770931460069832,-3.3571008972688974,-6.1064924799370832,-8.8558840626052788,-11.60527564527346,-14.354667227941663,-17.104058810609857,-19.853450393278045,-22.602841975946241],[18.919198354356169,17.689520860293744,16.459843366231318,15.230165872168897,14.000488378106473,12.77081088404405,11.541133389981624,10.311455895919202,9.0817784018567789,7.8521009077943562,6.6224234137319327,5.3927459196695091,4.1630684256070829,2.9333909315446629,1.7037134374822367,0.47403594341981381,-0.75564155064260885,-1.9853190447050315,-3.2149965387674579,-4.444674032829881],[8.2027986139629157,8.4928352085062606,8.7828718030496056,9.0729083975929523,9.3629449921362973,9.6529815866796422,9.9430181812229872,10.233054775766334,10.523091370309677,10.813127964853024,11.103164559396369,11.393201153939716,11.683237748483059,11.973274343026405,12.26331093756975,12.553347532113097,12.84338412665644,13.133420721199787,13.423457315743134,13.713493910286475],[-2.5136011264303506,-0.70385044328123481,1.1059002398678821,2.9156509230169991,4.7254016061661153,6.5351522893152314,8.3449029724643484,10.154653655613465,11.964404338762581,13.774155021911698,15.583905705060813,17.393656388209934,19.203407071359049,21.013157754508164,22.822908437657283,24.632659120806398,26.442409803955513,28.252160487104632,30.061911170253747,31.871661853402863],[-13.230000866823602,-9.900536095068718,-6.5710713233138316,-3.2416065515589447,0.087858220195938974,3.4173229919508237,6.7467877637057114,10.076252535460597,13.405717307215481,16.735182078970364,20.06464685072525,23.394111622480139,26.723576394235028,30.053041165989907,33.382505937744796,36.711970709499674,40.041435481254567,43.370900253009452,46.700365024764331,50.029829796519223],[-23.946400607216852,-19.097221746856199,-14.248042886495544,-9.3988640261348877,-4.5496851657742372,0.29949369458641656,5.1486725549470744,9.9978514153077285,14.847030275668381,19.696209136029033,24.545387996389685,29.394566856750345,34.243745717111004,39.092924577471649,43.942103437832309,48.791282298192954,53.640461158553613,58.489640018914272,63.338818879274918,68.187997739635577],[-34.662800347610123,-28.293907398643697,-21.925014449677271,-15.556121500710844,-9.1872285517444201,-2.8183356027779962,3.5505573461884339,9.9194502951548582,16.288343244121283,22.657236193087705,29.026129142054131,35.395022091020557,41.763915039986991,48.13280798895341,54.501700937919836,60.870593886886248,67.239486835852688,73.608379784819121,79.977272733785526,86.346165682751959],[-45.379200088003387,-37.490593050431194,-29.601986012858998,-21.713378975286794,-13.824771937714601,-5.9361649001424066,1.9524421374297949,9.8410491750019897,17.729656212574184,25.618263250146381,33.506870287718577,41.395477325290777,49.284084362862977,57.172691400435163,65.06129843800737,72.949905475579556,80.838512513151755,88.72711955072397,96.615726588296155,104.50433362586836],[-56.09559982839663,-46.687278702218663,-37.278957576040696,-27.870636449862729,-18.46231532368477,-9.0539941975068139,0.35432692867115967,9.7626480548491212,19.170969181027079,28.579290307205042,37.987611433383002,47.395932559560968,56.804253685738942,66.212574811916895,75.620895938094861,85.029217064272814,94.437538190450795,103.84585931662876,113.25418044280671,122.66250156898468],[-66.811999568789901,-55.88396435400616,-44.95592913922242,-34.027893924438679,-23.099858709654956,-12.171823494871223,-1.2437882800874784,9.6842469346962528,20.612282149479984,31.540317364263714,42.468352579047448,53.396387793831195,64.324423008614929,75.252458223398648,86.180493438182395,97.108528652966115,108.03656386774985,118.96459908253361,129.89263429731733,140.82066951210106],[-77.528399309183158,-65.080650005793657,-52.63290070240415,-40.185151399014643,-27.737402095625136,-15.289652792235637,-2.8419034888461203,9.6058458145433843,22.053595117932883,34.501344421322386,46.949093724711886,59.396843028101408,71.844592331490915,84.292341634880401,96.74009093826993,109.18784024165942,121.63558954504893,134.08333884843844,146.53108815182793,158.97883745521744],[-88.2447990495764,-74.277335657581119,-60.309872265585852,-46.342408873590578,-32.374945481595304,-18.407482089600038,-4.440018697604752,9.5274446943905158,23.494908086385781,37.462371478381051,51.429834870376318,65.397298262371606,79.364761654366887,93.33222504636214,107.29968843835742,121.26715183035267,135.23461522234797,149.20207861434326,163.16954200633847,177.13700539833377],[-98.961198789969657,-83.474021309368624,-67.986843828767576,-52.499666348166521,-37.012488867565487,-21.525311386964454,-6.0381339063633934,9.4490435742376455,24.936221054838686,40.423398535439723,55.910576016040757,71.397753496641826,86.884930977242874,102.37210845784391,117.85928593844496,133.34646341904599,148.83364089964704,164.32081838024808,179.8079958608491,195.29517334145018]],"type":"surface","x":[-11.545844378204063,-10.362554412767908,-9.179264447331752,-7.9959744818955976,-6.8126845164594423,-5.629394551023287,-4.4461045855871326,-3.2628146201509765,-2.0795246547148221,-0.89623468927866767,0.28705527615748849,1.4703452415936429,2.6536352070297973,3.8369251724659534,5.0202151379021096,6.2035051033382622,7.3867950687744184,8.5700850342105745,9.7533749996467272,10.936664965082883],"y":[-10.266236107702579,-8.8730026454702315,-7.4797691832378836,-6.0865357210055357,-4.6933022587731887,-3.3000687965408417,-1.9068353343084929,-0.51360187207614594,0.87963159015620107,2.2728650523885481,3.6660985146208951,5.0593319768532439,6.4525654390855927,7.8457989013179379,9.2390323635502867,10.632265825782632,12.025499288014981,13.418732750247329,14.811966212479675,16.205199674712023],"opacity":0.69999999999999996,"name":"Fitted Plane","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The term <code>X1 * X2</code> is a shortcut for <code>X1 + X2 + X1:X2</code> where <code>X1:X2</code> is the interaction term.
R automatically includes the main effects of the predictors when an interaction term is included.
The true but usually unknown <span class="math inline">\(\beta\)</span>s are estimated quite precisely.</p>
<div id="formal-test-for-interaction" class="section level4 hasAnchor" number="4.2.3.1">
<h4><span class="header-section-number">4.2.3.1</span> Formal test for interaction<a href="multiple-linear-regression.html#formal-test-for-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We could apply a formal test for the interaction term by model comparison.
The command <code>anova(., .)</code> would compare the two models and test if the change in the
residual sum of squares is statistically interesting.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="multiple-linear-regression.html#cb137-1" tabindex="-1"></a>m4<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X1 <span class="sc">+</span> X2, <span class="at">data =</span> d) <span class="co"># without interaction</span></span>
<span id="cb137-2"><a href="multiple-linear-regression.html#cb137-2" tabindex="-1"></a><span class="fu">anova</span>(m4<span class="fl">.5</span>, m4<span class="fl">.4</span>)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Y ~ X1 + X2
## Model 2: Y ~ X1 * X2
##   Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     97 38468                                  
## 2     96  2151  1     36316 1620.6 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>One can show that the following test statistic is <span class="math inline">\(F\)</span> distributed under the null hypothesis:</p>
<p><span class="math display">\[ F = \frac{\left(RSS_{\text{Model 1}} - RSS_{\text{Model 2}}\right) / \left(df_{\text{Model 1}} - df_{\text{Model 2}}\right)}{RSS_{\text{Model 2}} / df_{\text{Model 2}}}\]</span></p>
<p>where <span class="math inline">\(RSS\)</span> is the residual sum of squares,
<span class="math inline">\(df\)</span> are the degrees of freedom of the residual sum of squares for both models.</p>
<p>The output of the <code>anova</code> command shows us the residual degress of freedom (<code>Res.Df</code>)
of both models, the residual sum of squares errors of both models (<code>RSS</code>),
the sum of squared errors between model 1 and model 2 (<code>Sum of Sq</code>), the value of the
F-statistic and the <span class="math inline">\(p\)</span>-value for the hypothesis, that the coefficient for
the interaction term is zero (<span class="math inline">\(\beta_3=0\)</span>). Model 1 RSS has 97 degrees of freedom, since we have 100 data points
and 3 parameters to estimate (<span class="math inline">\(\beta_0, \beta_1, \beta_2\)</span>). Model 2 has 96 degrees of freedom, since
we have 100 data points and 4 parameters to estimate (<span class="math inline">\(\beta_0, \beta_1, \beta_2, \beta_3\)</span>).</p>
<p>Let’s verify the value of the <span class="math inline">\(F\)</span> statistic:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="multiple-linear-regression.html#cb139-1" tabindex="-1"></a>RSS_model1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(m4<span class="fl">.5</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb139-2"><a href="multiple-linear-regression.html#cb139-2" tabindex="-1"></a>RSS_model2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">residuals</span>(m4<span class="fl">.4</span>)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb139-3"><a href="multiple-linear-regression.html#cb139-3" tabindex="-1"></a>df_model1 <span class="ot">&lt;-</span> n <span class="sc">-</span> <span class="fu">length</span>(<span class="fu">coef</span>(m4<span class="fl">.5</span>))</span>
<span id="cb139-4"><a href="multiple-linear-regression.html#cb139-4" tabindex="-1"></a>df_model2 <span class="ot">&lt;-</span> n <span class="sc">-</span> <span class="fu">length</span>(<span class="fu">coef</span>(m4<span class="fl">.4</span>))</span>
<span id="cb139-5"><a href="multiple-linear-regression.html#cb139-5" tabindex="-1"></a>F <span class="ot">&lt;-</span> ((RSS_model1 <span class="sc">-</span> RSS_model2) <span class="sc">/</span> (df_model1 <span class="sc">-</span> df_model2)) <span class="sc">/</span> (RSS_model2 <span class="sc">/</span> df_model2)</span>
<span id="cb139-6"><a href="multiple-linear-regression.html#cb139-6" tabindex="-1"></a>F</span></code></pre></div>
<pre><code>## [1] 1620.606</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="multiple-linear-regression.html#cb141-1" tabindex="-1"></a><span class="co"># Sum of Sq</span></span>
<span id="cb141-2"><a href="multiple-linear-regression.html#cb141-2" tabindex="-1"></a>RSS_model1 <span class="sc">-</span> RSS_model2</span></code></pre></div>
<pre><code>## [1] 36316.28</code></pre>
<p>In the numerator of the <span class="math inline">\(F\)</span> statistic, we have the change in the residual sum of squares
(from the small (model 1) model to the larger one (model 2), <code>Sum of Sq</code>)
per additional parameter in the model (one additional parameter <span class="math inline">\(\beta_3\)</span>).</p>
<p>In the denominator, we have the residual sum of squares per residual degree of freedom of
the larger model (model 2). Hence, in the numerator we have the information on how much
better we get with respect to the number of variables added, and in the denominator
we have information on how good the full model is with respect to its degrees of freedom.</p>
<p>The <span class="math inline">\(p\)</span>-value is the probability of observing a value of the F statistic as extreme or more
extreme than the one we observed, given that the null hypothesis is true. Here,
the <span class="math inline">\(p\)</span>-value is extremely small. So, statistically we would see an improvement in RSS
which is not explainable by chance alone.
But <strong>let’s be careful with <span class="math inline">\(p\)</span>-values</strong> and especially with fixed cutoff values for <span class="math inline">\(\alpha\)</span>,
which we will <strong>never</strong> use in this script.
Even for a rather small effect <span class="math inline">\(\beta_3\)</span>, we would reject the null hypothesis, if only the sample
size is large enough. Since a very small effect relative to <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> would
probably not be of practical interest, one should be careful with looking at <span class="math inline">\(p\)</span>-values alone.
For instance, in Richard McElreath’s book <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a>,
there are no <span class="math inline">\(p\)</span>-values at all. I like that.</p>
<p>If you again look at the comparison of the RSS between the two models, you would
immediately see that the model with the interaction term is better (at least with respect to this metric).
The difference is huge. We have already mentioned in the context of <span class="math inline">\(R^2\)</span> not to overinterpret
such metric, because RSS is monotonically descreasing with number of variables added and reaches
zero when the number of variables equals the number of data points (see <a href="multiple-linear-regression.html#exercise3_multiple_regression">exercise 3</a>).</p>
</div>
</div>
<div id="interaction_plot" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Using an interaction plot to see a potential interaction<a href="multiple-linear-regression.html#interaction_plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chronologically before we include an interaction term in the model, we can use an interaction plot
to see if there is a potential interaction between the predictors.
We can just create a catorical predictor out of the continuous predictors.
We just categorize the predictors into quartiles and plot the means of the dependent variable (<span class="math inline">\(Y\)</span>).
If the lines are parallel, there is no interaction. If the lines are not parallel,
there might be an interaction.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="multiple-linear-regression.html#cb143-1" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb143-2"><a href="multiple-linear-regression.html#cb143-2" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb143-3"><a href="multiple-linear-regression.html#cb143-3" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb143-4"><a href="multiple-linear-regression.html#cb143-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">1</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fl">0.89</span> <span class="sc">*</span> X1 <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb143-5"><a href="multiple-linear-regression.html#cb143-5" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">X1 =</span> X1, <span class="at">X2 =</span> X2, <span class="at">Y =</span> Y)</span>
<span id="cb143-6"><a href="multiple-linear-regression.html#cb143-6" tabindex="-1"></a></span>
<span id="cb143-7"><a href="multiple-linear-regression.html#cb143-7" tabindex="-1"></a><span class="co"># Create categorical variables based on quartiles</span></span>
<span id="cb143-8"><a href="multiple-linear-regression.html#cb143-8" tabindex="-1"></a>d<span class="sc">$</span>X2_cat <span class="ot">&lt;-</span> <span class="fu">cut</span>(d<span class="sc">$</span>X2, </span>
<span id="cb143-9"><a href="multiple-linear-regression.html#cb143-9" tabindex="-1"></a>                <span class="at">breaks =</span> <span class="fu">quantile</span>(d<span class="sc">$</span>X2, <span class="at">probs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>), <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb143-10"><a href="multiple-linear-regression.html#cb143-10" tabindex="-1"></a>                <span class="at">include.lowest =</span> <span class="cn">TRUE</span>, </span>
<span id="cb143-11"><a href="multiple-linear-regression.html#cb143-11" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Q1&quot;</span>, <span class="st">&quot;Q2&quot;</span>, <span class="st">&quot;Q3&quot;</span>, <span class="st">&quot;Q4&quot;</span>))</span>
<span id="cb143-12"><a href="multiple-linear-regression.html#cb143-12" tabindex="-1"></a></span>
<span id="cb143-13"><a href="multiple-linear-regression.html#cb143-13" tabindex="-1"></a>d<span class="sc">$</span>X1_cat <span class="ot">&lt;-</span> <span class="fu">cut</span>(d<span class="sc">$</span>X1, </span>
<span id="cb143-14"><a href="multiple-linear-regression.html#cb143-14" tabindex="-1"></a>                <span class="at">breaks =</span> <span class="fu">quantile</span>(d<span class="sc">$</span>X1, <span class="at">probs =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>), <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb143-15"><a href="multiple-linear-regression.html#cb143-15" tabindex="-1"></a>                <span class="at">include.lowest =</span> <span class="cn">TRUE</span>, </span>
<span id="cb143-16"><a href="multiple-linear-regression.html#cb143-16" tabindex="-1"></a>                <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Q1&quot;</span>, <span class="st">&quot;Q2&quot;</span>, <span class="st">&quot;Q3&quot;</span>, <span class="st">&quot;Q4&quot;</span>))</span>
<span id="cb143-17"><a href="multiple-linear-regression.html#cb143-17" tabindex="-1"></a></span>
<span id="cb143-18"><a href="multiple-linear-regression.html#cb143-18" tabindex="-1"></a><span class="co"># Create the interaction plot</span></span>
<span id="cb143-19"><a href="multiple-linear-regression.html#cb143-19" tabindex="-1"></a><span class="fu">interaction.plot</span>(d<span class="sc">$</span>X2_cat, d<span class="sc">$</span>X1_cat, d<span class="sc">$</span>Y)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>There seems to be an intercation of the predictors with respect to <span class="math inline">\(Y\)</span>. The lines are not parallel.
If there was no interaction, the change in <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X_2\)</span> would be the same
for all levels of <span class="math inline">\(X_1\)</span>.
This seems not to be the case here.
See <a href="multiple-linear-regression.html#exercise5_multiple_regression">exercise 5</a>.</p>
<p>If we had one or both predictors already categorical, we would not have to discretize them before.</p>
</div>
<div id="simpsons_paradox" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> Simpsons Paradox<a href="multiple-linear-regression.html#simpsons_paradox" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Simpson%27s_paradox">Simpsons paradox</a> is a phenomenon,
in which a trend appears in several different groups of data but disappears
or reverses when these groups are combined. I agree with the criticism that this is not really
a paradox but a failure to consider confounding variables adequately. Let’s quickly invent an example.
We are interested in the relationship hours of muscle training and strength (not based on evidence)
in children vs. adults. Within both groups there will be an increasing relationship. The more training,
the more muscle strength. But if we combine the groups, we will see a decreasing relationship.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="multiple-linear-regression.html#cb144-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb144-2"><a href="multiple-linear-regression.html#cb144-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb144-3"><a href="multiple-linear-regression.html#cb144-3" tabindex="-1"></a>age <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">&quot;child&quot;</span>, n<span class="sc">/</span><span class="dv">2</span>), <span class="fu">rep</span>(<span class="st">&quot;adult&quot;</span>, n<span class="sc">/</span><span class="dv">2</span>))</span>
<span id="cb144-4"><a href="multiple-linear-regression.html#cb144-4" tabindex="-1"></a>training <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">5</span>) <span class="sc">+</span> <span class="dv">30</span>, <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">5</span>)<span class="sc">+</span> <span class="dv">10</span>)</span>
<span id="cb144-5"><a href="multiple-linear-regression.html#cb144-5" tabindex="-1"></a>strength <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb144-6"><a href="multiple-linear-regression.html#cb144-6" tabindex="-1"></a>  <span class="dv">10</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> training[<span class="dv">1</span><span class="sc">:</span>(n<span class="sc">/</span><span class="dv">2</span>)] <span class="sc">+</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>), <span class="co"># For children</span></span>
<span id="cb144-7"><a href="multiple-linear-regression.html#cb144-7" tabindex="-1"></a>  <span class="dv">25</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> training[(n<span class="sc">/</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>n] <span class="sc">+</span> <span class="fu">rnorm</span>(n<span class="sc">/</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>) <span class="co"># For adults</span></span>
<span id="cb144-8"><a href="multiple-linear-regression.html#cb144-8" tabindex="-1"></a>)</span>
<span id="cb144-9"><a href="multiple-linear-regression.html#cb144-9" tabindex="-1"></a></span>
<span id="cb144-10"><a href="multiple-linear-regression.html#cb144-10" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">age =</span> age, <span class="at">training =</span> training, <span class="at">strength =</span> strength)</span>
<span id="cb144-11"><a href="multiple-linear-regression.html#cb144-11" tabindex="-1"></a></span>
<span id="cb144-12"><a href="multiple-linear-regression.html#cb144-12" tabindex="-1"></a><span class="fu">ggplot</span>(d, <span class="fu">aes</span>(<span class="at">x =</span> training, <span class="at">y =</span> strength, <span class="at">color =</span> age)) <span class="sc">+</span></span>
<span id="cb144-13"><a href="multiple-linear-regression.html#cb144-13" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb144-14"><a href="multiple-linear-regression.html#cb144-14" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> <span class="co"># Group-specific regression lines</span></span>
<span id="cb144-15"><a href="multiple-linear-regression.html#cb144-15" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">data =</span> d, <span class="fu">aes</span>(<span class="at">x =</span> training, <span class="at">y =</span> strength), </span>
<span id="cb144-16"><a href="multiple-linear-regression.html#cb144-16" tabindex="-1"></a>              <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span> <span class="co"># Overall regression line</span></span>
<span id="cb144-17"><a href="multiple-linear-regression.html#cb144-17" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Regression Lines for Training and Strength&quot;</span>,</span>
<span id="cb144-18"><a href="multiple-linear-regression.html#cb144-18" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Training&quot;</span>,</span>
<span id="cb144-19"><a href="multiple-linear-regression.html#cb144-19" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Strength&quot;</span>) <span class="sc">+</span></span>
<span id="cb144-20"><a href="multiple-linear-regression.html#cb144-20" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb144-21"><a href="multiple-linear-regression.html#cb144-21" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;
## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<ul>
<li><strong>Group-Specific Trends</strong>:
<ul>
<li>In the group of <strong>children</strong> (blue line), strength <strong>increases</strong> with training, as indicated by the positive slope of the regression line.</li>
<li>Similarly, in the group of <strong>adults</strong> (red line), strength also <strong>increases</strong> with training.</li>
</ul></li>
<li><strong>Overall Trend</strong>:
<ul>
<li>When both groups are combined, the overall regression line (black, dashed) shows a <strong>negative slope</strong>, suggesting that <strong>strength decreases</strong> with training.</li>
<li>This overall trend is opposite to the trends observed within the individual groups.</li>
</ul></li>
<li><strong>Why Does This Happen?</strong>
<ul>
<li>This paradox occurs because the relationship between the grouping variable (<code>age</code>) and the independent variable (<code>training</code>) creates a confounding effect.</li>
<li>In this case: Children tend to have higher training values overall,
while adults tend to have lower training values.</li>
</ul></li>
</ul>
<p>This (fictitious) example shows that throwing variables into a multiple regression
model without thinking about it, is not a good idea.</p>
</div>
</div>
<div id="throwing_variables" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> What happens when you just throw variables into multiple regression?<a href="multiple-linear-regression.html#throwing_variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This sub-chapter is <strong>important</strong>. I can guarantee you that not too many applied
scientists using regression models know about this.</p>
<p>A first taste of causality.</p>
<p>Richard McElreath has 3 cool examples on
<a href="https://github.com/rmcelreath/causal_salad_2021/blob/main/1_causal_salad.r">github</a>
that show what happens if you include variables the wrong way and explains this in a
<a href="https://www.youtube.com/watch?v=KNPYUVmY3NM&amp;ab_channel=RichardMcElreath">video</a>.</p>
<p>We will look at these below - <strong>the pipe, the fork and the collider</strong>. These are causal graphs showing the
relationships between the variables. One is interested in the <strong>effect</strong> of X on Y.
In this case, it is truly an <em>effect</em>, since we create the models in such a way that changing
one variable, changes the other - which is indicated by an arrow in the graph.
These graphs are called DAGs - directed acyclic graphs. <em>Directed</em> because of the arrows,
<em>acyclic</em> because there are no cycles in the graph. One nice tool for drawing them is
<a href="https://www.dagitty.net/">dagitty</a>, which is also <a href="https://cran.r-project.org/web/packages/dagitty/index.html">implemented in R</a>.</p>
<div id="pipe" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Pipe<a href="multiple-linear-regression.html#pipe" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this setting <span class="math inline">\(X\)</span> is associated with <span class="math inline">\(Z\)</span> and <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(Y\)</span>.
<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not directly associated, but through <span class="math inline">\(Z\)</span> (see graph below).
If we condition on <span class="math inline">\(Z\)</span>, the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> <em>disappears</em>.
This means, if we <strong>know</strong> the value of <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> does not give us any <em>additional</em> information about <span class="math inline">\(Y\)</span>.</p>
<p>This can be seen in the scatterplot: Once we are within <span class="math inline">\(Z=0\)</span> (black dots) or <span class="math inline">\(Z=1\)</span> (red dots),
<span class="math inline">\(X\)</span> does not give us any information about <span class="math inline">\(Y\)</span>,
i.e., the point cloud is horizontal and there is no correlation.</p>
<p>The <code>inv_logit</code> function is the inverse of the logit function.
It assigns higher probability of <span class="math inline">\(Z\)</span> being <span class="math inline">\(1\)</span> if X has a higher value.
Let’s plot this for understanding:</p>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="multiple-linear-regression.html#cb146-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="fl">0.1</span>)</span>
<span id="cb146-2"><a href="multiple-linear-regression.html#cb146-2" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="fu">inv_logit</span>(x), <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;P(Z=1|X)&quot;</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>Higher <span class="math inline">\(X\)</span> values lead to higher probabilities of <span class="math inline">\(Z=1\)</span>.
As you can see, more red dots are on the right side of the scatterplot below.</p>
<p>Now to the pipe and a mini simulation for it:</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="multiple-linear-regression.html#cb147-1" tabindex="-1"></a><span class="co"># pipe</span></span>
<span id="cb147-2"><a href="multiple-linear-regression.html#cb147-2" tabindex="-1"></a><span class="fu">library</span>(dagitty)</span>
<span id="cb147-3"><a href="multiple-linear-regression.html#cb147-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb147-4"><a href="multiple-linear-regression.html#cb147-4" tabindex="-1"></a><span class="fu">library</span>(ggdag)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;ggdag&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="multiple-linear-regression.html#cb150-1" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb150-2"><a href="multiple-linear-regression.html#cb150-2" tabindex="-1"></a><span class="st">  X -&gt; Z -&gt; Y</span></span>
<span id="cb150-3"><a href="multiple-linear-regression.html#cb150-3" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb150-4"><a href="multiple-linear-regression.html#cb150-4" tabindex="-1"></a></span>
<span id="cb150-5"><a href="multiple-linear-regression.html#cb150-5" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb150-6"><a href="multiple-linear-regression.html#cb150-6" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb150-7"><a href="multiple-linear-regression.html#cb150-7" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">0</span>, <span class="at">Z=</span><span class="dv">0</span>) )</span>
<span id="cb150-8"><a href="multiple-linear-regression.html#cb150-8" tabindex="-1"></a></span>
<span id="cb150-9"><a href="multiple-linear-regression.html#cb150-9" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> </span>
<span id="cb150-10"><a href="multiple-linear-regression.html#cb150-10" tabindex="-1"></a>  <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="multiple-linear-regression.html#cb151-1" tabindex="-1"></a><span class="co"># d-separation plots</span></span>
<span id="cb151-2"><a href="multiple-linear-regression.html#cb151-2" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb151-3"><a href="multiple-linear-regression.html#cb151-3" tabindex="-1"></a>cols <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fu">col.alpha</span>(<span class="dv">1</span>,a) , <span class="fu">col.alpha</span>(<span class="dv">2</span>,a) )</span>
<span id="cb151-4"><a href="multiple-linear-regression.html#cb151-4" tabindex="-1"></a></span>
<span id="cb151-5"><a href="multiple-linear-regression.html#cb151-5" tabindex="-1"></a><span class="co"># pipe</span></span>
<span id="cb151-6"><a href="multiple-linear-regression.html#cb151-6" tabindex="-1"></a><span class="co"># X -&gt; Z -&gt; Y</span></span>
<span id="cb151-7"><a href="multiple-linear-regression.html#cb151-7" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb151-8"><a href="multiple-linear-regression.html#cb151-8" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb151-9"><a href="multiple-linear-regression.html#cb151-9" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(X))</span>
<span id="cb151-10"><a href="multiple-linear-regression.html#cb151-10" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb151-11"><a href="multiple-linear-regression.html#cb151-11" tabindex="-1"></a></span>
<span id="cb151-12"><a href="multiple-linear-regression.html#cb151-12" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb151-13"><a href="multiple-linear-regression.html#cb151-13" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb151-14"><a href="multiple-linear-regression.html#cb151-14" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb151-15"><a href="multiple-linear-regression.html#cb151-15" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-74-2.png" width="672" /></p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="multiple-linear-regression.html#cb152-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">1</span>],Y[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.007540748</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="multiple-linear-regression.html#cb154-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">0</span>],Y[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] 0.02629344</code></pre>
<p>Or in the framework of Simpsons paradox:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="multiple-linear-regression.html#cb156-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb156-2"><a href="multiple-linear-regression.html#cb156-2" tabindex="-1"></a></span>
<span id="cb156-3"><a href="multiple-linear-regression.html#cb156-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb156-4"><a href="multiple-linear-regression.html#cb156-4" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb156-5"><a href="multiple-linear-regression.html#cb156-5" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(X))</span>
<span id="cb156-6"><a href="multiple-linear-regression.html#cb156-6" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb156-7"><a href="multiple-linear-regression.html#cb156-7" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X) <span class="co"># without conditioning on Z</span></span>
<span id="cb156-8"><a href="multiple-linear-regression.html#cb156-8" tabindex="-1"></a><span class="fu">summary</span>(mod1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7717 -0.8634 -0.0037  0.9204  3.5437 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.00123    0.04210  -0.029    0.977    
## X            0.46041    0.04296  10.718   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.331 on 998 degrees of freedom
## Multiple R-squared:  0.1032, Adjusted R-squared:  0.1023 
## F-statistic: 114.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="multiple-linear-regression.html#cb158-1" tabindex="-1"></a>mod2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z) <span class="co"># with conditioning on Z</span></span>
<span id="cb158-2"><a href="multiple-linear-regression.html#cb158-2" tabindex="-1"></a><span class="fu">summary</span>(mod2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2979 -0.6399  0.0011  0.7047  2.8701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.98781    0.04662 -21.190   &lt;2e-16 ***
## X            0.02121    0.03542   0.599    0.549    
## Z            1.97980    0.06941  28.523   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9883 on 997 degrees of freedom
## Multiple R-squared:  0.5062, Adjusted R-squared:  0.5052 
## F-statistic:   511 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Adding <span class="math inline">\(Z\)</span> to the model (i.e., conditioning on <span class="math inline">\(Z\)</span>) makes the coefficient for <span class="math inline">\(X\)</span> disappear.
Knowing <span class="math inline">\(Z\)</span> means that <span class="math inline">\(X\)</span> does not give us any additional information about <span class="math inline">\(Y\)</span>.
See <a href="multiple-linear-regression.html#exercise7_multiple_regression">exercise 7</a>.</p>
<p><span class="math inline">\(Z\)</span> is also called a <a href="https://en.wikipedia.org/wiki/Mediation_(statistics)"><strong>mediator</strong></a>.</p>
<p>We could easily verify that <span class="math inline">\(Z\)</span> constitutes a mediator by using Baron and
Kenny’s 1986 approach (see Wiki):</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="multiple-linear-regression.html#cb160-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X))<span class="co"># Step 1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7717 -0.8634 -0.0037  0.9204  3.5437 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.00123    0.04210  -0.029    0.977    
## X            0.46041    0.04296  10.718   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.331 on 998 degrees of freedom
## Multiple R-squared:  0.1032, Adjusted R-squared:  0.1023 
## F-statistic: 114.9 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="multiple-linear-regression.html#cb162-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Z <span class="sc">~</span> X))<span class="co"># Step 2</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Z ~ X)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.97876 -0.40728 -0.00555  0.41502  1.09581 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.49832    0.01425   34.96   &lt;2e-16 ***
## X            0.22184    0.01454   15.25   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4507 on 998 degrees of freedom
## Multiple R-squared:  0.189,  Adjusted R-squared:  0.1882 
## F-statistic: 232.6 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="multiple-linear-regression.html#cb164-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z))<span class="co"># Step 3</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2979 -0.6399  0.0011  0.7047  2.8701 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.98781    0.04662 -21.190   &lt;2e-16 ***
## X            0.02121    0.03542   0.599    0.549    
## Z            1.97980    0.06941  28.523   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9883 on 997 degrees of freedom
## Multiple R-squared:  0.5062, Adjusted R-squared:  0.5052 
## F-statistic:   511 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Both coefficients for <span class="math inline">\(X\)</span> are “significant” in steps 1 and 2.
The coefficients for <span class="math inline">\(Z\)</span> in step 3 is “significant” and the coefficient for
<span class="math inline">\(X\)</span> is smaller compared to step 1. And not to forget: the very small <span class="math inline">\(p\)</span>-values are strongly
related to the very large smaple size and a strict cutoff (<span class="math inline">\(\alpha = 0.05\)</span>) makes no sense.</p>
</div>
<div id="fork" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Fork<a href="multiple-linear-regression.html#fork" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In health science, this is the classical <a href="https://en.wikipedia.org/wiki/Confounding">confounder</a>.
<span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>actually</em> not associated (no arrow) but an association is
still shown: <code>cor(X,Y) = 0.5097437</code>.
But if we condition on <span class="math inline">\(Z\)</span>, the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> disappears. The assocation is
spurious in this case and <em>adjusting</em> for the confounder yields the correct result.</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="multiple-linear-regression.html#cb166-1" tabindex="-1"></a><span class="co"># fork</span></span>
<span id="cb166-2"><a href="multiple-linear-regression.html#cb166-2" tabindex="-1"></a><span class="co"># X &lt;- Z -&gt; Y</span></span>
<span id="cb166-3"><a href="multiple-linear-regression.html#cb166-3" tabindex="-1"></a></span>
<span id="cb166-4"><a href="multiple-linear-regression.html#cb166-4" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb166-5"><a href="multiple-linear-regression.html#cb166-5" tabindex="-1"></a><span class="st">  X &lt;- Z -&gt; Y</span></span>
<span id="cb166-6"><a href="multiple-linear-regression.html#cb166-6" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb166-7"><a href="multiple-linear-regression.html#cb166-7" tabindex="-1"></a></span>
<span id="cb166-8"><a href="multiple-linear-regression.html#cb166-8" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb166-9"><a href="multiple-linear-regression.html#cb166-9" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb166-10"><a href="multiple-linear-regression.html#cb166-10" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="fl">0.5</span>, <span class="at">Y=</span><span class="fl">0.5</span>, <span class="at">Z=</span><span class="dv">0</span>) )</span>
<span id="cb166-11"><a href="multiple-linear-regression.html#cb166-11" tabindex="-1"></a></span>
<span id="cb166-12"><a href="multiple-linear-regression.html#cb166-12" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="multiple-linear-regression.html#cb167-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb167-2"><a href="multiple-linear-regression.html#cb167-2" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N)</span>
<span id="cb167-3"><a href="multiple-linear-regression.html#cb167-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>)</span>
<span id="cb167-4"><a href="multiple-linear-regression.html#cb167-4" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N,(<span class="dv">2</span><span class="sc">*</span>Z<span class="dv">-1</span>))</span>
<span id="cb167-5"><a href="multiple-linear-regression.html#cb167-5" tabindex="-1"></a></span>
<span id="cb167-6"><a href="multiple-linear-regression.html#cb167-6" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb167-7"><a href="multiple-linear-regression.html#cb167-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb167-8"><a href="multiple-linear-regression.html#cb167-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb167-9"><a href="multiple-linear-regression.html#cb167-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-77-2.png" width="672" /></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="multiple-linear-regression.html#cb168-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">1</span>],Y[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.05903899</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="multiple-linear-regression.html#cb170-1" tabindex="-1"></a><span class="fu">cor</span>(X[Z<span class="sc">==</span><span class="dv">0</span>],Y[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] -0.03401368</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="multiple-linear-regression.html#cb172-1" tabindex="-1"></a><span class="fu">cor</span>(X,Y)</span></code></pre></div>
<pre><code>## [1] 0.5097437</code></pre>
<p>In the example, we know that <span class="math inline">\(Z\)</span> is associated with both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (per construction).
As you can see in the definitions of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would not be associated
weren’t it for <span class="math inline">\(Z\)</span>.</p>
</div>
<div id="collider" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Collider<a href="multiple-linear-regression.html#collider" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This is rather interesting. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independently associated
with <span class="math inline">\(Z\)</span>. This can be seen in the toy example below, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are used
for the definition of <span class="math inline">\(Z\)</span>. <span class="math inline">\(Z\)</span> is defined as a <strong>sum
score</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Sum scores are <em>very</em> often used in health sciences (and others).
The higher the sum score, the higher the probability that <span class="math inline">\(Z=1\)</span>.
Now, <em>if</em> we know the value of <span class="math inline">\(Z\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are negatively associated (see graph).
The reason for this association is that there is a compensatory effect. In order to get a high
score, you can either have a high value of <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> or both.
This induces the negative correlation.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="multiple-linear-regression.html#cb174-1" tabindex="-1"></a><span class="co"># collider</span></span>
<span id="cb174-2"><a href="multiple-linear-regression.html#cb174-2" tabindex="-1"></a><span class="co"># X -&gt; Z &lt;- Y</span></span>
<span id="cb174-3"><a href="multiple-linear-regression.html#cb174-3" tabindex="-1"></a></span>
<span id="cb174-4"><a href="multiple-linear-regression.html#cb174-4" tabindex="-1"></a><span class="co">#dag</span></span>
<span id="cb174-5"><a href="multiple-linear-regression.html#cb174-5" tabindex="-1"></a>dag <span class="ot">&lt;-</span> <span class="fu">dagitty</span>( <span class="st">&#39;dag {</span></span>
<span id="cb174-6"><a href="multiple-linear-regression.html#cb174-6" tabindex="-1"></a><span class="st">  X -&gt; Z &lt;- Y</span></span>
<span id="cb174-7"><a href="multiple-linear-regression.html#cb174-7" tabindex="-1"></a><span class="st">}&#39;</span> )</span>
<span id="cb174-8"><a href="multiple-linear-regression.html#cb174-8" tabindex="-1"></a></span>
<span id="cb174-9"><a href="multiple-linear-regression.html#cb174-9" tabindex="-1"></a>dagitty<span class="sc">::</span><span class="fu">coordinates</span>( dag ) <span class="ot">&lt;-</span></span>
<span id="cb174-10"><a href="multiple-linear-regression.html#cb174-10" tabindex="-1"></a>  <span class="fu">list</span>( <span class="at">x=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">2</span>, <span class="at">Z=</span><span class="dv">1</span>),</span>
<span id="cb174-11"><a href="multiple-linear-regression.html#cb174-11" tabindex="-1"></a>        <span class="at">y=</span><span class="fu">c</span>(<span class="at">X=</span><span class="dv">0</span>, <span class="at">Y=</span><span class="dv">0</span>, <span class="at">Z=</span><span class="dv">1</span>) )</span>
<span id="cb174-12"><a href="multiple-linear-regression.html#cb174-12" tabindex="-1"></a></span>
<span id="cb174-13"><a href="multiple-linear-regression.html#cb174-13" tabindex="-1"></a><span class="fu">ggdag</span>(dag) <span class="sc">+</span> <span class="fu">theme_dag</span>()</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-78-1.png" width="672" /></p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="multiple-linear-regression.html#cb175-1" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb175-2"><a href="multiple-linear-regression.html#cb175-2" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb175-3"><a href="multiple-linear-regression.html#cb175-3" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N)</span>
<span id="cb175-4"><a href="multiple-linear-regression.html#cb175-4" tabindex="-1"></a>Z <span class="ot">&lt;-</span> <span class="fu">rbern</span>(N,<span class="fu">inv_logit</span>(<span class="dv">2</span><span class="sc">*</span>X<span class="sc">+</span><span class="dv">2</span><span class="sc">*</span>Y<span class="dv">-2</span>))</span>
<span id="cb175-5"><a href="multiple-linear-regression.html#cb175-5" tabindex="-1"></a></span>
<span id="cb175-6"><a href="multiple-linear-regression.html#cb175-6" tabindex="-1"></a><span class="fu">plot</span>( X , Y , <span class="at">col=</span>cols[Z<span class="sc">+</span><span class="dv">1</span>] , <span class="at">pch=</span><span class="dv">16</span> )</span>
<span id="cb175-7"><a href="multiple-linear-regression.html#cb175-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">1</span>]),<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb175-8"><a href="multiple-linear-regression.html#cb175-8" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>X[Z<span class="sc">==</span><span class="dv">0</span>]),<span class="at">col=</span><span class="dv">1</span>,<span class="at">lwd=</span><span class="dv">3</span>)</span>
<span id="cb175-9"><a href="multiple-linear-regression.html#cb175-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X),<span class="at">lwd=</span><span class="dv">3</span>,<span class="at">lty=</span><span class="dv">3</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-78-2.png" width="672" /></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="multiple-linear-regression.html#cb176-1" tabindex="-1"></a><span class="fu">cor</span>(Y[Z<span class="sc">==</span><span class="dv">1</span>], X[Z<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] -0.326671</code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="multiple-linear-regression.html#cb178-1" tabindex="-1"></a><span class="fu">cor</span>(Y[Z<span class="sc">==</span><span class="dv">0</span>], X[Z<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>## [1] -0.1768203</code></pre>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="multiple-linear-regression.html#cb180-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X)) <span class="co"># mod1</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3892 -0.6079 -0.0269  0.6462  3.1038 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.01882    0.03055  -0.616    0.538
## X            0.04061    0.02948   1.378    0.169
## 
## Residual standard error: 0.9662 on 998 degrees of freedom
## Multiple R-squared:  0.001898,   Adjusted R-squared:  0.0008978 
## F-statistic: 1.898 on 1 and 998 DF,  p-value: 0.1686</code></pre>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="multiple-linear-regression.html#cb182-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X <span class="sc">+</span> Z)) <span class="co"># mod2</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X + Z)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.03484 -0.55761  0.02022  0.55813  2.41336 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.33787    0.03215  -10.51  &lt; 2e-16 ***
## X           -0.19963    0.02906   -6.87 1.13e-11 ***
## Z            1.21398    0.06840   17.75  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8427 on 997 degrees of freedom
## Multiple R-squared:  0.2415, Adjusted R-squared:   0.24 
## F-statistic: 158.7 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>If you think of Simpsons paradox again, conditioning on <span class="math inline">\(Z\)</span> <em>creates</em> an association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,
which would otherwise be independent (by definition). So by learning the value of <span class="math inline">\(Z\)</span>, we can learn something
from <span class="math inline">\(X\)</span> about <span class="math inline">\(Y\)</span>. On the one hand, adding <span class="math inline">\(Z\)</span> to the model creates an association where there is none,
on the other hand, <em>prediction</em> of <span class="math inline">\(Y\)</span> is better with <span class="math inline">\(Z\)</span> in the model! In prediction, we largely
do not care about the causal structure of the data. We just want to predict <span class="math inline">\(Y\)</span> as accurately as possible.</p>
<p>The sum score example is also called <a href="https://en.wikipedia.org/wiki/Berkson%27s_paradox">Berkson’s paradox</a>.
Let’s check if this also works for three variables in a sum score
in <a href="multiple-linear-regression.html#exercise10_multiple_regression">exercise 10</a>.</p>
</div>
<div id="multicollinearity" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> Multicollinearity<a href="multiple-linear-regression.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Westfall section 8.4 and McElreath section 6.1 cover this topic.
<a href="https://en.wikipedia.org/wiki/Multicollinearity">Multicollinearity</a> means that there is a strong
correlation between two or more predictors. <em>Perfect</em> multicollinearity it is when the correlation is 1.
For example, if you accidentally include the same variable twice in the model, or when you include
the elements of a sum score and the sum score itself as predictors in the model. R just gives you
an <em>NA</em> if you do this in <code>lm</code>.</p>
<div id="mcElreath_6.1" class="section level4 hasAnchor" number="4.3.4.1">
<h4><span class="header-section-number">4.3.4.1</span> Example from McElreath 6.1<a href="multiple-linear-regression.html#mcElreath_6.1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The code creates body heights and leg lengths (left and right) for 100 people.</p>
<p><code>runif</code> draws a uniform random number between 40 and 50% of height as leg length, hence on average 45%.
The slope in the linear regression should therefore be around the average height divided by the average leg length:
<span class="math inline">\(\frac{10}{0.45 \cdot 10} \sim 2.2\)</span>.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="multiple-linear-regression.html#cb184-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb184-2"><a href="multiple-linear-regression.html#cb184-2" tabindex="-1"></a></span>
<span id="cb184-3"><a href="multiple-linear-regression.html#cb184-3" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb184-4"><a href="multiple-linear-regression.html#cb184-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">909</span>)</span>
<span id="cb184-5"><a href="multiple-linear-regression.html#cb184-5" tabindex="-1"></a>height <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb184-6"><a href="multiple-linear-regression.html#cb184-6" tabindex="-1"></a>leg_prop <span class="ot">&lt;-</span> <span class="fu">runif</span>(N, <span class="fl">0.4</span>, <span class="fl">0.5</span>)</span>
<span id="cb184-7"><a href="multiple-linear-regression.html#cb184-7" tabindex="-1"></a>leg_left <span class="ot">&lt;-</span> leg_prop <span class="sc">*</span> height <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb184-8"><a href="multiple-linear-regression.html#cb184-8" tabindex="-1"></a>leg_right <span class="ot">&lt;-</span> leg_prop <span class="sc">*</span> height <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="fl">0.02</span>)</span>
<span id="cb184-9"><a href="multiple-linear-regression.html#cb184-9" tabindex="-1"></a></span>
<span id="cb184-10"><a href="multiple-linear-regression.html#cb184-10" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">height =</span> height, <span class="at">leg_left =</span> leg_left, <span class="at">leg_right =</span> leg_right)</span>
<span id="cb184-11"><a href="multiple-linear-regression.html#cb184-11" tabindex="-1"></a><span class="fu">cor</span>(d<span class="sc">$</span>leg_left, d<span class="sc">$</span>leg_right)</span></code></pre></div>
<pre><code>## [1] 0.9997458</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="multiple-linear-regression.html#cb186-1" tabindex="-1"></a>m4<span class="fl">.6</span> <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb186-2"><a href="multiple-linear-regression.html#cb186-2" tabindex="-1"></a>  <span class="fu">alist</span>(</span>
<span id="cb186-3"><a href="multiple-linear-regression.html#cb186-3" tabindex="-1"></a>    height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb186-4"><a href="multiple-linear-regression.html#cb186-4" tabindex="-1"></a>    mu <span class="ot">&lt;-</span> a <span class="sc">+</span> bl <span class="sc">*</span> leg_left <span class="sc">+</span> br <span class="sc">*</span> leg_right,</span>
<span id="cb186-5"><a href="multiple-linear-regression.html#cb186-5" tabindex="-1"></a>    a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">10</span>, <span class="dv">100</span>),</span>
<span id="cb186-6"><a href="multiple-linear-regression.html#cb186-6" tabindex="-1"></a>    bl <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb186-7"><a href="multiple-linear-regression.html#cb186-7" tabindex="-1"></a>    br <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">2</span>, <span class="dv">10</span>),</span>
<span id="cb186-8"><a href="multiple-linear-regression.html#cb186-8" tabindex="-1"></a>    sigma <span class="sc">~</span> <span class="fu">dexp</span>(<span class="dv">1</span>)</span>
<span id="cb186-9"><a href="multiple-linear-regression.html#cb186-9" tabindex="-1"></a>  ),    <span class="at">data =</span> d</span>
<span id="cb186-10"><a href="multiple-linear-regression.html#cb186-10" tabindex="-1"></a>)</span>
<span id="cb186-11"><a href="multiple-linear-regression.html#cb186-11" tabindex="-1"></a><span class="fu">precis</span>(m4<span class="fl">.6</span>)</span></code></pre></div>
<pre><code>##            mean         sd       5.5%     94.5%
## a     0.9811938 0.28396068  0.5273698 1.4350178
## bl    0.2138475 2.52707954 -3.8249137 4.2526087
## br    1.7817046 2.53129314 -2.2637907 5.8271999
## sigma 0.6171141 0.04343629  0.5476945 0.6865337</code></pre>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="multiple-linear-regression.html#cb188-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">precis</span>(m4<span class="fl">.6</span>))</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>With the given regression model, one asks the question: “What is the value of knowing each leg’s length, after
already knowing the other leg’s length?” The answer is: “Not much.”, since they are highly correlated.
Both coefficients are not around the expected <span class="math inline">\(\beta\)</span> and the credible intervals are wide and include the
credible value <span class="math inline">\(0\)</span>.</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="multiple-linear-regression.html#cb189-1" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(m4<span class="fl">.6</span>)</span>
<span id="cb189-2"><a href="multiple-linear-regression.html#cb189-2" tabindex="-1"></a><span class="fu">plot</span>( bl <span class="sc">~</span> br, post, <span class="at">col=</span><span class="fu">col.alpha</span>(rangi2,<span class="fl">0.1</span>), <span class="at">pch =</span> <span class="dv">16</span> )</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-80-1.png" width="672" /></p>
<p>Since the two coefficients are almost perfectly multicollinear
(<code>cor(d$leg_left, d$leg_right)=0.9997458</code>), knowing one leg’s length does not
give us any additional information about the other leg’s length.
Leaving out the right leg length would give the correct result (<a href="multiple-linear-regression.html#exercise11_multiple_regression">exercise 11</a>).</p>
</div>
<div id="example-from-westfall-8.4" class="section level4 hasAnchor" number="4.3.4.2">
<h4><span class="header-section-number">4.3.4.2</span> Example from Westfall 8.4<a href="multiple-linear-regression.html#example-from-westfall-8.4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the example below, the second prector <span class="math inline">\(X2\)</span> is a perfect linear function of the first predictor <span class="math inline">\(X1\)</span>.</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="multiple-linear-regression.html#cb190-1" tabindex="-1"></a><span class="co"># Westfall 8.4.</span></span>
<span id="cb190-2"><a href="multiple-linear-regression.html#cb190-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb190-3"><a href="multiple-linear-regression.html#cb190-3" tabindex="-1"></a>X1 <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb190-4"><a href="multiple-linear-regression.html#cb190-4" tabindex="-1"></a>X2 <span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span>X1 <span class="sc">-</span><span class="dv">1</span>     <span class="co"># Perfect collinearity</span></span>
<span id="cb190-5"><a href="multiple-linear-regression.html#cb190-5" tabindex="-1"></a>Y <span class="ot">=</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>X1 <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>X2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb190-6"><a href="multiple-linear-regression.html#cb190-6" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y<span class="sc">~</span>X1<span class="sc">+</span>X2))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1 + X2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.20347 -0.60278 -0.01114  0.61898  2.60970 
## 
## Coefficients: (1 not defined because of singularities)
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.97795    0.10353  -19.11   &lt;2e-16 ***
## X1           8.09454    0.09114   88.82   &lt;2e-16 ***
## X2                NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.011 on 98 degrees of freedom
## Multiple R-squared:  0.9877, Adjusted R-squared:  0.9876 
## F-statistic:  7888 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Let’s look at a 3D plot:</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="multiple-linear-regression.html#cb192-1" tabindex="-1"></a><span class="fu">library</span>(plotly)</span>
<span id="cb192-2"><a href="multiple-linear-regression.html#cb192-2" tabindex="-1"></a></span>
<span id="cb192-3"><a href="multiple-linear-regression.html#cb192-3" tabindex="-1"></a><span class="co"># Generate data</span></span>
<span id="cb192-4"><a href="multiple-linear-regression.html#cb192-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb192-5"><a href="multiple-linear-regression.html#cb192-5" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb192-6"><a href="multiple-linear-regression.html#cb192-6" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> X1 <span class="sc">-</span> <span class="dv">1</span>  <span class="co"># Perfect collinearity</span></span>
<span id="cb192-7"><a href="multiple-linear-regression.html#cb192-7" tabindex="-1"></a>Y  <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> X1 <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> X2 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb192-8"><a href="multiple-linear-regression.html#cb192-8" tabindex="-1"></a></span>
<span id="cb192-9"><a href="multiple-linear-regression.html#cb192-9" tabindex="-1"></a><span class="co"># Create the 3D scatter plot with vertical lines</span></span>
<span id="cb192-10"><a href="multiple-linear-regression.html#cb192-10" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span></span>
<span id="cb192-11"><a href="multiple-linear-regression.html#cb192-11" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x =</span> X1, <span class="at">y =</span> X2, <span class="at">z =</span> Y, </span>
<span id="cb192-12"><a href="multiple-linear-regression.html#cb192-12" tabindex="-1"></a>              <span class="at">marker =</span> <span class="fu">list</span>(<span class="at">color =</span> Y, <span class="at">colorscale =</span> <span class="st">&quot;Viridis&quot;</span>, <span class="at">size =</span> <span class="dv">5</span>),</span>
<span id="cb192-13"><a href="multiple-linear-regression.html#cb192-13" tabindex="-1"></a>              <span class="at">name =</span> <span class="st">&quot;Data Points&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb192-14"><a href="multiple-linear-regression.html#cb192-14" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">title =</span> <span class="st">&quot;3D Scatter Plot&quot;</span>,</span>
<span id="cb192-15"><a href="multiple-linear-regression.html#cb192-15" tabindex="-1"></a>         <span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X1&quot;</span>),</span>
<span id="cb192-16"><a href="multiple-linear-regression.html#cb192-16" tabindex="-1"></a>                      <span class="at">yaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb192-17"><a href="multiple-linear-regression.html#cb192-17" tabindex="-1"></a>                      <span class="at">zaxis =</span> <span class="fu">list</span>(<span class="at">title =</span> <span class="st">&quot;Y&quot;</span>)))</span></code></pre></div>
<div class="plotly html-widget html-fill-item" id="htmlwidget-20e3c5f068839b234398" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-20e3c5f068839b234398">{"x":{"visdat":{"a35a271437ee":["function () ","plotlyVisDat"]},"cur_data":"a35a271437ee","attrs":{"a35a271437ee":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[1.3709584471466685,-0.56469817139608869,0.3631284113373392,0.63286260496104041,0.40426832314099903,-0.10612451609148403,1.5115219974389389,-0.094659038413097557,2.0184237138770418,-0.062714099052420993,1.3048696542234852,2.2866453927011068,-1.3888607011123393,-0.27878876681737136,-0.13332133639365804,0.63595039807007436,-0.28425292141607239,-2.6564554209047757,-2.4404669285755194,1.3201133457301921,-0.30663859407847455,-1.78130843398,-0.17191735575962136,1.2146746991725987,1.8951934612649652,-0.43046913160619965,-0.25726938276892963,-1.7631630851947802,0.4600973548312714,-0.63999487596011917,0.45545012324121936,0.70483733722881914,1.0351035219699223,-0.60892637540721106,0.50495512329797032,-1.7170086790733425,-0.784459008379496,-0.85090759417651829,-2.4142076499466318,0.036122606892255632,0.20599860020025385,-0.36105729854866631,0.75816323569951694,-0.72670482707657524,-1.3682810444192945,0.43281802588871715,-0.81139317618667162,1.4441012617212527,-0.43144620261334543,0.65564788340220681,0.32192526520394654,-0.78383894088037542,1.5757275197919773,0.64289930571731635,0.08976064659960567,0.27655074729146301,0.67928881605527081,0.0898328865790817,-2.9930900831529348,0.2848829535306594,-0.36723464274097534,0.18523056486560932,0.58182372736550669,1.3997368272926782,-0.72729205947446507,1.3025426320441438,0.33584811975207446,1.0385060986976211,0.92072856829064631,0.72087816286686246,-1.043118938567855,-0.090186386610706687,0.62351816199954357,-0.95352335777234398,-0.54282881457385668,0.58099649768168249,0.76817873783459101,0.4637675885401672,-0.88577629740967945,-1.0997808986478557,1.5127070098049282,0.25792143753203084,0.088440229159586353,-0.12089653753908947,-1.1943288951605282,0.61199689804038693,-0.21713984574652084,-0.18275670633192173,0.93334632857116007,0.82177311050824919,1.392116375934271,-0.47617392305467421,0.65034856072630509,1.3911104563900005,-1.1107888794478988,-0.8607925868778421,-1.1317386808537699,-1.4592139995023958,0.079982553241161172,0.65320433964919],"y":[1.741916894293337,-2.1293963427921776,-0.27374317732532161,0.26572520992208082,-0.19146335371800194,-1.212249032182968,2.0230439948778778,-1.1893180768261951,3.0368474277540836,-1.125428198104842,1.6097393084469704,3.5732907854022136,-3.7777214022246786,-1.5575775336347428,-1.2666426727873161,0.27190079614014873,-1.5685058428321448,-6.3129108418095514,-5.8809338571510388,1.6402266914603842,-1.6132771881569492,-4.5626168679600001,-1.3438347115192428,1.4293493983451975,2.7903869225299305,-1.8609382632123994,-1.5145387655378593,-4.5263261703895603,-0.079805290337457202,-2.2799897519202386,-0.089099753517561275,0.40967467445763828,1.0702070439398446,-2.2178527508144219,0.0099102465959406416,-4.4340173581466846,-2.568918016758992,-2.7018151883530366,-5.8284152998932637,-0.92775478621548868,-0.58800279959949231,-1.7221145970973326,0.51632647139903387,-2.4534096541531505,-3.7365620888385891,-0.1343639482225657,-2.6227863523733435,1.8882025234425055,-1.8628924052266909,0.31129576680441362,-0.35614946959210692,-2.5676778817607508,2.1514550395839547,0.2857986114346327,-0.8204787068007886,-0.44689850541707399,0.35857763211054161,-0.82033422684183654,-6.9861801663058696,-0.4302340929386812,-1.7344692854819508,-0.62953887026878141,0.16364745473101339,1.7994736545853565,-2.4545841189489304,1.6050852640882876,-0.32830376049585108,1.0770121973952422,0.84145713658129262,0.44175632573372492,-3.08623787713571,-1.1803727732214133,0.24703632399908715,-2.9070467155446877,-2.0856576291477134,0.16199299536336498,0.53635747566918202,-0.072464822919665606,-2.7715525948193589,-3.1995617972957113,2.0254140196098565,-0.48415712493593832,-0.82311954168082724,-1.2417930750781789,-3.3886577903210564,0.22399379608077385,-1.4342796914930416,-1.3655134126638435,0.86669265714232013,0.64354622101649839,1.7842327518685419,-1.9523478461093484,0.30069712145261018,1.7822209127800011,-3.2215777588957977,-2.7215851737556842,-3.2634773617075399,-3.9184279990047917,-0.84003489351767768,0.30640867929838],"z":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"type":"scatter3d","mode":"markers","marker":{"color":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"colorscale":"Viridis","size":5},"name":"Data Points","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"3D Scatter Plot","scene":{"xaxis":{"title":"X1"},"yaxis":{"title":"X2"},"zaxis":{"title":"Y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[1.3709584471466685,-0.56469817139608869,0.3631284113373392,0.63286260496104041,0.40426832314099903,-0.10612451609148403,1.5115219974389389,-0.094659038413097557,2.0184237138770418,-0.062714099052420993,1.3048696542234852,2.2866453927011068,-1.3888607011123393,-0.27878876681737136,-0.13332133639365804,0.63595039807007436,-0.28425292141607239,-2.6564554209047757,-2.4404669285755194,1.3201133457301921,-0.30663859407847455,-1.78130843398,-0.17191735575962136,1.2146746991725987,1.8951934612649652,-0.43046913160619965,-0.25726938276892963,-1.7631630851947802,0.4600973548312714,-0.63999487596011917,0.45545012324121936,0.70483733722881914,1.0351035219699223,-0.60892637540721106,0.50495512329797032,-1.7170086790733425,-0.784459008379496,-0.85090759417651829,-2.4142076499466318,0.036122606892255632,0.20599860020025385,-0.36105729854866631,0.75816323569951694,-0.72670482707657524,-1.3682810444192945,0.43281802588871715,-0.81139317618667162,1.4441012617212527,-0.43144620261334543,0.65564788340220681,0.32192526520394654,-0.78383894088037542,1.5757275197919773,0.64289930571731635,0.08976064659960567,0.27655074729146301,0.67928881605527081,0.0898328865790817,-2.9930900831529348,0.2848829535306594,-0.36723464274097534,0.18523056486560932,0.58182372736550669,1.3997368272926782,-0.72729205947446507,1.3025426320441438,0.33584811975207446,1.0385060986976211,0.92072856829064631,0.72087816286686246,-1.043118938567855,-0.090186386610706687,0.62351816199954357,-0.95352335777234398,-0.54282881457385668,0.58099649768168249,0.76817873783459101,0.4637675885401672,-0.88577629740967945,-1.0997808986478557,1.5127070098049282,0.25792143753203084,0.088440229159586353,-0.12089653753908947,-1.1943288951605282,0.61199689804038693,-0.21713984574652084,-0.18275670633192173,0.93334632857116007,0.82177311050824919,1.392116375934271,-0.47617392305467421,0.65034856072630509,1.3911104563900005,-1.1107888794478988,-0.8607925868778421,-1.1317386808537699,-1.4592139995023958,0.079982553241161172,0.65320433964919],"y":[1.741916894293337,-2.1293963427921776,-0.27374317732532161,0.26572520992208082,-0.19146335371800194,-1.212249032182968,2.0230439948778778,-1.1893180768261951,3.0368474277540836,-1.125428198104842,1.6097393084469704,3.5732907854022136,-3.7777214022246786,-1.5575775336347428,-1.2666426727873161,0.27190079614014873,-1.5685058428321448,-6.3129108418095514,-5.8809338571510388,1.6402266914603842,-1.6132771881569492,-4.5626168679600001,-1.3438347115192428,1.4293493983451975,2.7903869225299305,-1.8609382632123994,-1.5145387655378593,-4.5263261703895603,-0.079805290337457202,-2.2799897519202386,-0.089099753517561275,0.40967467445763828,1.0702070439398446,-2.2178527508144219,0.0099102465959406416,-4.4340173581466846,-2.568918016758992,-2.7018151883530366,-5.8284152998932637,-0.92775478621548868,-0.58800279959949231,-1.7221145970973326,0.51632647139903387,-2.4534096541531505,-3.7365620888385891,-0.1343639482225657,-2.6227863523733435,1.8882025234425055,-1.8628924052266909,0.31129576680441362,-0.35614946959210692,-2.5676778817607508,2.1514550395839547,0.2857986114346327,-0.8204787068007886,-0.44689850541707399,0.35857763211054161,-0.82033422684183654,-6.9861801663058696,-0.4302340929386812,-1.7344692854819508,-0.62953887026878141,0.16364745473101339,1.7994736545853565,-2.4545841189489304,1.6050852640882876,-0.32830376049585108,1.0770121973952422,0.84145713658129262,0.44175632573372492,-3.08623787713571,-1.1803727732214133,0.24703632399908715,-2.9070467155446877,-2.0856576291477134,0.16199299536336498,0.53635747566918202,-0.072464822919665606,-2.7715525948193589,-3.1995617972957113,2.0254140196098565,-0.48415712493593832,-0.82311954168082724,-1.2417930750781789,-3.3886577903210564,0.22399379608077385,-1.4342796914930416,-1.3655134126638435,0.86669265714232013,0.64354622101649839,1.7842327518685419,-1.9523478461093484,0.30069712145261018,1.7822209127800011,-3.2215777588957977,-2.7215851737556842,-3.2634773617075399,-3.9184279990047917,-0.84003489351767768,0.30640867929838],"z":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"type":"scatter3d","mode":"markers","marker":{"color":[10.168632952771842,-5.4728342840009851,-0.098181356141134435,4.91138274136107,0.56737317637017504,-2.7434823162758026,9.669920097642656,-2.8796224792597513,14.335582745517833,-2.3825518344223613,8.4138646829204795,16.401235869550888,-13.596320844745382,-4.7345272652268751,-4.7276697710640763,2.7052694576867768,-4.7866736292063798,-20.549752366893408,-22.885851659793875,8.6981629844001436,-5.946733819944086,-17.720903213276799,-3.2506364598799644,6.7207584584967526,13.159725075815015,-5.8720119342754131,-4.6718266686009322,-18.129982526977351,0.45603088829018512,-6.9404425665630161,2.21122158035329,3.1458213442770786,6.2808910598247296,-5.7485213598777216,3.4794967293599526,-16.833183200992561,-8.3929916272861451,-7.6057623524924454,-21.783390780139356,-1.7634886298009513,-0.43811849663505881,-5.7761374062957618,3.6206218807113975,-7.8430834957008404,-13.36011720441228,2.5759302304779403,-8.9721382511473564,9.1196410611692933,-4.7547070443546602,2.1888146540467464,0.5347036464803574,-9.8222563493905923,11.772989707571501,2.8695487443644501,-1.749760151875408,-1.0258463496545103,3.4265484946648397,-2.0816190853190055,-26.478212995173916,1.5667388738298638,-5.1134030121699308,-1.5899378652258016,2.817796701391436,8.8351562027134758,-7.2283229278083825,9.8527629840841406,-0.30590755309289719,6.7626990871612511,5.4507266050036574,4.6625908851994442,-10.574729647489105,-1.8848720244250399,1.2430894346596553,-7.9387279408653768,-5.4778525380722751,2.4971959925677121,2.6964227725375602,2.3531494083633193,-8.6030165154626683,-10.804602815604234,10.25311197130185,-0.52073747009355753,-0.92367143409306662,-2.6725179605932001,-11.8338905346268,1.5597385294299415,-3.036369947532132,-2.9078570283813407,4.630464035767865,2.9795967220597506,9.341889588061802,-6.1544793624102834,3.4554001891748953,7.8348811856354512,-11.845481479963553,-7.800565841342836,-10.650134542114445,-13.087224459299868,0.45508887208324222,3.3544561457959032],"colorscale":"Viridis","size":5,"line":{"color":"rgba(31,119,180,1)"}},"name":"Data Points","error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="multiple-linear-regression.html#cb193-1" tabindex="-1"></a><span class="co">#VIF(lm(Y ~ X1 + X2)) # error</span></span>
<span id="cb193-2"><a href="multiple-linear-regression.html#cb193-2" tabindex="-1"></a><span class="co">#check_model(lm(Y ~ X1 + X2)) # error</span></span></code></pre></div>
<p>There is no unique solution for a plane in this case.
Infinitely many planes can be defined using the “line” in space.
If anything, a perfectly vertical plane could go through with <span class="math inline">\(SSE = 0\)</span>,
but such a plane would not tell us how <span class="math inline">\(Y\)</span> changes with <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span>,
since any <span class="math inline">\(Y\)</span> value could be assigned to any credible combination of <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span> values.
The problem collapses into the simple linear regression problem.
One could just plug in the formula for <span class="math inline">\(X2\)</span> into the model, which yields the identical result:</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="multiple-linear-regression.html#cb194-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb194-2"><a href="multiple-linear-regression.html#cb194-2" tabindex="-1"></a>X1 <span class="ot">=</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb194-3"><a href="multiple-linear-regression.html#cb194-3" tabindex="-1"></a><span class="co"># Y = 1 + 2*X1 + 3*(2*X1 -1) + rnorm(100,0,1) = </span></span>
<span id="cb194-4"><a href="multiple-linear-regression.html#cb194-4" tabindex="-1"></a>Y <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">+</span> <span class="dv">8</span><span class="sc">*</span>X1 <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb194-5"><a href="multiple-linear-regression.html#cb194-5" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(Y <span class="sc">~</span> X1))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ X1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.20347 -0.60278 -0.01114  0.61898  2.60970 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.97795    0.10353  -19.11   &lt;2e-16 ***
## X1           8.09454    0.09114   88.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.011 on 98 degrees of freedom
## Multiple R-squared:  0.9877, Adjusted R-squared:  0.9876 
## F-statistic:  7888 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In reality, you only have one parameter (<span class="math inline">\(X1\)</span> or <span class="math inline">\(X2\)</span>) in the case of perfect multicollinearity.</p>
</div>
</div>
</div>
<div id="more-than-2-predictors" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> More than 2 predictors<a href="multiple-linear-regression.html#more-than-2-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>………</p>
</div>
<div id="exercises-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Exercises<a href="multiple-linear-regression.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[E] Easy, [M] Medium, [H] Hard</p>
<p>(Some) solutions to exercises can be found in the git-repo <a href="https://github.com/jdegenfellner/Script_QM2_ZHAW/tree/main/Solutions_Exercises">here</a>.</p>
<div id="exercise1_multiple_regression" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> [M] Exercise 1<a href="multiple-linear-regression.html#exercise1_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Fit a model with a cubic term for weight and height of the !Kung San people.</li>
<li>Add the prediction bands as seen in the book.</li>
<li>Come up with an explanation for the functional form of this relationship.</li>
<li>Could there be reasons to for taking a less complicated model
(<a href="https://en.wikipedia.org/wiki/Statistical_model_specification">1</a>, <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">2</a>)?</li>
</ul>
</div>
<div id="exercise2_multiple_regression" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> [E] Exercise 2<a href="multiple-linear-regression.html#exercise2_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the model equations from <a href="multiple-linear-regression.html#adding_transformed_predictor">above</a>
where we used polynomial regression to model the relationship between
weight and height:</p>
<ul>
<li>Draw the model hierarchy for the model.</li>
</ul>
</div>
<div id="exercise3_multiple_regression" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> [H] Exercise 3<a href="multiple-linear-regression.html#exercise3_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Invent a data set (or use the first 3 lines of a previous data set)
with 3 observations of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1, X_2\)</span> and <span class="math inline">\(X_3\)</span>. You have a data frame
with 3 rows and 4 columns.</p>
<ul>
<li>Fit a model with <span class="math inline">\(Y\)</span> as the dependent variable and <span class="math inline">\(X_1, X_2, X_3\)</span> as predictors.</li>
<li>How big is <span class="math inline">\(R^2\)</span>?</li>
<li>Could you have calulated this without <code>lm</code>and R?</li>
</ul>
</div>
<div id="exercise4_multiple_regression" class="section level3 hasAnchor" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> [E] Exercise 4<a href="multiple-linear-regression.html#exercise4_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the <a href="multiple-linear-regression.html#interaction_term">section about the interaction term</a> in the linear model.</p>
<ul>
<li>Use the code provided.</li>
<li>Standardise the predictors. How are the <span class="math inline">\(\beta\)</span>s changing and what is their interpration now?</li>
<li>Change the relative sizes of the true but usually unknown <span class="math inline">\(\beta\)</span>s.
What happens to the estimates and the graph?</li>
<li>What happens if you change the error term and increase or decrease its variance?</li>
</ul>
</div>
<div id="exercise5_multiple_regression" class="section level3 hasAnchor" number="4.5.5">
<h3><span class="header-section-number">4.5.5</span> [E] Exercise 5<a href="multiple-linear-regression.html#exercise5_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Draw the interaction plot from the <a href="multiple-linear-regression.html#interaction_plot">section about the interaction plot</a>
for the case when there is no interaction, i.e. <span class="math inline">\(\beta_3 = 0\)</span>.</p>
</div>
<div id="exercise6_multiple_regression" class="section level3 hasAnchor" number="4.5.6">
<h3><span class="header-section-number">4.5.6</span> [M] Exercise 6<a href="multiple-linear-regression.html#exercise6_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the model assumptions checks <a href="multiple-linear-regression.html#check_model_bayes">above</a>.</p>
<ul>
<li>Create the same two plots for the simple mean model without predictors, just with the intercept.</li>
<li>Which model fits the data better according to these posterior predictive checks?</li>
</ul>
</div>
<div id="exercise7_multiple_regression" class="section level3 hasAnchor" number="4.5.7">
<h3><span class="header-section-number">4.5.7</span> [E] Exercise 7<a href="multiple-linear-regression.html#exercise7_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the Simpson’s paradox <a href="multiple-linear-regression.html#simpsons_paradox">section</a>.</p>
<ul>
<li>Invent your own example for the pipe, fork and collider.</li>
</ul>
</div>
<div id="exercise8_multiple_regression" class="section level3 hasAnchor" number="4.5.8">
<h3><span class="header-section-number">4.5.8</span> [M] Exercise 8<a href="multiple-linear-regression.html#exercise8_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Take a data set of your choosing with many columns, say 10 or so, either from the internet
or from R (if available)</li>
<li>Fit a model for an arbitrary outcome, add more and more variables to predict the outcome
and verify that the <span class="math inline">\(R^2\)</span> increases.</li>
</ul>
</div>
<div id="exercise9_multiple_regression" class="section level3 hasAnchor" number="4.5.9">
<h3><span class="header-section-number">4.5.9</span> [M] Exercise 9<a href="multiple-linear-regression.html#exercise9_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Exponential curve fitting. Go back to the <a href="multiple-linear-regression.html#adding_transformed_predictor_freq">section</a> about adding a transformed predictor in the Frequentist
setting. Hint: You can use the <code>optim</code> function in R.</p>
<ul>
<li>Assume the relationship between weight and height looks like this:
<span class="math inline">\(height_i = \alpha + \beta_1 e^{\beta_2 weight_i}\)</span>.</li>
<li>Use R and the least squares method to estimate the parameters <span class="math inline">\(\alpha, \beta_1, \beta_2\)</span>.</li>
<li>Note that the sum of squared errors is this:
<span class="math inline">\(\sum_{i=1}^n (height_i - \alpha - \beta_1 e^{\beta_2 weight_i})^2\)</span>.</li>
<li>What happens if you do not constrain the parameters <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> to be negative?</li>
<li>Calculate the <span class="math inline">\(R^2\)</span> for this model.</li>
</ul>
</div>
<div id="exercise10_multiple_regression" class="section level3 hasAnchor" number="4.5.10">
<h3><span class="header-section-number">4.5.10</span> [E] Exercise 10<a href="multiple-linear-regression.html#exercise10_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s try to verify if Berkson’s paradox (which we have mentioned in the
<a href="multiple-linear-regression.html#collider">collider-section</a>) also works for three variables in a sum score.</p>
<ul>
<li>Now, we assume some college admits only applicants in the top 20% of a score
consisting of the sum of three variables: <span class="math inline">\(W, X, Z\)</span> (grade point average, math score, verbal score).</li>
<li>All three scores are individually normally distributed with mean 100 and standard deviation 15.</li>
<li>Calculate the correlation matrix <code>cor()</code> of all students and the admitted students.</li>
<li>Are any of the three variables correlated?</li>
<li>Plot a scatterplot of the math score and the verbal score und color the points according
to being admitted or not. You can add trendlines for the two groups.</li>
</ul>
</div>
<div id="exercise11_multiple_regression" class="section level3 hasAnchor" number="4.5.11">
<h3><span class="header-section-number">4.5.11</span> [E] Exercise 11<a href="multiple-linear-regression.html#exercise11_multiple_regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the multicollinearity <a href="multiple-linear-regression.html#multicollinearity">section</a> and the <a href="multiple-linear-regression.html#mcElreath_6.1">example</a>
from McElreath 6.1.</p>
<ul>
<li>Verify that the coefficient is correct when leaving out the right leg length from the model.</li>
</ul>
</div>
</div>
<div id="todos" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> TODOS<a href="multiple-linear-regression.html#todos" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><code>check_model</code> for 2 and more predictors…</li>
<li>Violations of model assumptions and how to fix them.</li>
<li>Add more Bayesian model checks, chapter 6 Gelman.</li>
<li>Show by simulation what Gelman talks about with significant p values. So I scan the data
for significant p values and then simulate data with the same effect size and see how often
I get significant p values. Especially the next effect would be probably smaller,
especially, if one did p-hacking! Calculate a priori probability for replication (def?).</li>
<li>Variability of confidence interval borders (draw from X…)</li>
<li>Logistic Regression, Poisson, …</li>
<li>Chapter: Sample size calculations for logistic and multivariate regression, Proportions, ICCs, t.test</li>
<li>Chapter about Reliability, Validity and ICCs (incl. simulation of what an ICC of 0.9 or so means), but maybe reduced</li>
<li>causal terror of richards book, fork, pipe, collider</li>
<li>Angenommen man hat ein masking eines Effekts und der Model fit ist aber gut (keine Voraussetzung verletzt),
ist diese Situation möglich?</li>
<li>What about 2 interactions?</li>
<li>Which variables should I include in a model and why?</li>
<li>What about a real life data set?</li>
<li>What about papers?</li>
<li>Simulate a cohort of Master thesis with small sample sizes and assume
that there is a true but unknown effect.</li>
<li>AIC, BIC, cross-validation, Model selection (best subset, leaps….), Variable selection</li>
<li>More on bias variance tradeoff, show for polynomial regression?</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="simple-linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/03-Multiple_Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
