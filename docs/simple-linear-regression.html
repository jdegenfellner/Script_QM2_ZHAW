<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Simple Linear Regression | Quantitive Methods 2, ZHAW</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Simple Linear Regression | Quantitive Methods 2, ZHAW" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Simple Linear Regression | Quantitive Methods 2, ZHAW" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jürgen Degenfellner" />


<meta name="date" content="2025-01-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#books-we-will-heavily-borrow-from-are"><i class="fa fa-check"></i><b>1.1</b> Books we will heavily borrow from are:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-statistical-modeling-and-what-do-we-need-this-for"><i class="fa fa-check"></i><b>2.1</b> What is statistical modeling and what do we need this for?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#explanatory-vs.-predictive-models"><i class="fa fa-check"></i><b>2.1.1</b> Explanatory vs. Predictive Models</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#individual-vs.-population-prediction"><i class="fa fa-check"></i><b>2.1.2</b> Individual vs. Population Prediction</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#practical-use-of-statistical-models"><i class="fa fa-check"></i><b>2.1.3</b> Practical Use of Statistical Models</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#start-at-the-beginning"><i class="fa fa-check"></i><b>2.1.4</b> Start at the beginning</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#a-simple-model-for-adult-body-heights-in-the-bayesian-framework"><i class="fa fa-check"></i><b>2.2</b> A (simple) model for adult body heights in the Bayesian framework</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#classical-approach-for-the-simplest-model"><i class="fa fa-check"></i><b>2.3</b> Classical approach for the simplest model</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#exercise1_Intro"><i class="fa fa-check"></i><b>2.4.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#exercise2_Intro"><i class="fa fa-check"></i><b>2.4.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#exercise3_Intro"><i class="fa fa-check"></i><b>2.4.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#exercise4_Intro"><i class="fa fa-check"></i><b>2.4.4</b> [M] Exercise 4</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#addendum"><i class="fa fa-check"></i><b>2.5</b> Addendum</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#bivariate_normal"><i class="fa fa-check"></i><b>2.5.1</b> The bivariate normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-in-the-bayesian-framework"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition"><i class="fa fa-check"></i><b>3.1.1</b> Model definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>3.1.2</b> Priors</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit-model"><i class="fa fa-check"></i><b>3.1.3</b> Fit model</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#result"><i class="fa fa-check"></i><b>3.1.4</b> Result</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#credible-bands"><i class="fa fa-check"></i><b>3.1.5</b> Credible bands</a></li>
<li class="chapter" data-level="3.1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition-1"><i class="fa fa-check"></i><b>3.2.1</b> Model definition</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic"><i class="fa fa-check"></i><b>3.2.2</b> Fit the model</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analysis_of_variance"><i class="fa fa-check"></i><b>3.2.3</b> ANOVA (Analysis of Variance)</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r2---coefficient-of-determination"><i class="fa fa-check"></i><b>3.2.4</b> <span class="math inline">\(R^2\)</span> - Coefficient of Determination</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#check-regression-assumptions"><i class="fa fa-check"></i><b>3.2.5</b> Check regression assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise1_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="3.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise2_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="3.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise3_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="3.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise4_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="3.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise5_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.5</b> [M] Exercise 5</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitive Methods 2, ZHAW</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Simple Linear Regression<a href="simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="simple-linear-regression-in-the-bayesian-framework" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression in the Bayesian Framework<a href="simple-linear-regression.html#simple-linear-regression-in-the-bayesian-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now add one covariate/explanatory variable to the model.
Refer to <a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">Statistical Rethinking</a>
“4.4 Linear prediction” or “4.4 Adding a predictor” as it’s called in the online version of the book.</p>
<p>So far, our “regression” did not do much to be honest. The mean of a list of values
was already calculated in the <a href="https://jdegenfellner.github.io/Script_QM1_ZHAW/descriptive_stats.html">descriptive statistics section</a>
before and we have mentioned how great this statistic is as measure of location and where its weaknesses are.</p>
<p>Now, we want to model <strong>how</strong> body height and weight are <strong>related</strong>.
Formally, one wants to <em>predict</em> body heights from body weights.</p>
<p>Here and in the frequentist framework, we will see that it is <strong>not the same</strong>
problem (and therefore results in a different statistical model)
<strong>to predict body weights from body heights or vice versa</strong>.</p>
<p>The word “predictor” is important here. It is a technical term
and describes a variable that we know (in our case weight) and with
which we want to “guess as good as possible” the value of the
dependent variable (in our case height). “As good as possible”
means that we put a penalty on an error. The farer our prediction
is aways from the true value (<span class="math inline">\(y_i\)</span>), the higher the penalty.
And not only that, but if you are twice as far away from the true value,
you should be penalized four times as much. This is the idea behind
the squared error <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>
and the core of the least squares method.
What if we would punish differently you ask?
There are many loss functions one could use, maybe we will see some
later. For now, we punish quadratically.</p>
<p>We <strong>always</strong> visualize the data first to improve our understanding.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="simple-linear-regression.html#cb41-1" tabindex="-1"></a><span class="fu">plot</span>(d2<span class="sc">$</span>height <span class="sc">~</span> d2<span class="sc">$</span>weight)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>It’s not often, that you see such a clean plot.
The scatterplot indicates a linear relationship between the two variables.
The higher the weight, the higher the height; with some deviations of course and
we decide that normally distributed errors are a good idea.
This relationsip is neither causal, not deterministic.</p>
<ul>
<li>It is not causal since an increase in weight does not
necessarily lead to an increase in height, especially in grown-ups.</li>
<li>It is not deterministic since there are deviations from the line.
It if was deterministic, we would not need statistical modeling.</li>
</ul>
<p>For simpler notation, we will call <code>d2$weight</code> <span class="math inline">\(x\)</span>. <span class="math inline">\(\bar{x}\)</span>
is the mean of <span class="math inline">\(x\)</span>.</p>
<div id="model-definition" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Model definition<a href="simple-linear-regression.html#model-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s write down our <strong>model</strong> (again with the Swiss population prior mean):</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma)\\
\mu_i &amp;\sim&amp; \alpha + \beta (x_i - \bar{x})\\
\alpha &amp;\sim&amp; \text{Normal}(171.1, 20)\\
\beta &amp;\sim&amp; \text{Normal}(0, 10)\\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>Visualization of the <strong>model structure</strong>:</p>
<div class="grViz html-widget html-fill-item" id="htmlwidget-aaf4be02e766ebeeecc2" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-aaf4be02e766ebeeecc2">{"x":{"diagram":"\ndigraph model {\n  graph [layout = dot, rankdir = TB]\n\n  # Nodes for variables\n  node [shape = ellipse, style = filled, fillcolor = lightblue]\n  alpha [label = \"α ~ Normal(171.1, 20)\"]\n  beta [label = \"β ~ Normal(0, 10)\"]\n  sigma [label = \"σ ~ Uniform(0, 50)\"]\n  mu [label = \"μ_i = α + β(x_i - x̄)\"]\n  h_i [label = \"h[i] ~ Normal(μ_i, σ)\"]\n\n  # Connections\n  alpha -> mu\n  beta -> mu\n  mu -> h_i\n  sigma -> h_i\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>There are now additional lines for the priors of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.
The model structure also shows the way to simulate from the prior.
One starts at the top and ends up with the heights.</p>
<ul>
<li><span class="math inline">\(h_i\)</span> is the height of the <span class="math inline">\(i\)</span>-th person and we assume it is normally distributed.</li>
<li><span class="math inline">\(\mu_i\)</span> is the mean of the height of the <span class="math inline">\(i\)</span>-th person and we
assume it is linearly dependent on the difference <span class="math inline">\(x_i-\bar{x}\)</span>.
Compared to the intercept model, a different mean is assumed for each person
depending on his/her weight.</li>
<li><span class="math inline">\(\alpha\)</span> is the intercept and we use the same prior as before.</li>
<li><span class="math inline">\(\beta\)</span> is the slope of the line and we use the normal distribution as prior for it,
hence it can be positive or negative and how plausible each value is, is
determined by that specific normal distribution. Note, that we could
easily adapt the distribtion to any distribution we like.</li>
<li>The prior for <span class="math inline">\(\sigma\)</span> is unchanged.</li>
<li><span class="math inline">\(x_i - \bar{x}\)</span> is the deviation of the weight from the mean weight, thereby <strong>we
center</strong> the weight variable. This is a common practice in regression analysis.</li>
</ul>
<p>The linear model is quite popular in applied statistics and one
reason is probably the rather straightforward interpretation of the coefficients.</p>
</div>
<div id="priors" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Priors<a href="simple-linear-regression.html#priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to plot our priors to get a feeling what
the model would predict without seeing the data.
This is a kind of “sanity check” to see if the priors are reasonable.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="simple-linear-regression.html#cb42-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2971</span>)</span>
<span id="cb42-2"><a href="simple-linear-regression.html#cb42-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># 100 lines</span></span>
<span id="cb42-3"><a href="simple-linear-regression.html#cb42-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="fl">171.1</span>, <span class="dv">20</span>)</span>
<span id="cb42-4"><a href="simple-linear-regression.html#cb42-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb42-5"><a href="simple-linear-regression.html#cb42-5" tabindex="-1"></a></span>
<span id="cb42-6"><a href="simple-linear-regression.html#cb42-6" tabindex="-1"></a><span class="co"># Assume d2$weight is defined, e.g., using some dataset or simulation</span></span>
<span id="cb42-7"><a href="simple-linear-regression.html#cb42-7" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb42-8"><a href="simple-linear-regression.html#cb42-8" tabindex="-1"></a></span>
<span id="cb42-9"><a href="simple-linear-regression.html#cb42-9" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>, <span class="dv">400</span>),</span>
<span id="cb42-10"><a href="simple-linear-regression.html#cb42-10" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;height&quot;</span>)</span>
<span id="cb42-11"><a href="simple-linear-regression.html#cb42-11" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># horizontal line at 0</span></span>
<span id="cb42-12"><a href="simple-linear-regression.html#cb42-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">272</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fl">0.5</span>)  <span class="co"># horizontal line at 272</span></span>
<span id="cb42-13"><a href="simple-linear-regression.html#cb42-13" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;b ~ dnorm(0, 10)&quot;</span>)</span>
<span id="cb42-14"><a href="simple-linear-regression.html#cb42-14" tabindex="-1"></a></span>
<span id="cb42-15"><a href="simple-linear-regression.html#cb42-15" tabindex="-1"></a><span class="co"># Overlay the 100 lines</span></span>
<span id="cb42-16"><a href="simple-linear-regression.html#cb42-16" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb42-17"><a href="simple-linear-regression.html#cb42-17" tabindex="-1"></a>  <span class="fu">curve</span>(a[i] <span class="sc">+</span> b[i] <span class="sc">*</span> (x <span class="sc">-</span> xbar),</span>
<span id="cb42-18"><a href="simple-linear-regression.html#cb42-18" tabindex="-1"></a>        <span class="at">from =</span> <span class="fu">min</span>(d2<span class="sc">$</span>weight), <span class="at">to =</span> <span class="fu">max</span>(d2<span class="sc">$</span>weight),</span>
<span id="cb42-19"><a href="simple-linear-regression.html#cb42-19" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(<span class="st">&quot;black&quot;</span>, <span class="fl">0.2</span>))</span>
<span id="cb42-20"><a href="simple-linear-regression.html#cb42-20" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>This relationship seems rather non-restrictive. According to our priors,
one could see very steeply rising or falling lines. We could at least make
the priors for the slope (<span class="math inline">\(\beta\)</span>) non-negative. One possibility to do this
is to use a <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal distribution</a>
for the prior of <span class="math inline">\(\beta\)</span> which can only take non-negative values.</p>
<p><span class="math display">\[ \beta \sim \text{Log-Normal}(0, 1) \]</span></p>
<p>Lets plot the priors again.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="simple-linear-regression.html#cb43-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2971</span>)</span>
<span id="cb43-2"><a href="simple-linear-regression.html#cb43-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># 100 lines</span></span>
<span id="cb43-3"><a href="simple-linear-regression.html#cb43-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="fl">171.1</span>, <span class="dv">20</span>)</span>
<span id="cb43-4"><a href="simple-linear-regression.html#cb43-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rlnorm</span>(N, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb43-5"><a href="simple-linear-regression.html#cb43-5" tabindex="-1"></a></span>
<span id="cb43-6"><a href="simple-linear-regression.html#cb43-6" tabindex="-1"></a><span class="co"># Assume d2$weight is defined, e.g., using some dataset or simulation</span></span>
<span id="cb43-7"><a href="simple-linear-regression.html#cb43-7" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb43-8"><a href="simple-linear-regression.html#cb43-8" tabindex="-1"></a></span>
<span id="cb43-9"><a href="simple-linear-regression.html#cb43-9" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>, <span class="dv">400</span>),</span>
<span id="cb43-10"><a href="simple-linear-regression.html#cb43-10" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;height&quot;</span>)</span>
<span id="cb43-11"><a href="simple-linear-regression.html#cb43-11" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># horizontal line at 0</span></span>
<span id="cb43-12"><a href="simple-linear-regression.html#cb43-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">272</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fl">0.5</span>)  <span class="co"># horizontal line at 272</span></span>
<span id="cb43-13"><a href="simple-linear-regression.html#cb43-13" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;b ~ dlnorm(0, 1)&quot;</span>)</span>
<span id="cb43-14"><a href="simple-linear-regression.html#cb43-14" tabindex="-1"></a></span>
<span id="cb43-15"><a href="simple-linear-regression.html#cb43-15" tabindex="-1"></a><span class="co"># Overlay the 100 lines</span></span>
<span id="cb43-16"><a href="simple-linear-regression.html#cb43-16" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb43-17"><a href="simple-linear-regression.html#cb43-17" tabindex="-1"></a>  <span class="fu">curve</span>(a[i] <span class="sc">+</span> b[i] <span class="sc">*</span> (x <span class="sc">-</span> xbar),</span>
<span id="cb43-18"><a href="simple-linear-regression.html#cb43-18" tabindex="-1"></a>        <span class="at">from =</span> <span class="fu">min</span>(d2<span class="sc">$</span>weight), <span class="at">to =</span> <span class="fu">max</span>(d2<span class="sc">$</span>weight),</span>
<span id="cb43-19"><a href="simple-linear-regression.html#cb43-19" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(<span class="st">&quot;black&quot;</span>, <span class="fl">0.2</span>))</span>
<span id="cb43-20"><a href="simple-linear-regression.html#cb43-20" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>This seems definitely more realistic.</p>
</div>
<div id="fit-model" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Fit model<a href="simple-linear-regression.html#fit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s <strong>estimate the posterior/fit the model</strong> as before:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="simple-linear-regression.html#cb44-1" tabindex="-1"></a><span class="co"># load data again, since it&#39;s a long way back</span></span>
<span id="cb44-2"><a href="simple-linear-regression.html#cb44-2" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb44-3"><a href="simple-linear-regression.html#cb44-3" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb44-4"><a href="simple-linear-regression.html#cb44-4" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb44-5"><a href="simple-linear-regression.html#cb44-5" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> d[d<span class="sc">$</span>age <span class="sc">&gt;=</span> <span class="dv">18</span>, ]</span>
<span id="cb44-6"><a href="simple-linear-regression.html#cb44-6" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb44-7"><a href="simple-linear-regression.html#cb44-7" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb44-8"><a href="simple-linear-regression.html#cb44-8" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb44-9"><a href="simple-linear-regression.html#cb44-9" tabindex="-1"></a>    <span class="fu">alist</span>(</span>
<span id="cb44-10"><a href="simple-linear-regression.html#cb44-10" tabindex="-1"></a>        height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb44-11"><a href="simple-linear-regression.html#cb44-11" tabindex="-1"></a>        mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> (weight <span class="sc">-</span> xbar),</span>
<span id="cb44-12"><a href="simple-linear-regression.html#cb44-12" tabindex="-1"></a>        a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="fl">171.1</span>, <span class="dv">100</span>),</span>
<span id="cb44-13"><a href="simple-linear-regression.html#cb44-13" tabindex="-1"></a>        b <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb44-14"><a href="simple-linear-regression.html#cb44-14" tabindex="-1"></a>        sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb44-15"><a href="simple-linear-regression.html#cb44-15" tabindex="-1"></a>    ) ,</span>
<span id="cb44-16"><a href="simple-linear-regression.html#cb44-16" tabindex="-1"></a><span class="at">data =</span> d2)</span></code></pre></div>
<p>Let’s look at the <strong>marginal distributions</strong> of the parameters:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="simple-linear-regression.html#cb45-1" tabindex="-1"></a><span class="fu">precis</span>(mod)</span></code></pre></div>
<pre><code>##              mean         sd        5.5%       94.5%
## a     154.5972120 0.27033045 154.1651717 155.0292523
## b       0.9050131 0.04192754   0.8380048   0.9720214
## sigma   5.0718673 0.19115323   4.7663675   5.3773671</code></pre>
<p>The analysis yields estimates for all our parameters of the model: <span class="math inline">\(\alpha\)</span>,
<span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span>. The estimates are the mean of the posterior distribution.</p>
<p>See <a href="simple-linear-regression.html#exercise2_simpl_lin_reg">exercise 2</a>.</p>
<p><strong>Interpretation of <span class="math inline">\(\beta\)</span></strong>:
The mean of the posterior distribution of <span class="math inline">\(\beta\)</span> is 0.9. A person with a weight
of 1 kg more weight can be expected to be 0.9 cm taller. A 96% credible interval
for this estimate is <span class="math inline">\([0.83, 0.97]\)</span>. We can be quite sure that the slope is
positive.</p>
<p>It might also be interesting to inspect the variance-covariance matrix,
respectively the correlation between the parameters as we did before
in the intercept model.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="simple-linear-regression.html#cb47-1" tabindex="-1"></a><span class="fu">diag</span>(<span class="fu">vcov</span>(mod))</span></code></pre></div>
<pre><code>##           a           b       sigma 
## 0.073078550 0.001757918 0.036539558</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-linear-regression.html#cb49-1" tabindex="-1"></a><span class="fu">cov2cor</span>(<span class="fu">vcov</span>(mod))</span></code></pre></div>
<pre><code>##                   a             b         sigma
## a      1.000000e+00 -9.591866e-10  3.330963e-05
## b     -9.591866e-10  1.000000e+00 -2.879607e-05
## sigma  3.330963e-05 -2.879607e-05  1.000000e+00</code></pre>
<p>As we can see the correlations are near zero. Compare to the graphical
display of the model structure. There is no connection.</p>
</div>
<div id="result" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Result<a href="simple-linear-regression.html#result" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Graphical end result</strong> of fitting the model:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-linear-regression.html#cb51-1" tabindex="-1"></a><span class="fu">plot</span>(d2<span class="sc">$</span>height <span class="sc">~</span> d2<span class="sc">$</span>weight, <span class="at">col =</span> rangi2)</span>
<span id="cb51-2"><a href="simple-linear-regression.html#cb51-2" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(mod)</span>
<span id="cb51-3"><a href="simple-linear-regression.html#cb51-3" tabindex="-1"></a>a_quap <span class="ot">&lt;-</span> <span class="fu">mean</span>(post<span class="sc">$</span>a)</span>
<span id="cb51-4"><a href="simple-linear-regression.html#cb51-4" tabindex="-1"></a>b_quap <span class="ot">&lt;-</span> <span class="fu">mean</span>(post<span class="sc">$</span>b)</span>
<span id="cb51-5"><a href="simple-linear-regression.html#cb51-5" tabindex="-1"></a><span class="fu">curve</span>(a_quap <span class="sc">+</span> b_quap <span class="sc">*</span> (x <span class="sc">-</span> xbar), <span class="at">add =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<div id="credible-bands" class="section level3 hasAnchor" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Credible bands<a href="simple-linear-regression.html#credible-bands" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We could draw again and again from the posterior distribution
and calculate the means like above. Plotting the regression lines
with the respective parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> would indicate
the variability of the estimates.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="simple-linear-regression.html#cb52-1" tabindex="-1"></a><span class="co"># Define a sequence of weights for predictions</span></span>
<span id="cb52-2"><a href="simple-linear-regression.html#cb52-2" tabindex="-1"></a>weight.seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">25</span>, <span class="at">to =</span> <span class="dv">70</span>, <span class="at">by =</span> <span class="dv">1</span>)</span>
<span id="cb52-3"><a href="simple-linear-regression.html#cb52-3" tabindex="-1"></a></span>
<span id="cb52-4"><a href="simple-linear-regression.html#cb52-4" tabindex="-1"></a><span class="co"># Use the model to compute mu for each weight</span></span>
<span id="cb52-5"><a href="simple-linear-regression.html#cb52-5" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">link</span>(mod, <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">weight =</span> weight.seq))</span>
<span id="cb52-6"><a href="simple-linear-regression.html#cb52-6" tabindex="-1"></a><span class="fu">str</span>(mu)</span></code></pre></div>
<pre><code>##  num [1:1000, 1:46] 138 136 137 136 137 ...</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="simple-linear-regression.html#cb54-1" tabindex="-1"></a><span class="co"># Visualize the distribution of mu values</span></span>
<span id="cb54-2"><a href="simple-linear-regression.html#cb54-2" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>)  <span class="co"># Hide raw data with type = &quot;n&quot;</span></span>
<span id="cb54-3"><a href="simple-linear-regression.html#cb54-3" tabindex="-1"></a></span>
<span id="cb54-4"><a href="simple-linear-regression.html#cb54-4" tabindex="-1"></a><span class="co"># Loop over samples and plot each mu value</span></span>
<span id="cb54-5"><a href="simple-linear-regression.html#cb54-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb54-6"><a href="simple-linear-regression.html#cb54-6" tabindex="-1"></a>  <span class="fu">points</span>(weight.seq, mu[i, ], <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.1</span>))</span>
<span id="cb54-7"><a href="simple-linear-regression.html#cb54-7" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>The <code>link</code> function fixes the weight at the values in <code>weight.seq</code> and
draws samples from the posterior distribution of the parameters. We will do
the analog thing in the frequentist framework.</p>
<p>We can also draw a nice shade for the regression line:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-linear-regression.html#cb55-1" tabindex="-1"></a><span class="co"># Summarize the distribution of mu</span></span>
<span id="cb55-2"><a href="simple-linear-regression.html#cb55-2" tabindex="-1"></a>mu.mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, mean)</span>
<span id="cb55-3"><a href="simple-linear-regression.html#cb55-3" tabindex="-1"></a>mu.PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb55-4"><a href="simple-linear-regression.html#cb55-4" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.5</span>))</span>
<span id="cb55-5"><a href="simple-linear-regression.html#cb55-5" tabindex="-1"></a><span class="fu">lines</span>(weight.seq, mu.mean)</span>
<span id="cb55-6"><a href="simple-linear-regression.html#cb55-6" tabindex="-1"></a><span class="fu">shade</span>(mu.PI, weight.seq)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>As we can see, we are pretty sure about the mean of height which
we wanted to model in the first place.
Mean modeling is one thing, individual prediction is another.
Given a certain weight of a person, what is the height of the same person?
The first line in the model definition (<span class="math inline">\(height_i \sim Normal(\mu_i, \sigma)\)</span>)
tells us that a person’s weight is distributed <em>around</em> the mean
(which linearly depends on weight) and is not necessary the mean itself.</p>
<p>To get to an <strong>individual prediction</strong>, we need to consider the uncertainty
of the parameter estimation <em>and</em> the uncertainty from the Gaussian distribution
around the mean (at a certain weight). We do this with <code>sim</code>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="simple-linear-regression.html#cb56-1" tabindex="-1"></a><span class="co"># Simulate heights from the posterior</span></span>
<span id="cb56-2"><a href="simple-linear-regression.html#cb56-2" tabindex="-1"></a>sim.height <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">data =</span> <span class="fu">list</span>(<span class="at">weight =</span> weight.seq))</span>
<span id="cb56-3"><a href="simple-linear-regression.html#cb56-3" tabindex="-1"></a><span class="fu">str</span>(sim.height)</span></code></pre></div>
<pre><code>##  num [1:1000, 1:46] 138 130 130 147 136 ...</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="simple-linear-regression.html#cb58-1" tabindex="-1"></a><span class="co"># Compute the 89% prediction interval for simulated heights</span></span>
<span id="cb58-2"><a href="simple-linear-regression.html#cb58-2" tabindex="-1"></a>height.PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(sim.height, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb58-3"><a href="simple-linear-regression.html#cb58-3" tabindex="-1"></a></span>
<span id="cb58-4"><a href="simple-linear-regression.html#cb58-4" tabindex="-1"></a><span class="co"># Plot the raw data</span></span>
<span id="cb58-5"><a href="simple-linear-regression.html#cb58-5" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.5</span>))</span>
<span id="cb58-6"><a href="simple-linear-regression.html#cb58-6" tabindex="-1"></a></span>
<span id="cb58-7"><a href="simple-linear-regression.html#cb58-7" tabindex="-1"></a><span class="co"># Draw MAP (mean a posteriori) line</span></span>
<span id="cb58-8"><a href="simple-linear-regression.html#cb58-8" tabindex="-1"></a><span class="fu">lines</span>(weight.seq, mu.mean)</span>
<span id="cb58-9"><a href="simple-linear-regression.html#cb58-9" tabindex="-1"></a></span>
<span id="cb58-10"><a href="simple-linear-regression.html#cb58-10" tabindex="-1"></a><span class="co"># Draw HPDI (highest posterior density interval) region for mu</span></span>
<span id="cb58-11"><a href="simple-linear-regression.html#cb58-11" tabindex="-1"></a><span class="fu">shade</span>(mu.PI, weight.seq)</span>
<span id="cb58-12"><a href="simple-linear-regression.html#cb58-12" tabindex="-1"></a></span>
<span id="cb58-13"><a href="simple-linear-regression.html#cb58-13" tabindex="-1"></a><span class="co"># Draw PI (prediction interval) region for simulated heights</span></span>
<span id="cb58-14"><a href="simple-linear-regression.html#cb58-14" tabindex="-1"></a><span class="fu">shade</span>(height.PI, weight.seq)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>The lighter and wider shaded region is where the model expects to find 89%
of the heights of a person with a certain weight.</p>
<p>This part is sometimes a bit desillusioning when seen for the first time:
Draw a horizontal line at 150 cm and see how many weights (according to
the individual prediction) are compatible with this height. Weights
from 30 to 50 kg are compatible with this height according to the
89% prediction interval. The higher the credibility, the wider the interval,
the wider the range of compatible weights (more than 60% of the weight-range).</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="simple-linear-regression.html#cb59-1" tabindex="-1"></a>(<span class="dv">50</span> <span class="sc">-</span> <span class="dv">30</span>) <span class="sc">/</span> (<span class="fu">range</span>(d2<span class="sc">$</span>weight)[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight)[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.6265362</code></pre>
</div>
<div id="summary" class="section level3 hasAnchor" number="3.1.6">
<h3><span class="header-section-number">3.1.6</span> Summary<a href="simple-linear-regression.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>We have added a covariate (weight) to the simple mean model to predict height.</li>
<li>We have centered the weight variable.</li>
<li>We have defined and refined priors for the intercept and slope.</li>
<li>We have estimated the posterior distribution of the parameters using quadratic approximation with <code>quap</code>.</li>
<li>We have visualized the result.</li>
<li>We have created credible bands for mean and individual predictions.</li>
</ul>
</div>
</div>
<div id="simple-linear-regression-in-the-frequentist-framework" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Simple Linear Regression in the Frequentist Framework<a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now do the same analysis in the frequentist framework while introducing
some foundational theory along the way.
I recommend reading the first couple of chapters from <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>.</p>
<div id="model-definition-1" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Model definition<a href="simple-linear-regression.html#model-definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our linear model is defined as:</p>
<p><span class="math display">\[ h_i = \beta_0 + \beta_1 x_i + \varepsilon_i \]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\varepsilon_i\)</span> is the error term with <span class="math inline">\(\varepsilon_i \sim N(0, \sigma), \forall i\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> is the unknown but fixed intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the unknown but fixed slope</li>
</ul>
<div id="model-assumptions-of-the-classical-regression-model-westfall-1.7" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Model Assumptions of the Classical Regression Model (<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>, 1.7):<a href="simple-linear-regression.html#model-assumptions-of-the-classical-regression-model-westfall-1.7" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first and <strong>most important assumption</strong> is that the data are produced<br />
probabilistically, which is specifically stated as
<span class="math display">\[ Y|X = x \sim p(y|x)\]</span></p>
<p>What does this mean?</p>
<ul>
<li><span class="math inline">\(Y|X = x\)</span> is the random variable Y <strong>conditional</strong> on X being equal to x, i.e. the
distribution of <span class="math inline">\(Y\)</span> if we know the value of <span class="math inline">\(X\)</span> (in our example the weight in kg).
<a href="https://blogs.sas.com/content/iml/files/2015/09/GLM_normal_identity.png">This</a> is a nice image of what is meant here.</li>
<li><span class="math inline">\(p(y|x)\)</span> is the distribution of potentially observable <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>.
In our case above this was the normal distribution with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>You can play with <a href="https://psychmeth.shinyapps.io/Regression-NVFehler/">this shiny app</a> to improve your understanding.
It offers the option “Bedingte Verteilung anzeigen”.</p>
<p>One always things about the so-called
<a href="https://en.wikipedia.org/wiki/Data_generating_process">data generating process</a>
(<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>, 1.2).
How did the data come about? There is a process behind it and this process
is attempted to be modeled.</p>
<p>Further assumptions:</p>
<ul>
<li><p>Correct functional specification: The conditional mean function <span class="math inline">\(f(x) = \mathbb{E}(Y|X=x)\)</span>.
In the case of the linear model, the assumption is <span class="math inline">\(\mathbb{E}(Y|X=x) = \alpha + \beta x\)</span>.
The expectation of <span class="math inline">\(Y\)</span> (height) depends linearly on <span class="math inline">\(x\)</span> (weight).</p></li>
<li><p>The errors are homoscedastic (constant variance <span class="math inline">\(\sigma\)</span>). This means the
variances of all conditional distributions <span class="math inline">\(p(y|x)\)</span> are constant (<span class="math inline">\(=\sigma^2\)</span>).</p></li>
<li><p>Normality. For the classical linear regression model all the conditional
distributions <span class="math inline">\(p(y|x)\)</span> are normal distributions.</p></li>
<li><p>The errors are independent of each other.
The potentially observable <span class="math inline">\(\varepsilon_i = Y_i - f(\mathbf{x_i}, \mathbf{\beta})\)</span>
is uncorrelated with <span class="math inline">\(\varepsilon_j = Y_j - f(\mathbf{x_j}, \mathbf{\beta})\)</span> for
<span class="math inline">\(i \neq j\)</span>.</p></li>
</ul>
<p>These assumptions become clearer as we go along and should be checked
for every model we fit. They are not connected, they can all be true or false.
The question is not “Are the assumptions met?” since they never are exactly met.
The question is <strong>how</strong> “badly” the assumptions are violated?</p>
<p>Remember, <strong>all models are wrong, but some are useful</strong>.</p>
<p>In full, the classical linear regression model can be written as:</p>
<p><span class="math display">\[ Y_i|X_i = x_i \sim_{independent} N(\beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik},\sigma^2)\]</span>
for <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
</div>
</div>
<div id="fit_model_simple_lin_reg_classic" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Fit the model<a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Again, we fit the model using the least squares method. For a neat animated explanation,
visit <a href="https://www.youtube.com/watch?v=jEEJNz0RK4Q&amp;ab_channel=COCCmath">this video</a>.
One has to minimize the sum of squared differences between the true heights and
the model-predicted heights in order to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p><span class="math display">\[ SSE(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 \]</span></p>
<p>We omit the technical details and give the results for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - (\hat{\beta_1} \bar{x}),
\]</span>
<span class="math display">\[
\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} =
\frac{s_{x,y}}{s_x^2} = r_{xy} \frac{s_y}{s_x}.
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(r_{xy}\)</span> is the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">sample correlation coefficient</a> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the <a href="https://en.wikipedia.org/wiki/Standard_deviation">uncorrected sample standard deviations</a> of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(s_x^2\)</span> and <span class="math inline">\(s_{xy}\)</span> are the <a href="https://en.wikipedia.org/wiki/Variance">sample variance</a> and <a href="https://en.wikipedia.org/wiki/Covariance">sample covariance</a>, respectively</li>
</ul>
<p>Interpretation of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>: see <a href="simple-linear-regression.html#exercise3_simpl_lin_reg">exercise 3</a>.</p>
<p>Let’s use R again to solve the problem:</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="simple-linear-regression.html#cb61-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb61-2"><a href="simple-linear-regression.html#cb61-2" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb61-3"><a href="simple-linear-regression.html#cb61-3" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb61-4"><a href="simple-linear-regression.html#cb61-4" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> d[d<span class="sc">$</span>age <span class="sc">&gt;=</span> <span class="dv">18</span>, ]</span>
<span id="cb61-5"><a href="simple-linear-regression.html#cb61-5" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight, <span class="at">data =</span> d2)</span>
<span id="cb61-6"><a href="simple-linear-regression.html#cb61-6" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = height ~ weight, data = d2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.7464  -2.8835   0.0222   3.1424  14.7744 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 113.87939    1.91107   59.59   &lt;2e-16 ***
## weight        0.90503    0.04205   21.52   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.086 on 350 degrees of freedom
## Multiple R-squared:  0.5696, Adjusted R-squared:  0.5684 
## F-statistic: 463.3 on 1 and 350 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation of R-output</strong>:</p>
<ul>
<li><code>Call</code>: The model that was fitted.</li>
<li><code>Residuals</code>: <span class="math inline">\(r_i = height_i - \widehat{height}_i\)</span>.
Difference between true heights and model-predicted heights.</li>
<li><code>Coefficients</code>: The estimated <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
<ul>
<li><code>Estimate</code>: The (least squares) estimated value of the coefficient.</li>
<li><code>Std. Error</code>: The standard error of the estimate.</li>
<li><code>t value</code>: The value of the <span class="math inline">\(t\)</span>-statistic for the (Wald-) hypothesis test
<span class="math inline">\(H_0: \beta_i = 0\)</span>.</li>
<li><code>Pr(&gt;|t|)</code>: The <span class="math inline">\(p\)</span>-value of the hypothesis test.</li>
</ul></li>
<li><code>Residual standard error</code>: The estimate of <span class="math inline">\(\sigma\)</span>
which is also a model parameter (as in the Bayesian framework).</li>
<li><code>Multiple R-squared</code>: The proportion of the variance explained by the
model (we will explain this below).</li>
<li><code>Adjusted R-squared</code>: A corrected version of the <span class="math inline">\(R^2\)</span> which takes into account
the number of predictors in the model.</li>
<li><code>F-statistic</code>: The value of the <span class="math inline">\(F\)</span>-statistic for the hypothesis test:
<span class="math inline">\(H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0\)</span>. Note, the alternative
hypotheses to this test is that <em>any</em> of the <span class="math inline">\(\beta_i\)</span> is not zero. If that Is
the case, the model explains more than the mean model with just <span class="math inline">\(\beta_0\)</span>.</li>
</ul>
<p>We could also <strong>solve the problem graphically</strong>: We want to find the
values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squared differences
which can be plotted as 3D function. All we have to do is to ask R which of
the coordinates minimizes the sum of squared errors. The result confirmes
the results from the <code>lm</code> function.The dot in red marks the spot (Code is in the git repository):</p>
<p><strong>—COMPILE CODE AT DEOPLOYMENT—</strong></p>
</div>
<div id="analysis_of_variance" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> ANOVA (Analysis of Variance)<a href="simple-linear-regression.html#analysis_of_variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A non-obvious and very useful finding is that the total variability in the data can be
<strong>decomposed</strong> (or <a href="https://en.wiktionary.org/wiki/analysis">analysed</a>)
into two parts:</p>
<ul>
<li>The variability explained by the model (the regression line)</li>
<li>The variability not explained by the model (the residuals)</li>
</ul>
<p><span class="math display">\[ \text{Sum of Squares in Total} = \text{Sum of Squares from Regression} + \text{Sum of Squared Errors} \]</span></p>
<p><span class="math display">\[ SST = SSR + SSE \]</span></p>
<p><span class="math display">\[ \sum (y_i - \bar{y})^2 = \sum (\hat{y}_i - \bar{y})^2 + \sum (y_i - \hat{y}_i)^2 \]</span></p>
<p>If you are interested in the details, check out <a href="https://en.wikipedia.org/wiki/Explained_sum_of_squares#Partitioning_in_the_general_ordinary_least_squares_model">this</a>.</p>
<p><a href="https://www.youtube.com/watch?v=NxRTs7sXKAQ&amp;ab_channel=365DataScience">This video</a>
explains the concept nicely.</p>
<p>Let’s visualize our regression result:</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="r2---coefficient-of-determination" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> <span class="math inline">\(R^2\)</span> - Coefficient of Determination<a href="simple-linear-regression.html#r2---coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><span class="math inline">\(R^2\)</span></a>
is the <strong>amount of variance explained by the model</strong>.
You can also read <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a> 8.1.</p>
<p>As you can see
<a href="simple-linear-regression.html#analysis_of_variance">above</a>, the total variance (SST) of our outcome (height)
can be decomposed into two parts: the variance explained by the model (SSR)
and the variance not explained by the model (SSE).</p>
<p>Maybe the most intuitive definition of <span class="math inline">\(R^2\)</span> is:</p>
<p><span class="math display">\[ R^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>The value is between 0 and 1. The higher the value, the more variance is explained.
But be cautious. Depending on the context, a really high <span class="math inline">\(R^2\)</span> is not
necessarily a good thing. With the data we are working with,
it could easily hint towards an error. If we are near 1,
all points in the simple linear regression model are on the line.
If we are near 0, the model does not explain much of the variance
and we would see “noise with no slope” in the scatterplot (<a href="simple-linear-regression.html#exercise4_simpl_lin_reg">exercise 4</a>).
The normal <span class="math inline">\(R^2\)</span> can be found in the R output under <code>Multiple R-squared</code>.</p>
<p>If you add a lot of variables to your regression model, you can get an arbitrarily large (<span class="math inline">\(\le 1\)</span>)
<span class="math inline">\(R^2\)</span>. We will verify this when we have more than 2 explanatory variables.
As a non-formal explanation for this: In the Sum of Squares Errors (SSE),
if you add more covariates (<span class="math inline">\(\beta_2, \beta_3\)</span>), you have more freedom
to choose values that minimize the number that will be squared. Simple regression
is just a special case of multiple (more than one predictor) regression with <span class="math inline">\(\beta_2=\beta_3=\dots=0\)</span>.
Hence, you will definitely not be worse off with regards to SSE when using more covariates.
A smaller SSE implies a larger SSR (sum constraint) and hence a larger <span class="math inline">\(R^2\)</span>. SST remains constant.</p>
<p>Although not perfect, one way to mitigate the influence of “too many” variables
on <span class="math inline">\(R^2\)</span> is to use the adjusted <span class="math inline">\(R^2\)</span>, which an also be found in the R output (<code>Adjusted R-squared</code>).</p>
<div id="seperating-property-of-regression-due-to-r2" class="section level4 hasAnchor" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Seperating property of regression due to <span class="math inline">\(R^2\)</span>:<a href="simple-linear-regression.html#seperating-property-of-regression-due-to-r2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Peter Westfall</a>
explains (in Figure 8.1 of the book) how <span class="math inline">\(R^2\)</span> influences the separation of distributions in our simple regression model.</p>
<p>In our regression of height on weight (order is correct, that’s how you say it),
the <span class="math inline">\(R^2\)</span> is <span class="math inline">\(0.5696\)</span>. The following plot shows how well one can predict
height if we use the 10% and 90% quantile of the weights (x_low and x_high).
In both, you see the conditional distribution of height given the weight <span class="math inline">\(X = x_{low}\)</span> or <span class="math inline">\(X = x_{high}\)</span>.
Scenario 1 is the original model, scenario 2 is the same data with added noise (in Y-direction),
which reduces <span class="math inline">\(R^2\)</span> to <span class="math inline">\(0.13\)</span>, much lower. In the right plot, the distributions have a large
overlap and it is hard to distinguish between heights when knowing even very low (10% quantile)
or very high (90% quantile) weights.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>See also <a href="simple-linear-regression.html#exercise5_simpl_lin_reg">exercise 5</a>.</p>
</div>
</div>
<div id="check-regression-assumptions" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Check regression assumptions<a href="simple-linear-regression.html#check-regression-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>……….</p>
<p>……. <strong>TODOS</strong></p>
<ul>
<li>random X vs fixed X</li>
<li>If you add a lot of variables to your regression model, you can get an arbitrarily large (<span class="math inline">\(\le 1\)</span>)
<span class="math inline">\(R^2\)</span>. We will verify this when we have more than 2 explanatory variables.</li>
</ul>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Exercises<a href="simple-linear-regression.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exercise1_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> [E] Exercise 1<a href="simple-linear-regression.html#exercise1_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the model from above:</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma)\\
\mu_i &amp;\sim&amp; \alpha + \beta (x_i - \bar{x})\\
\alpha &amp;\sim&amp; \text{Normal}(171.1, 20)\\
\beta &amp;\sim&amp; \text{Normal}(0, 10)\\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<ul>
<li>What ist the expected height when <span class="math inline">\(x_i = \bar{x}\)</span>?</li>
<li>What is the expected height when <span class="math inline">\(x_i\)</span> changes by 1 unit?</li>
</ul>
</div>
<div id="exercise2_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> [E] Exercise 2<a href="simple-linear-regression.html#exercise2_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Look at the marginal distrubutions of the parameters in the Bayesian model.</p>
<ul>
<li>Plot the posterior distribution of all 3 parameters.</li>
<li>Include in the plot a 99% credible interval (HDI).</li>
</ul>
</div>
<div id="exercise3_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> [M] Exercise 3<a href="simple-linear-regression.html#exercise3_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go to the coefficient estimates in the simple linear regression setting
above (<a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic">Fit the model</a>)
in the classical framework.</p>
<ul>
<li>Create an R file to simulate the simple linear regression model.</li>
<li>Change your input parameters and see how the estimates change.</li>
<li>Does this make sense with respect to the estimates given, specifically with respect
to <span class="math inline">\(\beta_1\)</span>?</li>
</ul>
</div>
<div id="exercise4_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> [M] Exercise 4<a href="simple-linear-regression.html#exercise4_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verify the statement above in the text for high and low values of <span class="math inline">\(R^2\)</span>.</p>
</div>
<div id="exercise5_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> [M] Exercise 5<a href="simple-linear-regression.html#exercise5_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verify with simulation in R that the separation of the distributions in the simple linear regression model
improves if the true (but usually unknown) slope increases.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-Simple_Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
