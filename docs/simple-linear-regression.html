<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Simple Linear Regression | Quantitative Methods 2, ZHAW</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Simple Linear Regression | Quantitative Methods 2, ZHAW" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Simple Linear Regression | Quantitative Methods 2, ZHAW" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="Jürgen Degenfellner" />


<meta name="date" content="2025-02-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="multiple-linear-regression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.11/grViz.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantitative Methods 2</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#books-we-will-heavily-borrow-from-are"><i class="fa fa-check"></i><b>1.1</b> Books we will heavily borrow from are:</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#if-you-need-a-good-reason-to-buy-great-books"><i class="fa fa-check"></i><b>1.2</b> If you need a good reason to buy great books…</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-statistical-modeling-and-what-do-we-need-this-for"><i class="fa fa-check"></i><b>2.1</b> What is statistical modeling and what do we need this for?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="intro.html"><a href="intro.html#explanatory-vs.-predictive-models"><i class="fa fa-check"></i><b>2.1.1</b> Explanatory vs. Predictive Models</a></li>
<li class="chapter" data-level="2.1.2" data-path="intro.html"><a href="intro.html#individual-vs.-population-prediction"><i class="fa fa-check"></i><b>2.1.2</b> Individual vs. Population Prediction</a></li>
<li class="chapter" data-level="2.1.3" data-path="intro.html"><a href="intro.html#practical-use-of-statistical-models"><i class="fa fa-check"></i><b>2.1.3</b> Practical Use of Statistical Models</a></li>
<li class="chapter" data-level="2.1.4" data-path="intro.html"><a href="intro.html#start-at-the-beginning"><i class="fa fa-check"></i><b>2.1.4</b> Start at the beginning</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#a-simple-model-for-adult-body-heights-in-the-bayesian-framework"><i class="fa fa-check"></i><b>2.2</b> A (simple) model for adult body heights in the Bayesian framework</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#classical-approach-for-the-simplest-model"><i class="fa fa-check"></i><b>2.3</b> Classical approach for the simplest model</a></li>
<li class="chapter" data-level="2.4" data-path="intro.html"><a href="intro.html#exercises"><i class="fa fa-check"></i><b>2.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro.html"><a href="intro.html#exercise1_Intro"><i class="fa fa-check"></i><b>2.4.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro.html"><a href="intro.html#exercise2_Intro"><i class="fa fa-check"></i><b>2.4.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro.html"><a href="intro.html#exercise3_Intro"><i class="fa fa-check"></i><b>2.4.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="2.4.4" data-path="intro.html"><a href="intro.html#exercise4_Intro"><i class="fa fa-check"></i><b>2.4.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro.html"><a href="intro.html#exercise5_Intro"><i class="fa fa-check"></i><b>2.4.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="2.4.6" data-path="intro.html"><a href="intro.html#exercise6_Intro"><i class="fa fa-check"></i><b>2.4.6</b> [M] Exercise 6</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro.html"><a href="intro.html#addendum"><i class="fa fa-check"></i><b>2.5</b> Addendum</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro.html"><a href="intro.html#bivariate_normal"><i class="fa fa-check"></i><b>2.5.1</b> The bivariate normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple_lin_reg_bayes"><i class="fa fa-check"></i><b>3.1</b> Simple Linear Regression in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition"><i class="fa fa-check"></i><b>3.1.1</b> Model definition</a></li>
<li class="chapter" data-level="3.1.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#priors"><i class="fa fa-check"></i><b>3.1.2</b> Priors</a></li>
<li class="chapter" data-level="3.1.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit-model"><i class="fa fa-check"></i><b>3.1.3</b> Fit model</a></li>
<li class="chapter" data-level="3.1.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#result"><i class="fa fa-check"></i><b>3.1.4</b> Result</a></li>
<li class="chapter" data-level="3.1.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#credible-bands"><i class="fa fa-check"></i><b>3.1.5</b> Credible bands</a></li>
<li class="chapter" data-level="3.1.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#summary"><i class="fa fa-check"></i><b>3.1.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-definition-1"><i class="fa fa-check"></i><b>3.2.1</b> Model definition</a></li>
<li class="chapter" data-level="3.2.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic"><i class="fa fa-check"></i><b>3.2.2</b> Fit the model</a></li>
<li class="chapter" data-level="3.2.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#confidence_intervals_frequentist"><i class="fa fa-check"></i><b>3.2.3</b> Confidence Intervals of coefficients (frequentist)</a></li>
<li class="chapter" data-level="3.2.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#analysis_of_variance"><i class="fa fa-check"></i><b>3.2.4</b> ANOVA (Analysis of Variance)</a></li>
<li class="chapter" data-level="3.2.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#r2---coefficient-of-determination"><i class="fa fa-check"></i><b>3.2.5</b> <span class="math inline">\(R^2\)</span> - Coefficient of Determination</a></li>
<li class="chapter" data-level="3.2.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#check-regression-assumptions"><i class="fa fa-check"></i><b>3.2.6</b> Check regression assumptions</a></li>
<li class="chapter" data-level="3.2.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#bootstrap-fit"><i class="fa fa-check"></i><b>3.2.7</b> Bootstrap fit</a></li>
<li class="chapter" data-level="3.2.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#regression-towards-the-mean"><i class="fa fa-check"></i><b>3.2.8</b> Regression towards the mean</a></li>
<li class="chapter" data-level="3.2.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#random-x-vs.-fixed-x"><i class="fa fa-check"></i><b>3.2.9</b> Random X vs. fixed X</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>3.3</b> Exercises</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise1_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.1</b> [E] Exercise 1</a></li>
<li class="chapter" data-level="3.3.2" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise2_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="3.3.3" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise3_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.3</b> [M] Exercise 3</a></li>
<li class="chapter" data-level="3.3.4" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise4_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.4</b> [M] Exercise 4</a></li>
<li class="chapter" data-level="3.3.5" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise5_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.5</b> [M] Exercise 5</a></li>
<li class="chapter" data-level="3.3.6" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise6_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.6</b> [H] Exercise 6</a></li>
<li class="chapter" data-level="3.3.7" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise7_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.7</b> [M] Exercise 7</a></li>
<li class="chapter" data-level="3.3.8" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise8_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.8</b> [E] Exercise 8</a></li>
<li class="chapter" data-level="3.3.9" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise9_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.9</b> [M] Exercise 9</a></li>
<li class="chapter" data-level="3.3.10" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise10_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.10</b> [M] Exercise 10</a></li>
<li class="chapter" data-level="3.3.11" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise11_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.11</b> [E] Exercise 11</a></li>
<li class="chapter" data-level="3.3.12" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise12_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.12</b> [M] Exercise 12</a></li>
<li class="chapter" data-level="3.3.13" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise13_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.13</b> [M] Exercise 13</a></li>
<li class="chapter" data-level="3.3.14" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#exercise14_simpl_lin_reg"><i class="fa fa-check"></i><b>3.3.14</b> [M] Exercise 14</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-bayesian-framework"><i class="fa fa-check"></i><b>4.1</b> Linear Regression with 2 predictors in the Bayesian Framework</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#meaning-of-linear"><i class="fa fa-check"></i><b>4.1.1</b> Meaning of “linear”</a></li>
<li class="chapter" data-level="4.1.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor"><i class="fa fa-check"></i><b>4.1.2</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.1.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_bayes"><i class="fa fa-check"></i><b>4.1.3</b> Adding another predictor to the model</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#linear-regression-with-2-predictors-in-the-frequentist-framework"><i class="fa fa-check"></i><b>4.2</b> Linear regression with 2 predictors in the Frequentist Framework</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_transformed_predictor_freq"><i class="fa fa-check"></i><b>4.2.1</b> Adding a transformed predictor to the model</a></li>
<li class="chapter" data-level="4.2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#adding_predictor_freq"><i class="fa fa-check"></i><b>4.2.2</b> Adding another predictor to the model</a></li>
<li class="chapter" data-level="4.2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_term"><i class="fa fa-check"></i><b>4.2.3</b> Interaction Term <span class="math inline">\(X_1 \times X_2\)</span></a></li>
<li class="chapter" data-level="4.2.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#interaction_plot"><i class="fa fa-check"></i><b>4.2.4</b> Using an interaction plot to see a potential interaction</a></li>
<li class="chapter" data-level="4.2.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#simpsons_paradox"><i class="fa fa-check"></i><b>4.2.5</b> Simpsons Paradox</a></li>
<li class="chapter" data-level="4.2.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#throwing_variables"><i class="fa fa-check"></i><b>4.2.6</b> What happens when you just throw variables into multiple regression?</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#more-than-2-predictors"><i class="fa fa-check"></i><b>4.3</b> More than 2 predictors</a></li>
<li class="chapter" data-level="4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercises-2"><i class="fa fa-check"></i><b>4.4</b> Exercises</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise1_multiple_regression"><i class="fa fa-check"></i><b>4.4.1</b> [M] Exercise 1</a></li>
<li class="chapter" data-level="4.4.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise2_multiple_regression"><i class="fa fa-check"></i><b>4.4.2</b> [E] Exercise 2</a></li>
<li class="chapter" data-level="4.4.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise3_multiple_regression"><i class="fa fa-check"></i><b>4.4.3</b> [H] Exercise 3</a></li>
<li class="chapter" data-level="4.4.4" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise4_multiple_regression"><i class="fa fa-check"></i><b>4.4.4</b> [E] Exercise 4</a></li>
<li class="chapter" data-level="4.4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise5_multiple_regression"><i class="fa fa-check"></i><b>4.4.5</b> [E] Exercise 5</a></li>
<li class="chapter" data-level="4.4.6" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise6_multiple_regression"><i class="fa fa-check"></i><b>4.4.6</b> [M] Exercise 6</a></li>
<li class="chapter" data-level="4.4.7" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise7_multiple_regression"><i class="fa fa-check"></i><b>4.4.7</b> [E] Exercise 7</a></li>
<li class="chapter" data-level="4.4.8" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise8_multiple_regression"><i class="fa fa-check"></i><b>4.4.8</b> [M] Exercise 8</a></li>
<li class="chapter" data-level="4.4.9" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#exercise9_multiple_regression"><i class="fa fa-check"></i><b>4.4.9</b> [M] Exercise 9</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#todos"><i class="fa fa-check"></i><b>4.5</b> TODOS</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantitative Methods 2, ZHAW</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simple-linear-regression" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Simple Linear Regression<a href="simple-linear-regression.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="simple_lin_reg_bayes" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Simple Linear Regression in the Bayesian Framework<a href="simple-linear-regression.html#simple_lin_reg_bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>You can watch <a href="https://www.youtube.com/watch?v=14mkCpJ7tKs&amp;ab_channel=VeryNormal">this video</a> as primer.</p>
<p>We will now add one covariate/explanatory variable to the model.
Refer to <a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">Statistical Rethinking</a>
“4.4 Linear prediction” or “4.4 Adding a predictor” as it’s called in the online version of the book.</p>
<p>So far, our “regression” did not do much to be honest. The mean of a list of values
was already calculated in the <a href="https://jdegenfellner.github.io/Script_QM1_ZHAW/descriptive_stats.html">descriptive statistics section</a>
before and we have mentioned how great this statistic is as measure of location and where its weaknesses are.</p>
<p>Now, we want to model <strong>how</strong> body height and weight are <strong>related</strong>.
Formally, one wants to <em>predict</em> body heights from body weights.</p>
<p>Here and in the frequentist framework, we will see that it is <strong>not the same</strong>
problem (and therefore results in a different statistical model)
<strong>to predict body weights from body heights or vice versa</strong>.</p>
<p>We remember the following:</p>
<ul>
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>, which is equivalent to predict <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span>.
We know X and want to predict Y.</li>
<li>Regress <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>, which is equivalent to predict <span class="math inline">\(X\)</span> from <span class="math inline">\(Y\)</span>.
We know Y and want to predict X.</li>
</ul>
<p>The word “predictor” is important here. It is a technical term
and describes a variable that we know (in our case weight) and with
which we want to “guess as good as possible” the value of the
dependent variable (in our case height). “As good as possible”
means that we put a penalty on an error. The farer our prediction
is aways from the true value (<span class="math inline">\(y_i\)</span>), the higher the penalty.
And not only that, but if you are twice as far away from the true value,
you should be penalized four times as much. This is the idea behind
the squared error <a href="https://en.wikipedia.org/wiki/Loss_function">loss function</a>
and the core of the least squares method.
What if we would punish differently, you ask?
There are many loss functions one could use
(for instance the <a href="https://en.wikipedia.org/wiki/Huber_loss">Huber loss</a>),
maybe we will see some later. For now, we punish quadratically.</p>
<p>We <strong>always</strong> visualize the data first to improve our understanding.
First comes descriptive statistics, then one can think about modeling.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="simple-linear-regression.html#cb42-1" tabindex="-1"></a><span class="fu">plot</span>(d2<span class="sc">$</span>height <span class="sc">~</span> d2<span class="sc">$</span>weight)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>It’s not often that you see such a clean plot.
The scatterplot indicates a linear relationship between the two variables.
The higher the weight, the higher the height; with some deviations of course and
we decide that normally distributed errors are a good idea.
This relationsip is neither causal, nor deterministic.</p>
<ul>
<li>It is not causal since an increase in weight does not
necessarily lead to an increase in height, especially in grown-ups.</li>
<li>It is not deterministic since there are deviations from the line.
It if was deterministic, we would not need statistical modeling.</li>
</ul>
<p>For simpler notation, we will call <code>d2$weight</code> <span class="math inline">\(x\)</span>. <span class="math inline">\(\bar{x}\)</span>
is the mean of <span class="math inline">\(x\)</span>.</p>
<div id="model-definition" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Model definition<a href="simple-linear-regression.html#model-definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s write down our <strong>model</strong> (again with the Swiss population prior mean):</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma)\\
\mu_i &amp;\sim&amp; \alpha + \beta (x_i - \bar{x})\\
\alpha &amp;\sim&amp; \text{Normal}(171.1, 20)\\
\beta &amp;\sim&amp; \text{Normal}(0, 10)\\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<p>Visualization of the <strong>model structure</strong>:</p>
<div class="grViz html-widget html-fill-item" id="htmlwidget-4a2136222874c910bd64" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-4a2136222874c910bd64">{"x":{"diagram":"\ndigraph model {\n  graph [layout = dot, rankdir = TB]\n\n  # Nodes for variables\n  node [shape = ellipse, style = filled, fillcolor = lightblue]\n  alpha [label = \"α ~ Normal(171.1, 20)\"]\n  beta [label = \"β ~ Normal(0, 10)\"]\n  sigma [label = \"σ ~ Uniform(0, 50)\"]\n  mu [label = \"μ_i = α + β(x_i - x̄)\"]\n  h_i [label = \"h[i] ~ Normal(μ_i, σ)\"]\n\n  # Connections\n  alpha -> mu\n  beta -> mu\n  mu -> h_i\n  sigma -> h_i\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>There are now additional lines for the priors of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>
(compared to the simple mean model before).
The model structure also shows the way to simulate from the prior.
One starts at the top and ends up with the heights.</p>
<ul>
<li><span class="math inline">\(h_i\)</span> is the height of the <span class="math inline">\(i\)</span>-th person and we assume it is normally distributed.</li>
<li><span class="math inline">\(\mu_i\)</span> is the mean of the height of the <span class="math inline">\(i\)</span>-th person and we
assume it is linearly dependent on the difference <span class="math inline">\(x_i-\bar{x}\)</span>.
Compared to the intercept model, a different mean is assumed for each person
depending on his/her weight.</li>
<li><span class="math inline">\(\alpha\)</span> is the intercept and we use the same prior as before.</li>
<li><span class="math inline">\(\beta\)</span> is the slope of the line and we use the normal distribution as prior for it,
hence it can be positive or negative and how plausible each value is, is
determined by that specific normal distribution. Note, that we could
easily adapt the distribtion to any distribution we like.</li>
<li>The prior for <span class="math inline">\(\sigma\)</span> is unchanged.</li>
<li><span class="math inline">\(x_i - \bar{x}\)</span> is the deviation of the weight from the mean weight, thereby <strong>we
center</strong> the weight variable. This is a common practice in regression analysis.
A value <span class="math inline">\(x_i - \bar{x} &gt; 0\)</span> implies that the person is heavier than the average.</li>
</ul>
<p>The linear model is quite popular in applied statistics and one
reason is probably the rather straightforward interpretation of the coefficients:
A one unit increase in weight is on average (in the mean) associated with
a <span class="math inline">\(\beta\)</span> unit increase/decrease (depending if <span class="math inline">\(\beta\)</span> is <span class="math inline">\(&gt;0\)</span> or <span class="math inline">\(&lt;0\)</span>) in height.</p>
</div>
<div id="priors" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Priors<a href="simple-linear-regression.html#priors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We want to plot our prior predictions to get a feeling <strong>what
the model would predict without seeing the data</strong>.
This is a kind of “sanity check” to see if the priors and
the model definition are reasonable.
Again, we just draw from the assumed distributions for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>
100 times and draw the corresponding lines. Just as the model
definition says.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="simple-linear-regression.html#cb43-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2971</span>)</span>
<span id="cb43-2"><a href="simple-linear-regression.html#cb43-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># 100 lines</span></span>
<span id="cb43-3"><a href="simple-linear-regression.html#cb43-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="fl">171.1</span>, <span class="dv">20</span>)</span>
<span id="cb43-4"><a href="simple-linear-regression.html#cb43-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb43-5"><a href="simple-linear-regression.html#cb43-5" tabindex="-1"></a></span>
<span id="cb43-6"><a href="simple-linear-regression.html#cb43-6" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb43-7"><a href="simple-linear-regression.html#cb43-7" tabindex="-1"></a></span>
<span id="cb43-8"><a href="simple-linear-regression.html#cb43-8" tabindex="-1"></a><span class="co"># start with empty plot</span></span>
<span id="cb43-9"><a href="simple-linear-regression.html#cb43-9" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>, <span class="dv">400</span>),</span>
<span id="cb43-10"><a href="simple-linear-regression.html#cb43-10" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;height&quot;</span>)</span>
<span id="cb43-11"><a href="simple-linear-regression.html#cb43-11" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># horizontal line at 0</span></span>
<span id="cb43-12"><a href="simple-linear-regression.html#cb43-12" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">272</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fl">0.5</span>)  <span class="co"># horizontal line at 272</span></span>
<span id="cb43-13"><a href="simple-linear-regression.html#cb43-13" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;b ~ dnorm(0, 10)&quot;</span>)</span>
<span id="cb43-14"><a href="simple-linear-regression.html#cb43-14" tabindex="-1"></a></span>
<span id="cb43-15"><a href="simple-linear-regression.html#cb43-15" tabindex="-1"></a><span class="co"># Overlay the 100 lines</span></span>
<span id="cb43-16"><a href="simple-linear-regression.html#cb43-16" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb43-17"><a href="simple-linear-regression.html#cb43-17" tabindex="-1"></a>  <span class="fu">curve</span>(a[i] <span class="sc">+</span> b[i] <span class="sc">*</span> (x <span class="sc">-</span> xbar),</span>
<span id="cb43-18"><a href="simple-linear-regression.html#cb43-18" tabindex="-1"></a>        <span class="at">from =</span> <span class="fu">min</span>(d2<span class="sc">$</span>weight), <span class="at">to =</span> <span class="fu">max</span>(d2<span class="sc">$</span>weight),</span>
<span id="cb43-19"><a href="simple-linear-regression.html#cb43-19" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(<span class="st">&quot;black&quot;</span>, <span class="fl">0.2</span>))</span>
<span id="cb43-20"><a href="simple-linear-regression.html#cb43-20" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>This linear relationship defined with the chosen priors seems rather
non-restrictive. According to our priors,
one could see very steeply rising or falling relationships between weight and expected heights.
We could at least make the priors for the slope (<span class="math inline">\(\beta\)</span>) non-negative.
One possibility to do this
is to use a <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal distribution</a>
for the prior of <span class="math inline">\(\beta\)</span> which can only take non-negative values.</p>
<p><span class="math display">\[ \beta \sim \text{Log-Normal}(0, 1) \]</span></p>
<p>Lets plot the priors again.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="simple-linear-regression.html#cb44-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2971</span>)</span>
<span id="cb44-2"><a href="simple-linear-regression.html#cb44-2" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># 100 lines</span></span>
<span id="cb44-3"><a href="simple-linear-regression.html#cb44-3" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="fl">171.1</span>, <span class="dv">20</span>)</span>
<span id="cb44-4"><a href="simple-linear-regression.html#cb44-4" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">rlnorm</span>(N, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb44-5"><a href="simple-linear-regression.html#cb44-5" tabindex="-1"></a></span>
<span id="cb44-6"><a href="simple-linear-regression.html#cb44-6" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb44-7"><a href="simple-linear-regression.html#cb44-7" tabindex="-1"></a></span>
<span id="cb44-8"><a href="simple-linear-regression.html#cb44-8" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlim =</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">100</span>, <span class="dv">400</span>),</span>
<span id="cb44-9"><a href="simple-linear-regression.html#cb44-9" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;weight&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;height&quot;</span>)</span>
<span id="cb44-10"><a href="simple-linear-regression.html#cb44-10" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">0</span>, <span class="at">lty =</span> <span class="dv">2</span>)  <span class="co"># horizontal line at 0</span></span>
<span id="cb44-11"><a href="simple-linear-regression.html#cb44-11" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> <span class="dv">272</span>, <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="fl">0.5</span>)  <span class="co"># horizontal line at 272</span></span>
<span id="cb44-12"><a href="simple-linear-regression.html#cb44-12" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;b ~ dlnorm(0, 1)&quot;</span>)</span>
<span id="cb44-13"><a href="simple-linear-regression.html#cb44-13" tabindex="-1"></a></span>
<span id="cb44-14"><a href="simple-linear-regression.html#cb44-14" tabindex="-1"></a><span class="co"># Overlay the 100 lines</span></span>
<span id="cb44-15"><a href="simple-linear-regression.html#cb44-15" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb44-16"><a href="simple-linear-regression.html#cb44-16" tabindex="-1"></a>  <span class="fu">curve</span>(a[i] <span class="sc">+</span> b[i] <span class="sc">*</span> (x <span class="sc">-</span> xbar),</span>
<span id="cb44-17"><a href="simple-linear-regression.html#cb44-17" tabindex="-1"></a>        <span class="at">from =</span> <span class="fu">min</span>(d2<span class="sc">$</span>weight), <span class="at">to =</span> <span class="fu">max</span>(d2<span class="sc">$</span>weight),</span>
<span id="cb44-18"><a href="simple-linear-regression.html#cb44-18" tabindex="-1"></a>        <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(<span class="st">&quot;black&quot;</span>, <span class="fl">0.2</span>))</span>
<span id="cb44-19"><a href="simple-linear-regression.html#cb44-19" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>This seems definitely more realistic. There is some sort of positive linear relationship
between weight and expected height.</p>
</div>
<div id="fit-model" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Fit model<a href="simple-linear-regression.html#fit-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s <strong>estimate the posterior/fit the model</strong> as before:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="simple-linear-regression.html#cb45-1" tabindex="-1"></a><span class="co"># load data again, since it&#39;s a long way back</span></span>
<span id="cb45-2"><a href="simple-linear-regression.html#cb45-2" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb45-3"><a href="simple-linear-regression.html#cb45-3" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb45-4"><a href="simple-linear-regression.html#cb45-4" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb45-5"><a href="simple-linear-regression.html#cb45-5" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> d[d<span class="sc">$</span>age <span class="sc">&gt;=</span> <span class="dv">18</span>, ]</span>
<span id="cb45-6"><a href="simple-linear-regression.html#cb45-6" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb45-7"><a href="simple-linear-regression.html#cb45-7" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb45-8"><a href="simple-linear-regression.html#cb45-8" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">quap</span>(</span>
<span id="cb45-9"><a href="simple-linear-regression.html#cb45-9" tabindex="-1"></a>    <span class="fu">alist</span>(</span>
<span id="cb45-10"><a href="simple-linear-regression.html#cb45-10" tabindex="-1"></a>        height <span class="sc">~</span> <span class="fu">dnorm</span>(mu, sigma),</span>
<span id="cb45-11"><a href="simple-linear-regression.html#cb45-11" tabindex="-1"></a>        mu <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> (weight <span class="sc">-</span> xbar),</span>
<span id="cb45-12"><a href="simple-linear-regression.html#cb45-12" tabindex="-1"></a>        a <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="fl">171.1</span>, <span class="dv">100</span>),</span>
<span id="cb45-13"><a href="simple-linear-regression.html#cb45-13" tabindex="-1"></a>        b <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="dv">10</span>),</span>
<span id="cb45-14"><a href="simple-linear-regression.html#cb45-14" tabindex="-1"></a>        sigma <span class="sc">~</span> <span class="fu">dunif</span>(<span class="dv">0</span>, <span class="dv">50</span>)</span>
<span id="cb45-15"><a href="simple-linear-regression.html#cb45-15" tabindex="-1"></a>    ) ,</span>
<span id="cb45-16"><a href="simple-linear-regression.html#cb45-16" tabindex="-1"></a><span class="at">data =</span> d2)</span></code></pre></div>
<p>Note that the model definition was now directly included in the <code>quap</code> function.
Let’s look at the <strong>marginal distributions</strong> of the parameters:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="simple-linear-regression.html#cb46-1" tabindex="-1"></a><span class="fu">precis</span>(mod)</span></code></pre></div>
<pre><code>##              mean         sd        5.5%       94.5%
## a     154.5972120 0.27033045 154.1651717 155.0292523
## b       0.9050131 0.04192754   0.8380048   0.9720214
## sigma   5.0718673 0.19115323   4.7663675   5.3773671</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="simple-linear-regression.html#cb48-1" tabindex="-1"></a><span class="fu">plot</span>(mod)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>Note, that the credible intervals in the plot of the coefficients are
hardly noticable. The reason is that the intercept <span class="math inline">\(a\)</span> is rather large,
whereas <span class="math inline">\(b\)</span> and sigma are comparativly small. The plot is scaled to the
largest parameter.</p>
<p>We can improve this by centering the height variable as well. Or,
we could standardize both weight and height. This would change the interpretation
of the <span class="math inline">\(\beta\)</span> parameter. It would then be the expected change in standard deviations
when changing weight by one standard deviation.
See <a href="simple-linear-regression.html#exercise12_simpl_lin_reg">exercise 12</a>.</p>
<p>The analysis yields estimates for all our parameters of the model: <span class="math inline">\(\alpha\)</span>,
<span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma\)</span>. The estimates are the mean of the posterior distribution.</p>
<p>See <a href="simple-linear-regression.html#exercise2_simpl_lin_reg">exercise 2</a>.</p>
<p><strong>Interpretation of <span class="math inline">\(\beta\)</span></strong>:
The mean of the posterior distribution of <span class="math inline">\(\beta\)</span> is 0.9. A person with a weight
of 1 kg more weight can be expected to be 0.9 cm taller. A 89% credible interval
for this estimate is <span class="math inline">\([0.83, 0.97]\)</span>. We can be quite sure that the slope is
positive (of course we designed it that way too via the prior).</p>
<p>It might also be interesting to inspect the <strong>variance-covariance matrix</strong>,
respectively the correlation between the parameters as we did before
in the intercept model. Remember, these are the correlations of
parameters in the multivariate (because three paremeters simulatenously)
posterior distribution.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="simple-linear-regression.html#cb49-1" tabindex="-1"></a><span class="fu">diag</span>(<span class="fu">vcov</span>(mod))</span></code></pre></div>
<pre><code>##           a           b       sigma 
## 0.073078550 0.001757918 0.036539558</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="simple-linear-regression.html#cb51-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cov2cor</span>(<span class="fu">vcov</span>(mod)),<span class="dv">2</span>)</span></code></pre></div>
<pre><code>##       a b sigma
## a     1 0     0
## b     0 1     0
## sigma 0 0     1</code></pre>
<ul>
<li><code>diag(vcov(mod))</code> gives the variances of the parameters and</li>
<li><code>cov2cor(vcov(mod))</code>the correlations. As we can see the correlations are (near) zero. Compare to the graphical
display of the model structure. There is no connection.</li>
</ul>
</div>
<div id="result" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Result<a href="simple-linear-regression.html#result" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Graphical end result</strong> of fitting the model:
We plot the marginal posterior distributions of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>,
and also the raw data with the found regression line.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="simple-linear-regression.html#cb53-1" tabindex="-1"></a>post <span class="ot">&lt;-</span> <span class="fu">extract.samples</span>(mod)</span>
<span id="cb53-2"><a href="simple-linear-regression.html#cb53-2" tabindex="-1"></a><span class="fu">dens</span>(post<span class="sc">$</span>a, <span class="at">col =</span> rangi2)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="simple-linear-regression.html#cb54-1" tabindex="-1"></a><span class="fu">dens</span>(post<span class="sc">$</span>b, <span class="at">col =</span> rangi2)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="simple-linear-regression.html#cb55-1" tabindex="-1"></a><span class="co"># both posteror plots seem symmetric</span></span>
<span id="cb55-2"><a href="simple-linear-regression.html#cb55-2" tabindex="-1"></a><span class="co"># we use the mean as point estimate</span></span>
<span id="cb55-3"><a href="simple-linear-regression.html#cb55-3" tabindex="-1"></a><span class="co"># for a and b.</span></span>
<span id="cb55-4"><a href="simple-linear-regression.html#cb55-4" tabindex="-1"></a>a_quap <span class="ot">&lt;-</span> <span class="fu">mean</span>(post<span class="sc">$</span>a)</span>
<span id="cb55-5"><a href="simple-linear-regression.html#cb55-5" tabindex="-1"></a>b_quap <span class="ot">&lt;-</span> <span class="fu">mean</span>(post<span class="sc">$</span>b)</span>
<span id="cb55-6"><a href="simple-linear-regression.html#cb55-6" tabindex="-1"></a></span>
<span id="cb55-7"><a href="simple-linear-regression.html#cb55-7" tabindex="-1"></a><span class="fu">plot</span>(d2<span class="sc">$</span>height <span class="sc">~</span> d2<span class="sc">$</span>weight, <span class="at">col =</span> rangi2)</span>
<span id="cb55-8"><a href="simple-linear-regression.html#cb55-8" tabindex="-1"></a><span class="fu">curve</span>(a_quap <span class="sc">+</span> b_quap <span class="sc">*</span> (x <span class="sc">-</span> xbar), <span class="at">add =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-28-3.png" width="672" /></p>
</div>
<div id="credible-bands" class="section level3 hasAnchor" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Credible bands<a href="simple-linear-regression.html#credible-bands" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We could draw again and again from the posterior distribution
and calculate the means like above. Plotting the regression lines
with the respective parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> would indicate
the variability of the estimates.
Note that we do not draw from the data (as one does in bootstrap resampling),
but from the posterior distribution.
The <code>link</code> function does this for us. It takes the posterior distribution We
have just fit, samples <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> from it, calculates the mean and then
samples from this normal distribution for the mean at a given weight.
Refer to pages 98-106 in the current version of the book Statistical Rethinking
for all details.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="simple-linear-regression.html#cb56-1" tabindex="-1"></a><span class="co"># Define a sequence of weights for predictions</span></span>
<span id="cb56-2"><a href="simple-linear-regression.html#cb56-2" tabindex="-1"></a>weight.seq <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">25</span>, <span class="at">to =</span> <span class="dv">70</span>, <span class="at">by =</span> <span class="dv">1</span>)</span>
<span id="cb56-3"><a href="simple-linear-regression.html#cb56-3" tabindex="-1"></a></span>
<span id="cb56-4"><a href="simple-linear-regression.html#cb56-4" tabindex="-1"></a><span class="co"># Use the model to compute mu for each weight</span></span>
<span id="cb56-5"><a href="simple-linear-regression.html#cb56-5" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">link</span>(mod, <span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">weight =</span> weight.seq))</span>
<span id="cb56-6"><a href="simple-linear-regression.html#cb56-6" tabindex="-1"></a><span class="fu">str</span>(mu)</span></code></pre></div>
<pre><code>##  num [1:1000, 1:46] 138 136 137 136 137 ...</code></pre>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="simple-linear-regression.html#cb58-1" tabindex="-1"></a><span class="co"># Visualize the distribution of mu values</span></span>
<span id="cb58-2"><a href="simple-linear-regression.html#cb58-2" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>)  <span class="co"># Hide raw data with type = &quot;n&quot;</span></span>
<span id="cb58-3"><a href="simple-linear-regression.html#cb58-3" tabindex="-1"></a></span>
<span id="cb58-4"><a href="simple-linear-regression.html#cb58-4" tabindex="-1"></a><span class="co"># Loop over samples and plot each mu value</span></span>
<span id="cb58-5"><a href="simple-linear-regression.html#cb58-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {</span>
<span id="cb58-6"><a href="simple-linear-regression.html#cb58-6" tabindex="-1"></a>  <span class="fu">points</span>(weight.seq, mu[i, ], <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.1</span>))</span>
<span id="cb58-7"><a href="simple-linear-regression.html#cb58-7" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>The <code>link</code> function fixes the weight at the values in <code>weight.seq</code> and
draws samples from the posterior distribution of the parameters. We will do
the analog thing in the frequentist framework.</p>
<p>We can also draw a nice shade for the regression line:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="simple-linear-regression.html#cb59-1" tabindex="-1"></a><span class="co"># Summarize the distribution of mu</span></span>
<span id="cb59-2"><a href="simple-linear-regression.html#cb59-2" tabindex="-1"></a>mu.mean <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, mean)</span>
<span id="cb59-3"><a href="simple-linear-regression.html#cb59-3" tabindex="-1"></a>mu.PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(mu, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb59-4"><a href="simple-linear-regression.html#cb59-4" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.5</span>))</span>
<span id="cb59-5"><a href="simple-linear-regression.html#cb59-5" tabindex="-1"></a><span class="fu">lines</span>(weight.seq, mu.mean)</span>
<span id="cb59-6"><a href="simple-linear-regression.html#cb59-6" tabindex="-1"></a><span class="fu">shade</span>(mu.PI, weight.seq)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>The function <code>PI</code> from the <code>rethinking</code> package calculates the 89% percentile interval
for the mean of the height at a certain weight. The <code>apply</code> function
apples this function to each columns (hence the <code>2</code>) of the matrix <code>mu</code>.</p>
<p>As we can see, we are pretty sure about the mean of height which
we wanted to model in the first place.</p>
<p><strong>Mean modeling is one thing, individual prediction is another</strong>.
Given a certain weight of a person, what is the height of the same person?
The first line in the model definition (<span class="math inline">\(height_i \sim Normal(\mu_i, \sigma)\)</span>)
tells us that a person’s weight is distributed <em>around</em> the mean
(which linearly depends on weight) and is not necessary the mean itself.</p>
<p>To get to an <strong>individual prediction</strong>, we need to consider the uncertainty
of the parameter estimation <em>and</em> the uncertainty from the Gaussian distribution
around the mean (at a certain weight). We do this with <code>sim</code>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="simple-linear-regression.html#cb60-1" tabindex="-1"></a><span class="co"># Simulate heights from the posterior</span></span>
<span id="cb60-2"><a href="simple-linear-regression.html#cb60-2" tabindex="-1"></a>sim.height <span class="ot">&lt;-</span> <span class="fu">sim</span>(mod, <span class="at">data =</span> <span class="fu">list</span>(<span class="at">weight =</span> weight.seq))</span>
<span id="cb60-3"><a href="simple-linear-regression.html#cb60-3" tabindex="-1"></a><span class="fu">str</span>(sim.height)</span></code></pre></div>
<pre><code>##  num [1:1000, 1:46] 138 130 130 147 136 ...</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="simple-linear-regression.html#cb62-1" tabindex="-1"></a><span class="co"># Compute the 89% prediction interval for simulated heights</span></span>
<span id="cb62-2"><a href="simple-linear-regression.html#cb62-2" tabindex="-1"></a>height.PI <span class="ot">&lt;-</span> <span class="fu">apply</span>(sim.height, <span class="dv">2</span>, PI, <span class="at">prob =</span> <span class="fl">0.89</span>)</span>
<span id="cb62-3"><a href="simple-linear-regression.html#cb62-3" tabindex="-1"></a></span>
<span id="cb62-4"><a href="simple-linear-regression.html#cb62-4" tabindex="-1"></a><span class="co"># Plot the raw data</span></span>
<span id="cb62-5"><a href="simple-linear-regression.html#cb62-5" tabindex="-1"></a><span class="fu">plot</span>(height <span class="sc">~</span> weight, d2, <span class="at">col =</span> <span class="fu">col.alpha</span>(rangi2, <span class="fl">0.5</span>))</span>
<span id="cb62-6"><a href="simple-linear-regression.html#cb62-6" tabindex="-1"></a></span>
<span id="cb62-7"><a href="simple-linear-regression.html#cb62-7" tabindex="-1"></a><span class="co"># Draw MAP (mean a posteriori) line</span></span>
<span id="cb62-8"><a href="simple-linear-regression.html#cb62-8" tabindex="-1"></a><span class="fu">lines</span>(weight.seq, mu.mean)</span>
<span id="cb62-9"><a href="simple-linear-regression.html#cb62-9" tabindex="-1"></a></span>
<span id="cb62-10"><a href="simple-linear-regression.html#cb62-10" tabindex="-1"></a><span class="co"># Draw HPDI (highest posterior density interval) region for mu</span></span>
<span id="cb62-11"><a href="simple-linear-regression.html#cb62-11" tabindex="-1"></a><span class="fu">shade</span>(mu.PI, weight.seq)</span>
<span id="cb62-12"><a href="simple-linear-regression.html#cb62-12" tabindex="-1"></a></span>
<span id="cb62-13"><a href="simple-linear-regression.html#cb62-13" tabindex="-1"></a><span class="co"># Draw PI (prediction interval) region for simulated heights</span></span>
<span id="cb62-14"><a href="simple-linear-regression.html#cb62-14" tabindex="-1"></a><span class="fu">shade</span>(height.PI, weight.seq)</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Here, the <code>PI</code> function is applied to the simulated heights and calculates the 89%
percentile interval for each weight.</p>
<p>The lighter and wider shaded region is where the model expects to find 89%
of the heights of a person with a certain weight.</p>
<p>This part is <strong>sometimes a bit desillusioning</strong> when seen for the first time:
Draw a horizontal line at 150 cm and see how many weights (according to
the individual prediction) are compatible with this height. Weights
from 30 to 50 kg are compatible with this height according to the
89% prediction interval:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>The higher the credibility, the wider the interval,
the wider the range of compatible weights.
In our example, more than 60% of the weight-range are
plausible to predict a height of 150 cm.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="simple-linear-regression.html#cb63-1" tabindex="-1"></a>(<span class="dv">50</span> <span class="sc">-</span> <span class="dv">30</span>) <span class="sc">/</span> (<span class="fu">range</span>(d2<span class="sc">$</span>weight)[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">range</span>(d2<span class="sc">$</span>weight)[<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.6265362</code></pre>
<p>On the other hand: We did not model the relationship this way.
We modeled <strong>height depending on weight</strong> and not the other way around.
In the next chapter, we will regress weight on height (yes, this is the correct order)
and see what changes.</p>
</div>
<div id="summary" class="section level3 hasAnchor" number="3.1.6">
<h3><span class="header-section-number">3.1.6</span> Summary<a href="simple-linear-regression.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>We have added a covariate (weight) to the simple mean model to predict height.</li>
<li>We have centered the weight variable.</li>
<li>We have defined and refined priors for the intercept and slope.</li>
<li>We have estimated the posterior distribution of the parameters using quadratic approximation with <code>quap</code>.</li>
<li>We have visualized the result.</li>
<li>We have created credible bands for mean and individual predictions.</li>
</ul>
</div>
</div>
<div id="simple-linear-regression-in-the-frequentist-framework" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Simple Linear Regression in the Frequentist Framework<a href="simple-linear-regression.html#simple-linear-regression-in-the-frequentist-framework" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will now do the same analysis in the frequentist framework while introducing
some foundational theory along the way.
I recommend reading the first couple of chapters from <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>.</p>
<div id="model-definition-1" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Model definition<a href="simple-linear-regression.html#model-definition-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our linear model is defined as:</p>
<p><span class="math display">\[ h_i = \beta_0 + \beta_1 x_i + \varepsilon_i \]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\varepsilon_i\)</span> is the error term with <span class="math inline">\(\varepsilon_i \sim N(0, \sigma^2), \forall i\)</span></li>
<li><span class="math inline">\(\beta_0\)</span> is the unknown but fixed intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the unknown but fixed slope</li>
</ul>
<div id="id__model_assumptions" class="section level4 hasAnchor" number="3.2.1.1">
<h4><span class="header-section-number">3.2.1.1</span> Model Assumptions of the Classical Regression Model (<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>, 1.7):<a href="simple-linear-regression.html#id__model_assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first and <strong>most important assumption</strong> is that the data are produced<br />
probabilistically, which is specifically stated as
<span class="math display">\[ Y|X = x \sim p(y|x)\]</span></p>
<p>What does this mean?</p>
<ul>
<li><span class="math inline">\(Y|X = x\)</span> is the random variable Y <strong>conditional</strong> on X being equal to x, i.e. the
distribution of <span class="math inline">\(Y\)</span> if we know the value of <span class="math inline">\(X\)</span> (in our example the weight in kg).
<a href="https://blogs.sas.com/content/iml/files/2015/09/GLM_normal_identity.png">This</a> is a nice image of what is meant here.</li>
<li><span class="math inline">\(p(y|x)\)</span> is the distribution of potentially observable <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>.
In our case above this was the normal distribution with mean <span class="math inline">\(\mu_i\)</span> and variance <span class="math inline">\(\sigma\)</span>.</li>
</ul>
<p>You can play with <a href="https://psychmeth.shinyapps.io/Regression-NVFehler/">this shiny app</a> to improve your understanding.
It offers the option “Bedingte Verteilung anzeigen”.</p>
<p>One always thinks about the so-called
<a href="https://en.wikipedia.org/wiki/Data_generating_process">data generating process</a>
(<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a>, 1.2).
How did the data come about? There is a process behind it and this process
is attempted to be modeled.</p>
<p><strong>Further assumptions</strong>:</p>
<ul>
<li><p><strong>Correct functional specification</strong>: The conditional mean function <span class="math inline">\(f(x) = \mathbb{E}(Y|X=x)\)</span>.
In the case of the linear model, the assumption is <span class="math inline">\(\mathbb{E}(Y|X=x) = \alpha + \beta x\)</span>.
The <strong>expectation</strong> of <span class="math inline">\(Y\)</span> (height) depends linearly on <span class="math inline">\(x\)</span> (weight).
This assumption is violated when the true relationship is not linear or the
data at least suggest that it is not linear, like <a href="https://www.alexanderdemos.org/Class5_files/figure-html/unnamed-chunk-2-1.png">here</a>.</p></li>
<li><p><strong>The errors are homoscedastic</strong> (constant variance <span class="math inline">\(\sigma^2\)</span>). This means the
variances of all conditional distributions <span class="math inline">\(p(y|x)\)</span> are constant (<span class="math inline">\(=\sigma^2\)</span>).
This assumption is (for instance) violated if points are <a href="https://www.investopedia.com/thmb/n9S9lWMv6X9-sKC2DtAOtUTPSik=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/Heteroskedasticity22-ce5acc2acef6494d91935588b0599579.png">spreading out more and more
around the regression line</a>,
indicating that the errors are getting larger.</p></li>
<li><p><strong>Normality</strong>. For the classical linear regression model all the conditional
distributions <span class="math inline">\(p(y|x)\)</span> are normal distributions. It could well be, that
the errors are not nicely normally distributed around the regression line,
for instance if we have a lot of outliers upwards and the distribution
is skewed, like <a href="https://www.bookdown.org/rwnahhas/RMPH/mlr-normality.html">here (Figure 5.20)</a>.</p></li>
<li><p>The <strong>errors are independent</strong> of each other.
The potentially observable <span class="math inline">\(\varepsilon_i = Y_i - f(\mathbf{x_i}, \mathbf{\beta})\)</span>
is uncorrelated with <span class="math inline">\(\varepsilon_j = Y_j - f(\mathbf{x_j}, \mathbf{\beta})\)</span> for
<span class="math inline">\(i \neq j\)</span>. This assumption is violated if the errors are correlated,
here is an example: The true data comes from a sine curve and we estimate a
linear model (green), which does not fit the data well (left plot).
The residuals plot shows clear patterns (right plot) and indicates
that the errors are correlated. Specifically, the errors around <span class="math inline">\(x=2\)</span> and <span class="math inline">\(x=4\)</span>
are negatively correlated (see <a href="simple-linear-regression.html#exercise6_simpl_lin_reg">exercise 6</a>).</p></li>
</ul>
<pre><code>## 
## Attaching package: &#39;patchwork&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     area</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>In the case above, the errors are <strong>not conditionally independent</strong>.
If we condition on <span class="math inline">\(X=2\)</span> and <span class="math inline">\(X=4.5\)</span>, the errors are correlated (<span class="math inline">\(r \sim -0.3\)</span>),
which they should not be.</p>
<p>These assumptions become clearer as we go along and should be checked
for every model we fit. They are not connected, they can all be true or false.
The question is not “Are the assumptions met?” since they never are exactly met.
The question is <strong>how</strong> “badly” the assumptions are violated?</p>
<p>Remember, <strong>all models are wrong, but some are useful</strong>.</p>
<p>In full, the classical linear regression model can be written as:</p>
<p><span class="math display">\[ Y_i|X_i = x_i \sim_{independent} N(\beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik},\sigma^2)\]</span>
for <span class="math inline">\(i = 1, \dots, n\)</span>.</p>
</div>
</div>
<div id="fit_model_simple_lin_reg_classic" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Fit the model<a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Again, we fit the model using the least squares method. For a neat animated explanation,
visit <a href="https://www.youtube.com/watch?v=jEEJNz0RK4Q&amp;ab_channel=COCCmath">this video</a>.
There are literally hundreds of videos on the topic. Choose wisely. Not all are good.
If in doubt, use our recommended books as reading materials. This is the most reliable source.
A hint along the way: Be very sceptical if you ask GPT about information,
although for this special case one has a good chance of getting a decent answer due to
the vast amount of training data.</p>
<p>One has to minimize the sum of squared differences between the true heights and
the model-predicted heights in order to find <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p><span class="math display">\[ SSE(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 \]</span></p>
<p>We omit the technical details (set derivative to zero and solve the system) and give the results for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>:</p>
<p><span class="math display">\[
\hat{\beta_0} = \bar{y} - (\hat{\beta_1} \bar{x}),
\]</span>
<span class="math display">\[
\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} =
\frac{s_{x,y}}{s_x^2} = r_{xy} \frac{s_y}{s_x}.
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(r_{xy}\)</span> is the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">sample correlation coefficient</a> between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(s_x\)</span> and <span class="math inline">\(s_y\)</span> are the <a href="https://en.wikipedia.org/wiki/Standard_deviation">uncorrected sample standard deviations</a> of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(s_x^2\)</span> and <span class="math inline">\(s_{xy}\)</span> are the <a href="https://en.wikipedia.org/wiki/Variance">sample variance</a> and <a href="https://en.wikipedia.org/wiki/Covariance">sample covariance</a>, respectively</li>
</ul>
<p>Interpretation of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>: see <a href="simple-linear-regression.html#exercise3_simpl_lin_reg">exercise 3</a>.</p>
<p>Let’s <strong>use R again to solve the problem</strong>:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="simple-linear-regression.html#cb67-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb67-2"><a href="simple-linear-regression.html#cb67-2" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb67-3"><a href="simple-linear-regression.html#cb67-3" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb67-4"><a href="simple-linear-regression.html#cb67-4" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> d[d<span class="sc">$</span>age <span class="sc">&gt;=</span> <span class="dv">18</span>, ]</span>
<span id="cb67-5"><a href="simple-linear-regression.html#cb67-5" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight, <span class="at">data =</span> d2)</span>
<span id="cb67-6"><a href="simple-linear-regression.html#cb67-6" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = height ~ weight, data = d2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -19.7464  -2.8835   0.0222   3.1424  14.7744 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 113.87939    1.91107   59.59   &lt;2e-16 ***
## weight        0.90503    0.04205   21.52   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.086 on 350 degrees of freedom
## Multiple R-squared:  0.5696, Adjusted R-squared:  0.5684 
## F-statistic: 463.3 on 1 and 350 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation of R-output</strong>:</p>
<ul>
<li><code>Call</code>: The model that was fitted.</li>
<li><code>Residuals</code>: <span class="math inline">\(r_i = height_i - \widehat{height}_i\)</span>.
Differences between true heights and model-predicted heights.</li>
<li><code>Coefficients</code>: Estimated for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We call them <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.
<ul>
<li><code>Estimate</code>: The (least squares) estimated value of the coefficient.</li>
<li><code>Std. Error</code>: The standard error of the estimate.</li>
<li><code>t value</code>: The value of the <span class="math inline">\(t\)</span>-statistic for the (Wald-) hypothesis test
<span class="math inline">\(H_0: \beta_i = 0\)</span>.</li>
<li><code>Pr(&gt;|t|)</code>: The <span class="math inline">\(p\)</span>-value of the hypothesis test.</li>
</ul></li>
<li><code>Residual standard error</code>: The estimate of <span class="math inline">\(\sigma\)</span>
which is also a model parameter (as in the Bayesian framework).</li>
<li><code>Multiple R-squared</code>: The proportion of the variance explained by the
model (we will explain this below).</li>
<li><code>Adjusted R-squared</code>: A corrected version of the <span class="math inline">\(R^2\)</span> which takes into account
the number of predictors in the model.</li>
<li><code>F-statistic</code>: The value of the <span class="math inline">\(F\)</span>-statistic for the hypothesis test:
<span class="math inline">\(H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0\)</span>. Note, the alternative
hypotheses to this test is that <em>any</em> of the <span class="math inline">\(\beta_i\)</span> is not zero. If that is
the case, the model explains more than the mean model with just <span class="math inline">\(\beta_0\)</span>.</li>
</ul>
<p>We could also <strong>solve the least squares problem graphically</strong>: We want to find the
values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the sum of squared differences
which can be plotted as 3D function. Since the function is a sum of squared terms,
we should expected a paraboloid form.
All we have to do is to ask R which of
the coordinates (<span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>) minimizes the sum of squared errors. The result confirmes
the results from the <code>lm</code> function. The dot in red marks the spot
(Code is in the <a href="https://github.com/jdegenfellner/Script_QM2_ZHAW">git repository</a>):</p>
<p><strong>—COMPILE CODE AT DEOPLOYMENT (takes a bit)—</strong></p>
</div>
<div id="confidence_intervals_frequentist" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Confidence Intervals of coefficients (frequentist)<a href="simple-linear-regression.html#confidence_intervals_frequentist" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can get CI’s conveniently with the <code>confint</code> function:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="simple-linear-regression.html#cb69-1" tabindex="-1"></a><span class="fu">confint</span>(mod, <span class="at">level =</span> <span class="fl">0.96</span>)</span></code></pre></div>
<pre><code>##                    2 %        98 %
## (Intercept) 109.939864 117.8189232
## weight        0.818351   0.9917072</code></pre>
<p>Remember, these are frequenetist confidence intervals.</p>
<p><strong>If one repeats the experiment
many times, the true but unknown value of the parameter will
be in the interval in 96% of the cases</strong></p>
<p>We can also use the simple <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">bootstrap</a>.
The advantage of this technique is that we can basically always use it,
no matter how compliated the estimator is. We do not need formulae.
We simply</p>
<ul>
<li>create 1000 bootstrap samples,</li>
<li>fit the model,</li>
<li>store the coefficients.</li>
<li>The 2% and 98% quantiles of the coefficients constitute the 96% bootstrap confidence interval.</li>
</ul>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="simple-linear-regression.html#cb71-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb71-2"><a href="simple-linear-regression.html#cb71-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(d2)</span>
<span id="cb71-3"><a href="simple-linear-regression.html#cb71-3" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb71-4"><a href="simple-linear-regression.html#cb71-4" tabindex="-1"></a>boot_coefs <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> B, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb71-5"><a href="simple-linear-regression.html#cb71-5" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>B) {</span>
<span id="cb71-6"><a href="simple-linear-regression.html#cb71-6" tabindex="-1"></a>  boot_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb71-7"><a href="simple-linear-regression.html#cb71-7" tabindex="-1"></a>  boot_mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight, <span class="at">data =</span> d2[boot_idx, ])</span>
<span id="cb71-8"><a href="simple-linear-regression.html#cb71-8" tabindex="-1"></a>  boot_coefs[i, ] <span class="ot">&lt;-</span> <span class="fu">coef</span>(boot_mod)</span>
<span id="cb71-9"><a href="simple-linear-regression.html#cb71-9" tabindex="-1"></a>}</span>
<span id="cb71-10"><a href="simple-linear-regression.html#cb71-10" tabindex="-1"></a><span class="co">#head(boot_coefs)</span></span>
<span id="cb71-11"><a href="simple-linear-regression.html#cb71-11" tabindex="-1"></a><span class="fu">t</span>(<span class="fu">apply</span>(boot_coefs, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.02</span>, <span class="fl">0.98</span>)))</span></code></pre></div>
<pre><code>##               2%         98%
## [1,] 110.1862455 117.4516455
## [2,]   0.8229982   0.9859997</code></pre>
<p>The CIs are quite similar to the ones from the <code>confint</code> function.</p>
<p>In the Bayesian setting, we used the centered weight variable. Let’s to this here too for
comparison and use 89% coverage probability.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="simple-linear-regression.html#cb73-1" tabindex="-1"></a>d2<span class="sc">$</span>weight_centered <span class="ot">&lt;-</span> d2<span class="sc">$</span>weight <span class="sc">-</span> <span class="fu">mean</span>(d2<span class="sc">$</span>weight)</span>
<span id="cb73-2"><a href="simple-linear-regression.html#cb73-2" tabindex="-1"></a>mod_centered <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight_centered, <span class="at">data =</span> d2)</span>
<span id="cb73-3"><a href="simple-linear-regression.html#cb73-3" tabindex="-1"></a><span class="co">#summary(mod_centered)</span></span>
<span id="cb73-4"><a href="simple-linear-regression.html#cb73-4" tabindex="-1"></a><span class="fu">confint</span>(mod_centered, <span class="at">level =</span> <span class="fl">0.89</span>)</span></code></pre></div>
<pre><code>##                      5.5 %      94.5 %
## (Intercept)     154.162715 155.0314698
## weight_centered   0.837658   0.9724002</code></pre>
<p>Compare with <code>precis</code> from the Bayesian model:</p>
<pre><code>##              mean         sd       5.5%       94.5%
## a     154.5972131 0.27033041 154.165173 155.0292533
## b       0.9050133 0.04192753   0.838005   0.9720216
## sigma   5.0718667 0.19115317   4.766367   5.3773663</code></pre>
<p>We are glad to see that both analyses align really nicely.</p>
</div>
<div id="analysis_of_variance" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> ANOVA (Analysis of Variance)<a href="simple-linear-regression.html#analysis_of_variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A non-obvious and very useful finding is that the total variability (SST)
in the data (our heights) can be
<strong>decomposed</strong> (or <a href="https://en.wiktionary.org/wiki/analysis">analysed</a>)
into two parts:</p>
<ul>
<li>The variability explained by the model (the regression line): SSR</li>
<li>The variability not explained by the model (the residuals): SSE</li>
</ul>
<p><span class="math display">\[ \text{Sum of Squares in Total} = \text{Sum of Squares from Regression} + \text{Sum of Squared Errors} \]</span></p>
<p><span class="math display">\[ SST = SSR + SSE \]</span></p>
<p><span class="math display">\[ \sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]</span></p>
<p>If you are interested in the details, check out <a href="https://en.wikipedia.org/wiki/Explained_sum_of_squares#Partitioning_in_the_general_ordinary_least_squares_model">this</a>.</p>
<p><a href="https://www.youtube.com/watch?v=NxRTs7sXKAQ&amp;ab_channel=365DataScience">This video</a>
explains the concept nicely.</p>
<p>Let’s visualize our regression result:</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>The blue dotted line is the distance from
the mean to the point (total variance), the red dotted line is the distance from
the mean to the regression line (explained variance) and the green dotted line
is the distance from the regression line to the point (unexplained variance).
We see that it adds up. I find this fact quite fascinating.
One finds additivity by considering not determinisic values, but variances.
Thank you <a href="https://en.wikipedia.org/wiki/Analysis_of_variance#:~:text=ANOVA%20was%20developed%20by%20the,to%20different%20sources%20of%20variation.">Ronald Fisher</a>.</p>
<p>See <a href="simple-linear-regression.html#exercise13_simpl_lin_reg">exercise 13</a>.</p>
</div>
<div id="r2---coefficient-of-determination" class="section level3 hasAnchor" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> <span class="math inline">\(R^2\)</span> - Coefficient of Determination<a href="simple-linear-regression.html#r2---coefficient-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><span class="math inline">\(R^2\)</span></a>
is the <strong>amount of variance explained by the model</strong>.
You can also read <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a> 8.1.</p>
<p>As you can see
<a href="simple-linear-regression.html#analysis_of_variance">above</a>, the total variance (SST) of our outcome (height)
can be decomposed into two parts: the variance explained by the model (SSR)
and the variance not explained by the model (SSE).</p>
<p>Maybe the most intuitive definition of <span class="math inline">\(R^2\)</span> is:</p>
<p><span class="math display">\[ R^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}\]</span></p>
<p>The value is between 0 and 1. The higher the value, the more variance is explained.
But be cautious. Depending on the context, a really high <span class="math inline">\(R^2\)</span> is not
necessarily a good thing. With the data we are working with,
it could easily hint towards an error. If we are near 1,
all points in the simple linear regression model are on the line.
If we are near 0, the model does not explain much of the variance
and we would see “noise with no slope” in the scatterplot (<a href="simple-linear-regression.html#exercise4_simpl_lin_reg">exercise 4</a>).
The normal <span class="math inline">\(R^2\)</span> can be found in the R output under <code>Multiple R-squared</code>.</p>
<p>If you add a lot of variables to your regression model, you can get an
<span class="math inline">\(R^2\)</span> of 1. The <span class="math inline">\(R^2\)</span> will never decrease when adding more variables.
We will verify this when we have more than 2 explanatory variables.
As a non-formal explanation for this: In the Sum of Squares Errors (SSE),
if you add more covariates (<span class="math inline">\(\beta_2, \beta_3\)</span>), you have more freedom
to choose values that minimize the number that will be squared. Simple regression
is just a special case of multiple (more than one predictor) regression with <span class="math inline">\(\beta_2=\beta_3=\dots=0\)</span>.
Hence, you will definitely not be worse off with regards to SSE when using more covariates.
A smaller SSE implies a larger SSR (sum constraint; SST remains constant) and hence a larger <span class="math inline">\(R^2\)</span>.
If you have as many explanatory variables as data points, you can get an <span class="math inline">\(R^2\)</span> of 1. This is
<a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> at its “best” (which we want to avoid).
You would just get a value for each data point by setting all other <span class="math inline">\(\beta_i\)</span> to zero and
<span class="math inline">\(\beta_i = \frac{y_i}{x_i}\)</span>.
Since we want to find the unterlying process, we want to avoid this.</p>
<p>Although not perfect, one way to mitigate the influence of “too many” variables
on <span class="math inline">\(R^2\)</span> is to use the adjusted <span class="math inline">\(R^2\)</span>, which an also be found in the R output (<code>Adjusted R-squared</code>).</p>
<div id="seperating_property" class="section level4 hasAnchor" number="3.2.5.1">
<h4><span class="header-section-number">3.2.5.1</span> Seperating property of regression due to <span class="math inline">\(R^2\)</span>:<a href="simple-linear-regression.html#seperating_property" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Peter Westfall</a>
explains (in Figure 8.1 of the book) how <span class="math inline">\(R^2\)</span> influences the separation of distributions in our simple regression model.</p>
<p>In our regression of <em>height on weight</em> (the order is correct, that’s how you say it),
the <span class="math inline">\(R^2\)</span> is <span class="math inline">\(0.5696\)</span>. The following plot shows how “well” (i.e. precise) one can predict
height if we use the 10% and 90% quantile of the weights (x_low and x_high).
In both, you see the conditional distribution of height given the weight <span class="math inline">\(X = x_{low}\)</span> or <span class="math inline">\(X = x_{high}\)</span>.
Scenario 1 is the original model, scenario 2 is the same data with added noise (in Y-direction),
which reduces <span class="math inline">\(R^2\)</span> to <span class="math inline">\(0.13\)</span>, much lower. In the right plot, the distributions have a large
overlap and it is hard to distinguish between weights when seeing the height.
With a very low <span class="math inline">\(R^2\)</span>, the height prediction does not really change and we could just as well
use the mean model.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>In the left plot, given <span class="math inline">\(X = x_{low}\)</span> gives a a rather strongly shifted normal distribution of
potentially observable heights for this weight compared to <span class="math inline">\(X=x_{high}\)</span>.
We would have a lower missclassification error when trying to distinguish heights
of very light and very heavy people in the sample just by seeing the height.
You can think of an even more extreme separation of these distributions,
which would happen, when <span class="math inline">\(R^2\)</span> is very high or the true slope is much higher.</p>
<p>See also <a href="simple-linear-regression.html#exercise5_simpl_lin_reg">exercise 5</a>.</p>
<p>An interesting way to look at <span class="math inline">\(R^2\)</span> is the following:
Given, that one person is in the 90% quantile of the weight,
the other is in the 10% quantile. What is the probability that the
height of the person in the 90% quantile is higher than the height of the other person?
We could calculate this relatively easy using theorems about additivity of normal distributions.
Since we are all about application, we simulate this:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="simple-linear-regression.html#cb77-1" tabindex="-1"></a><span class="fu">library</span>(rethinking)</span>
<span id="cb77-2"><a href="simple-linear-regression.html#cb77-2" tabindex="-1"></a><span class="fu">data</span>(Howell1)</span>
<span id="cb77-3"><a href="simple-linear-regression.html#cb77-3" tabindex="-1"></a>d <span class="ot">&lt;-</span> Howell1</span>
<span id="cb77-4"><a href="simple-linear-regression.html#cb77-4" tabindex="-1"></a>d2 <span class="ot">&lt;-</span> d[d<span class="sc">$</span>age <span class="sc">&gt;=</span> <span class="dv">18</span>, ]</span>
<span id="cb77-5"><a href="simple-linear-regression.html#cb77-5" tabindex="-1"></a><span class="co"># Simulate heights for the two quantiles</span></span>
<span id="cb77-6"><a href="simple-linear-regression.html#cb77-6" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb77-7"><a href="simple-linear-regression.html#cb77-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb77-8"><a href="simple-linear-regression.html#cb77-8" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">lm</span>(height <span class="sc">~</span> weight, <span class="at">data =</span> d2)</span>
<span id="cb77-9"><a href="simple-linear-regression.html#cb77-9" tabindex="-1"></a><span class="fu">summary</span>(mod)<span class="sc">$</span>r.squared</span></code></pre></div>
<pre><code>## [1] 0.5696444</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="simple-linear-regression.html#cb79-1" tabindex="-1"></a>x_low_high <span class="ot">&lt;-</span> <span class="fu">quantile</span>(d2<span class="sc">$</span>weight, <span class="at">probs =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.9</span>))</span>
<span id="cb79-2"><a href="simple-linear-regression.html#cb79-2" tabindex="-1"></a>x_low_high</span></code></pre></div>
<pre><code>##      10%      90% 
## 36.48581 54.01997</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="simple-linear-regression.html#cb81-1" tabindex="-1"></a>mean1 <span class="ot">&lt;-</span> <span class="fl">113.8793936</span> <span class="sc">+</span> <span class="fl">0.9050291</span> <span class="sc">*</span> x_low_high[<span class="dv">1</span>]</span>
<span id="cb81-2"><a href="simple-linear-regression.html#cb81-2" tabindex="-1"></a>mean2 <span class="ot">&lt;-</span> <span class="fl">113.8793936</span> <span class="sc">+</span> <span class="fl">0.9050291</span> <span class="sc">*</span> x_low_high[<span class="dv">2</span>]</span>
<span id="cb81-3"><a href="simple-linear-regression.html#cb81-3" tabindex="-1"></a>sd <span class="ot">&lt;-</span> <span class="fl">5.086</span>  <span class="co"># Standard deviation for both distributions</span></span>
<span id="cb81-4"><a href="simple-linear-regression.html#cb81-4" tabindex="-1"></a>simulated_heights <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb81-5"><a href="simple-linear-regression.html#cb81-5" tabindex="-1"></a>  <span class="co"># conditional normal distribution according to the model</span></span>
<span id="cb81-6"><a href="simple-linear-regression.html#cb81-6" tabindex="-1"></a>  <span class="at">low_heights =</span> <span class="fu">rnorm</span>(n_sims, <span class="at">mean =</span> mean1, <span class="at">sd =</span> sd),</span>
<span id="cb81-7"><a href="simple-linear-regression.html#cb81-7" tabindex="-1"></a>  <span class="at">high_heights =</span> <span class="fu">rnorm</span>(n_sims, <span class="at">mean =</span>  mean2, <span class="at">sd =</span> sd)</span>
<span id="cb81-8"><a href="simple-linear-regression.html#cb81-8" tabindex="-1"></a>)</span>
<span id="cb81-9"><a href="simple-linear-regression.html#cb81-9" tabindex="-1"></a></span>
<span id="cb81-10"><a href="simple-linear-regression.html#cb81-10" tabindex="-1"></a><span class="co"># Calculate the probability that the height of the person in the 90% quantile is higher</span></span>
<span id="cb81-11"><a href="simple-linear-regression.html#cb81-11" tabindex="-1"></a><span class="co"># than the height of the person in the 10% quantile</span></span>
<span id="cb81-12"><a href="simple-linear-regression.html#cb81-12" tabindex="-1"></a>simulated_heights <span class="sc">%&gt;%</span></span>
<span id="cb81-13"><a href="simple-linear-regression.html#cb81-13" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">higher_height =</span> high_heights <span class="sc">&gt;</span> low_heights) <span class="sc">%&gt;%</span></span>
<span id="cb81-14"><a href="simple-linear-regression.html#cb81-14" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="fu">mean</span>(higher_height))</span></code></pre></div>
<pre><code>## # A tibble: 1 × 1
##   `mean(higher_height)`
##                   &lt;dbl&gt;
## 1                 0.987</code></pre>
<p>In other words, we can be almost sure, that a person in the 90% quantile of the weight
is taller than a person in the 10% quantile of the weight given this data set.
See also <a href="#exercise11_simpl_lin_r">exercise 11</a>.</p>
</div>
</div>
<div id="check-regression-assumptions" class="section level3 hasAnchor" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Check regression assumptions<a href="simple-linear-regression.html#check-regression-assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Everytime we fit a model, we should check the assumptions <a href="simple-linear-regression.html#id__model_assumptions">above</a>.
We do this for different reasons (which will become clearer over the course).
The assumptions are independent of each other.
They can all be true or false or some can be true and some false
(<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Wesftall</a>, p.21).
Chapter 4 in the book is dedicated to this topic. It is important to know
that the assumptions are usually not met exactly. The question is <em>how badly</em> they are violated,
not <em>if</em> they are violated.
Furthermore, the asssumptions refer to the data generating process, not the data itself.
Thus, the evaluation of the assumptions should involve subject matter knowledge.</p>
<p><span class="math inline">\(p\)</span>-values to evaluate model assumptions are not a good idea. To <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#d1e949">quote</a> the
American Statistical Association (ASA): “By itself, a <span class="math inline">\(p\)</span>-value does not provide a good measure
of evidence regarding a model or hypothesis.”
Deicision-tree thinking might not be the best idea for statistical modeling.</p>
<p>We will not use hypothesis tests for assumptions, because</p>
<ul>
<li>They are never met exactly. You cannot “prove” them.</li>
<li>With small sample sizes, the statistical <em>power</em> is often low.</li>
<li>With large sample sizes, the smallest deviation will be “significant”.</li>
</ul>
<p>We will follow the book (chapter 4, pages 99ff) and use graphical and simulation methods to check the assumptions.
As guidance, we could check them in the following order:</p>
<ul>
<li>Linearity</li>
<li>Constant variance</li>
<li>Independence</li>
<li>Normality</li>
</ul>
<div id="linearity" class="section level4 hasAnchor" number="3.2.6.1">
<h4><span class="header-section-number">3.2.6.1</span> Linearity<a href="simple-linear-regression.html#linearity" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First, we <strong>plot the data</strong>, as we did already above. We can add a
a smoothing line to the raw data as well:</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>It looks like this relationship is describable in a linear way.
No apparent curvature or patches.
A refined version of the scatterplot of the raw data is The
<strong>residual scatter plot</strong> (<span class="math inline">\(x_i, e_i\)</span>):</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>Compared to the scatterplot above, the residuals plot magnifies
possible curvature. The reason is that the range of residuals
is smaller than the range of the heights.</p>
<p>In multiple regression, we will use the (<span class="math inline">\(\hat{y_i}, e_i\)</span>) plot, which is
identical to the plot above in simple linear regression, but
very helpful in multiple regression.
You get the (<span class="math inline">\(\hat{y_i}, e_i\)</span>) plot in R with <code>plot(mod, which = 1)</code>.</p>
</div>
<div id="constant_variance" class="section level4 hasAnchor" number="3.2.6.2">
<h4><span class="header-section-number">3.2.6.2</span> Constant variance<a href="simple-linear-regression.html#constant_variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This assumption means that the variance of the residuals is constant.
If it is violated, the spread around a hypothetical regression line is not constant.
We look at the residual scatter plot again:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Look for a changes in patterns of vertical variability. Note, that you
often do not have as many data points near the end of the range of the predictor.
Here are some examples of heteroskedasticity:
<a href="https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2017/08/residuals_unfixed.png?resize=576%2C384">1</a>,
<a href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQPAPMGV5U0Pe0nKnhoZWHJ8KypqVCls6ZQZud84C3KM8SJqOkuMNeJl2oTg-UDuukJRhk&amp;usqp=CAU">2</a>,
<a href="https://i.sstatic.net/R5KIH.png">3</a>.
The above looks homoscedastic. If it was heteroskedastic, this is not a problem,
we just have to model it differently (later).</p>
<p>As always, it is a good idea to study the variability of these plots
using simulation (<a href="simple-linear-regression.html#exercise7_simpl_lin_reg">exercise 7</a>).</p>
<p>Better for detecting heteroscedasticity is the (<span class="math inline">\(\hat{y_i}, |e_i|\)</span>) plot
with a smoothing line:</p>
<pre><code>## &#39;data.frame&#39;:    544 obs. of  4 variables:
##  $ height: num  152 140 137 157 145 ...
##  $ weight: num  47.8 36.5 31.9 53 41.3 ...
##  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...
##  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>This will probably not be a perfectly horizontal line, since
there are less points at the end of the range of the predictor.
For instance, There are less people with extreme weights in the data set.</p>
</div>
<div id="independence-uncorrelated-errors" class="section level4 hasAnchor" number="3.2.6.3">
<h4><span class="header-section-number">3.2.6.3</span> Independence, uncorrelated errors<a href="simple-linear-regression.html#independence-uncorrelated-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We had an example above, where the errors were correlated.
The sine curve could stem from time series data, where the <span class="math inline">\(x\)</span>-variable
is the time and the <span class="math inline">\(y\)</span>-variable is a seasonally changing variable
like temperature (purely hypothetical). In <a href="simple-linear-regression.html#exercise6_simpl_lin_reg">exercise 6</a>,
the values are autocorrelated: Correlated with previous values of the same variable.
If we would track the body heights of persons over time, we would have
an autocorrelated time series since the height of a person at time <span class="math inline">\(t\)</span> is
correlated with the height of the same person at time <span class="math inline">\(t-1\)</span>, but a little
less with the height at time <span class="math inline">\(t-2\)</span> and so on. If I am tall today, it is
very likely that I was tall yesterday.</p>
<p>In our case of the !Kung San data, we do not have autocorrelated data.</p>
<p>But still, we could look at the correlations the residuals with lagged residuals.
A <span class="math inline">\(lag=1\)</span> means I compare the residuals with the ones right next to me and check if
they are correlated.</p>
<pre><code>## cor= 0.01858044</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>It least with a lag of <span class="math inline">\(1\)</span>, there so no large correlation between the residuals.</p>
</div>
<div id="normality_assumption" class="section level4 hasAnchor" number="3.2.6.4">
<h4><span class="header-section-number">3.2.6.4</span> Normality<a href="simple-linear-regression.html#normality_assumption" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This assumptions states that the conditional distribution <span class="math inline">\(Y|X=x\)</span>
is a normal distribution. We do not look at the normality of <span class="math inline">\(Y\)</span> itself,
since it is not a formal requirement, that <span class="math inline">\(Y\)</span> is normally distributed.
We need to assess the normality of the residuals <span class="math display">\[e_i = y_i - \hat{y}_i\]</span>
I like doing this with a <a href="https://jdegenfellner.github.io/Script_QM1_ZHAW/descriptive_stats.html#q-q-plots">Q-Q plot</a>
from the R package <code>car</code> (command <code>qqPlot</code>):</p>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     some</code></pre>
<pre><code>## The following object is masked from &#39;package:rethinking&#39;:
## 
##     logit</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<pre><code>## 479  94 
## 317  72</code></pre>
<p>With the 97% confidence envelope, we can see if the residuals are
consistent with coming from a normal distribution.
We could again use simulation to see how the Q-Q Plot changes
to get a better feeling (see <a href="simple-linear-regression.html#exercise9_simpl_lin_reg">exercise 9</a>).</p>
<p>Another convenient way to check model assumptions (for a wide class of models)
is to use the <code>check_model</code> function from the <code>performance</code> package:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>In this case, everything looks fine. Let’s go through the plots and explain them one by one:</p>
<div id="posterior-predictive-check-upper-left" class="section level5 hasAnchor" number="3.2.6.4.1">
<h5><span class="header-section-number">3.2.6.4.1</span> Posterior predictive check (upper left)<a href="simple-linear-regression.html#posterior-predictive-check-upper-left" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Here, you create new data from the estimated model. We did that already in
the Bayesian framework using <code>sim</code> from the <code>rethinking</code> package.
You could also try the command <code>extract.samples()</code>from the <code>rethinking</code> package
(Statistical Rethinking p. 196) to create
new data from the Frequentist model. You can try this in our exercises sessions.
Using least squares, the estimated model was:
<span class="math display">\[ height_i = 113.87939 + 0.90503 \cdot weight_i + Normal(0, 5.086) \]</span>
From this model, we can simulate new data (or the original weights, if we want to use fixed X)
by plugging in different weights, and adding a random error from a normal distribution
with mean 0 and standard deviation 5.086.
Then, we can compare the distribution of the simulated data with the observed data.
The blue lines in the graph are model predicted heights, the green line are the
observed heights.</p>
<p>Let’s try to replicate this plot.
First, we want to simulate new data from the model.
A scatter plot is also a nice way to check of the model predictions
are in line with the observed data. One could repeat this process
multiple times to get a feeling for the variability of the model predictions.</p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p>The simulated and original data (green) fit nicely together.
This lends some credibility to the model - at least in terms of prediction.
And now the density plots of the model created data (blue) and the observed data (green):</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<p>As you can see, the densities of the observed and simulated data are broadly similar.
One could argue that heights in the range of 150-160 cm are a bit overestimated by the model.
Depending on the context, this might be a problem or not. Let’s accept the model for now.</p>
</div>
<div id="linear-fit-upper-right" class="section level5 hasAnchor" number="3.2.6.4.2">
<h5><span class="header-section-number">3.2.6.4.2</span> Linear fit (upper right)<a href="simple-linear-regression.html#linear-fit-upper-right" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>This is the same plot as <a href="simple-linear-regression.html#linearity">above</a>: <span class="math inline">\((\hat{y}_i, e_i)\)</span>.</p>
</div>
<div id="homogeneity-of-variance-middle-left" class="section level5 hasAnchor" number="3.2.6.4.3">
<h5><span class="header-section-number">3.2.6.4.3</span> Homogeneity of variance (middle left)<a href="simple-linear-regression.html#homogeneity-of-variance-middle-left" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>This is the same plot as <a href="simple-linear-regression.html#constant_variance">above</a>: <span class="math inline">\((\hat{y}_i, |e_i|)\)</span>.</p>
</div>
<div id="influential-observations-middle-right" class="section level5 hasAnchor" number="3.2.6.4.4">
<h5><span class="header-section-number">3.2.6.4.4</span> Influential observations (middle right)<a href="simple-linear-regression.html#influential-observations-middle-right" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The standard method to check for influential observations is to compute the
so-called <a href="https://en.wikipedia.org/wiki/Cook%27s_distance">Cook’s distance</a>.
This is a measure of how much leverage a single observation has on the model.
We can extract the Cook’s distance from the model using <code>cooks.distance()</code>.
An ugly rule of thumb would be to look at observations with a Cook’s distance
greater than 1.
In the plot, the leverage <span class="math inline">\(h_{ii}\)</span> is on the x-axis, and the standardized residuals
are on the y-axis. In short, a high leverage means that the estimated value <span class="math inline">\(\hat{y}_i\)</span>
is potentially far away from the original value <span class="math inline">\(y_i\)</span>.
The contours (dotted green lines) are the Cook’s distance, in this
case at a Cook’s distance of 0.5.
There is <a href="https://en.wikipedia.org/wiki/Cook%27s_distance#Relationship_to_other_influence_measures_(and_interpretation)">formula</a>
that relates the leverage (<span class="math inline">\(h_{ii}\)</span>), the Cook’s distance
and the residuals. Holding Cook`s distant constant at 0.5,
gives you the green dotted line in the plot.</p>
<p>Let’s create an outlier and see what happens:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-53-2.png" width="672" /></p>
<pre><code>## Cook&#39;s distance =  7.025207</code></pre>
<p>One single outlier changes the diagnostic plots. Observation 352
is clearly identified as influential. The one point
changes all diagnostic plots notably.
The estimates of the regression coefficients
are also affected:</p>
<pre><code>## [1] &quot;Original Model&quot;</code></pre>
<pre><code>## (Intercept)      weight 
## 113.8793936   0.9050291</code></pre>
<pre><code>## [1] &quot;Model with outlier&quot;</code></pre>
<pre><code>## (Intercept)      weight 
##  127.172494    0.599054</code></pre>
<p>Admitted, this is a somewhat artificial example.</p>
</div>
<div id="normality-of-residuals-lower-left" class="section level5 hasAnchor" number="3.2.6.4.5">
<h5><span class="header-section-number">3.2.6.4.5</span> Normality of residuals (lower left)<a href="simple-linear-regression.html#normality-of-residuals-lower-left" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>This is basically the same plot as above, just <a href="https://cran.r-project.org/web/packages/qqplotr/vignettes/introduction.html">detrended</a>.
Let’s try to replicate it:</p>
<pre><code>## 
## Attaching package: &#39;qqplotr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     stat_qq_line, StatQqLine</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>The normality assumption seems to hold. Note that in the above QQ plot,
we detrended the residuals and used a bootrap confidence band. Depending
on the band type, the confidence band can be wider or narrower and include
or exclude points.</p>
<p><strong>After having checked the assumptions for the classical regression model</strong>
and we feel comfortable with the model, we get exact confidence intervals
for the effect sizes (<span class="math inline">\(\beta\)</span>s) using <code>confint()</code> (p. 74 in Westfall).</p>
<p>Heureka! We have a model that fits the data well.</p>
</div>
</div>
</div>
<div id="bootstrap-fit" class="section level3 hasAnchor" number="3.2.7">
<h3><span class="header-section-number">3.2.7</span> Bootstrap fit<a href="simple-linear-regression.html#bootstrap-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to get a feeling for the variability of the model with regards to the predictors as well,
we can bootstrap the whole data set, fit the model and draw regression lines.
We create <span class="math inline">\(100\)</span> bootrap replicates of our data set <code>d2</code> by drawing with
replication. For every bootstrap replicate, we fit the model and draw the regression line.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<p>The results is very stable. Neither intercept nor slope change much.</p>
</div>
<div id="regression-towards-the-mean" class="section level3 hasAnchor" number="3.2.8">
<h3><span class="header-section-number">3.2.8</span> Regression towards the mean<a href="simple-linear-regression.html#regression-towards-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are great explanations for “<a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">regression towards the mean</a>” in
<a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman</a> p.58 and
<a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a> p.36.
This <a href="https://www.youtube.com/watch?v=1tSqSMOyNFE&amp;ab_channel=Veritasium">video</a> might be interesting to watch.</p>
<p>It describes the phenomenon that the predicted value (y) is closer to its
mean than the predictor (x) to its mean.
In our case this means, if the weight of a person is 1 standard deviation above the mean of body weights,
the (model-)predicted height is less than 1 standard deviation above the mean of body heights, but
still larger than the average height. Let’s verify:</p>
<pre><code>## predicted height= 0.7547479</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<p>As Gelman points out in his book (Figure 4.2), there is a fine detail:
The regression line is not the line most people would draw through the data.
They would draw a line through the main directions of variability
(directions are the eigenvectors of the covariance matrix).
In dotted green, you can see these directions. The regression line is a solution
to another problem (as we have seen): It minimizes the sum of squared residuals,
which are defined via the <strong>vertical</strong> distances to the line.
See <a href="simple-linear-regression.html#exercise8_simpl_lin_reg">exercise 8</a>.</p>
</div>
<div id="random-x-vs.-fixed-x" class="section level3 hasAnchor" number="3.2.9">
<h3><span class="header-section-number">3.2.9</span> Random X vs. fixed X<a href="simple-linear-regression.html#random-x-vs.-fixed-x" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A detail that is not mentioned often in introductory statistics courses
is the question of whether the predictor variable <span class="math inline">\(X\)</span> is random or fixed.
In our case, we did not specify the weights of the people in the !Kung San data set.
<em>Obervational</em> data was collected and we have no control over the weights.
In an <em>experimental</em> setting, we could have controlled the weights of the people.
We could have for instance only included people with weights at certain steps (50 kg, 60 kg, 70 kg).
In the latter case, we could consider X as fixed. In the former case, we consider X as random.
If we would draw another sample from the population, we would get different weights again.
Further reading: <a href="https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4">Westfall</a> 1.5.</p>
<p>There are many more details we could look into, but we wanted to give a first
practical introduction to regression analysis with less emphasis on theory and proofs behind it.</p>
<p>We want to make clear what the confidence intervals from <code>confint</code> of a model mean. For this see
<a href="simple-linear-regression.html#exercise14_simpl_lin_reg">exercise 14</a>.</p>
</div>
</div>
<div id="exercises-1" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Exercises<a href="simple-linear-regression.html#exercises-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>[E] Easy, [M] Medium, [H] Hard</p>
<p>(Some) solutions to exercises can be found in the git-repo <a href="https://github.com/jdegenfellner/Script_QM2_ZHAW/tree/main/Solutions_Exercises">here</a>.</p>
<div id="exercise1_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> [E] Exercise 1<a href="simple-linear-regression.html#exercise1_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the model from above:</p>
<p><span class="math display">\[\begin{eqnarray*}
h_i &amp;\sim&amp; \text{Normal}(\mu_i, \sigma)\\
\mu_i &amp;\sim&amp; \alpha + \beta (x_i - \bar{x})\\
\alpha &amp;\sim&amp; \text{Normal}(171.1, 20)\\
\beta &amp;\sim&amp; \text{Normal}(0, 10)\\
\sigma &amp;\sim&amp; \text{Uniform}(0, 50)
\end{eqnarray*}\]</span></p>
<ul>
<li>What ist the expected height when <span class="math inline">\(x_i = \bar{x}\)</span>?</li>
<li>What is the expected height when <span class="math inline">\(x_i\)</span> changes by 1 unit?</li>
</ul>
</div>
<div id="exercise2_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> [E] Exercise 2<a href="simple-linear-regression.html#exercise2_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Look at the marginal distrubutions of the parameters in the Bayesian model.</p>
<ul>
<li>Plot the posterior distribution of all 3 parameters.</li>
<li>Include in the plot a 99% credible interval (HDI).</li>
</ul>
</div>
<div id="exercise3_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> [M] Exercise 3<a href="simple-linear-regression.html#exercise3_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go to the coefficient estimates in the simple linear regression setting
above (<a href="simple-linear-regression.html#fit_model_simple_lin_reg_classic">Fit the model</a>)
in the classical framework.</p>
<ul>
<li>Create an R file to simulate the simple linear regression model.</li>
<li>Change your input parameters and see how the estimates change.</li>
<li>Does this make sense with respect to the estimates given, specifically with respect
to <span class="math inline">\(\beta_1\)</span>?</li>
</ul>
</div>
<div id="exercise4_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> [M] Exercise 4<a href="simple-linear-regression.html#exercise4_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verify the statement above in the text for high and low values of <span class="math inline">\(R^2\)</span>.</p>
</div>
<div id="exercise5_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> [M] Exercise 5<a href="simple-linear-regression.html#exercise5_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Verify with simulation in R that the separation of the distributions in the simple linear regression model
improves if the true (but usually unknown) slope increases.</p>
</div>
<div id="exercise6_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.6">
<h3><span class="header-section-number">3.3.6</span> [H] Exercise 6<a href="simple-linear-regression.html#exercise6_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go to the model assumptions in the classical regression model (<a href="simple-linear-regression.html#id__model_assumptions">Model Assumptions</a>).
- Use the code from <a href="https://github.com/jdegenfellner/Script_QM2_ZHAW/blob/main/02-Simple_Linear_Regression.Rmd">github</a>
to recreate the regression model with the sine-curve.
- Check the independence assumption as described. Look at the residuals, when <span class="math inline">\(X=2\)</span> and <span class="math inline">\(X=4.5\)</span>.
You can get those by filtering residuals that are <span class="math inline">\(&gt;0.5\)</span> and <span class="math inline">\(&lt;0.5\)</span>.</p>
</div>
<div id="exercise7_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.7">
<h3><span class="header-section-number">3.3.7</span> [M] Exercise 7<a href="simple-linear-regression.html#exercise7_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Simulate data from the regression of heights on weights in our !Kung San data set.</li>
<li>Draw the <span class="math inline">\(\hat{y_i}, e_i\)</span> plot.</li>
<li>Draw the <span class="math inline">\(\hat{y_i}, |e_i|\)</span> plot.</li>
<li>Repeat the simulation and look at the variability of the plot.</li>
</ul>
</div>
<div id="exercise8_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.8">
<h3><span class="header-section-number">3.3.8</span> [E] Exercise 8<a href="simple-linear-regression.html#exercise8_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Go to p.36 in Westfall’s book and read Appendix A.</li>
<li>Pay close attention to the explanation about regression toward the mean.</li>
</ul>
</div>
<div id="exercise9_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.9">
<h3><span class="header-section-number">3.3.9</span> [M] Exercise 9<a href="simple-linear-regression.html#exercise9_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go to the resuials <a href="simple-linear-regression.html#normality_assumption">above</a> where we tested the normality assumption.</p>
<ul>
<li>Calcuate mean and standard deviation from the residuals of the model
that regresses height on weight.</li>
<li>Simulate from a normal distribution using these parameters.</li>
<li>Get a feeling how the QQ plot changes by drawing the QQ plot repeatedly.</li>
</ul>
</div>
<div id="exercise10_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.10">
<h3><span class="header-section-number">3.3.10</span> [M] Exercise 10<a href="simple-linear-regression.html#exercise10_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using our !Kung San data,</p>
<ul>
<li>show that the regression of height on weight (<code>lm(height ~ weight)</code>)
is not the same as the regression of weight on height (<code>lm(weight ~ height)</code>).</li>
<li>Draw both regression lines in one diagram.</li>
<li>Can you simulate data where the two regressions deliver (almost) identical results?</li>
<li>Explain why the results differ and what consequences this would have for a research question.
Which question do I answer with each?</li>
</ul>
</div>
<div id="exercise11_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.11">
<h3><span class="header-section-number">3.3.11</span> [E] Exercise 11<a href="simple-linear-regression.html#exercise11_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the <a href="simple-linear-regression.html#seperating_property">section</a> about <span class="math inline">\(R^2\)</span> and the separation of the distributions.
How would the probability that a person in the 90% quantile of the weight
is taller than a person in the 10% quantile of the weight change if you change</p>
<ul>
<li>the true slope of the regression line</li>
<li>the true <span class="math inline">\(\sigma\)</span>, i.e. if you add more noise and have a lower <span class="math inline">\(R^2\)</span>?</li>
</ul>
</div>
<div id="exercise12_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.12">
<h3><span class="header-section-number">3.3.12</span> [M] Exercise 12<a href="simple-linear-regression.html#exercise12_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the Bayesian simple regression model for height <a href="simple-linear-regression.html#simple_lin_reg_bayes">above</a>.</p>
<ul>
<li>Standardize weights and heights in the data set <code>d2</code>.</li>
<li>Estimate the regression model as we did in the section using <code>quap</code>.</li>
<li>Plot the posterior distribution using <code>plot(model)</code>.</li>
<li>Interpret the regression coefficient <span class="math inline">\(\beta\)</span>.</li>
</ul>
</div>
<div id="exercise13_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.13">
<h3><span class="header-section-number">3.3.13</span> [M] Exercise 13<a href="simple-linear-regression.html#exercise13_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Go back to the <a href="simple-linear-regression.html#analysis_of_variance">ANOVA section</a>.</p>
<ul>
<li><p>Calculate SSE, SST and SSR for the regression of height on weight.</p></li>
<li><p>How many degrees of freedom does each term have?</p></li>
</ul>
</div>
<div id="exercise14_simpl_lin_reg" class="section level3 hasAnchor" number="3.3.14">
<h3><span class="header-section-number">3.3.14</span> [M] Exercise 14<a href="simple-linear-regression.html#exercise14_simpl_lin_reg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When we fit a simple linear regression model (Frequentist), we get confidence intervals for the coefficients
using the command <code>confint</code>. As stated previously, these are frequentist CIs. That means,
if we draw data from the true (but usually unknown) data generating process, we would expect
that the true value of the coefficient (intercept and slope) lies in the CI in X% of the cases.</p>
<ul>
<li>Simulate data from the true data generating process of the simple linear regression model.</li>
<li>Fit the model and get the confidence intervals for the coefficients.</li>
<li>Repeat this process 1000 times and see how many times the true value of the coefficients
lies in the CI.</li>
</ul>
<p>Changing the distribution of the covariate <span class="math inline">\(x\)</span> should not change the results, since
we only get a different amount of data points agross the range of <span class="math inline">\(x\)</span>.</p>
<ul>
<li>Verify this statement by changing the distribution of <span class="math inline">\(x\)</span>.</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="multiple-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/02-Simple_Linear_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
