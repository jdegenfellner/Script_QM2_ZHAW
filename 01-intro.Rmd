# Introduction {#intro}

## What is statistical modeling and what do we need this for?

Typically, one simplifies the complex reality (and loses information) in order to make it better
understandable, mathematically treatable and to make predictions. 

Underlying our models, there are theories which should be [falsifiable](https://en.wikipedia.org/wiki/Falsifiability)
and testable.
For instance, I would be really surprised if I pull up my multimeter and measure the voltage (V) and
electric current (I) at a resistence (R) in a circuit and find that [Ohm's law](https://en.wikipedia.org/wiki/Ohm%27s_law) $V = IR$ is not true. 
This [**law**](https://en.wikipedia.org/wiki/Scientific_law)
can be tested over and over again and if one would find a single valid counterexample, 
the law would be falsified. It is also true that the law is probably not 100% accularate,
but an extremely good approximation of reality. Real-world measurements carry 
measurement errors and when plotting the data, one would see that the data points
might not lie exactly on a straight line. This is not a problem.

A [statistical model](https://en.wikipedia.org/wiki/Statistical_model) 
is a mathematical framework that represents the 
relationships between variables, helping us understand, infer, and
predict patterns in data. It acts as a bridge between observed data 
and the real-world processes that generated them. In health research, 
where variability and uncertainty are inherent, statistical models are 
valuable tools for making sense of complex phenomena. 
You can watch [this](https://www.youtube.com/watch?v=3d5ivs_8amQ&ab_channel=VeryNormal) as short intro.

Depending on the task at hand, we would use different models.
In any case, logical reasoning and critical thinking comes first, 
then comes the model. **It makes no sense to estimate statistical models just for the sake of it**.

**All models are wrong, but some are useful**. 
Or to quote [George Box](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949):

> "Since all models are wrong the scientist cannot obtain
> a 'correct' one by excessive elaboration. On the contrary
> following William of Occam he should seek an economical
> description of natural phenomena. Just as the ability to
> devise simple but evocative models is the signature of the
> great scientist so overelaboration and overparameterization is often the mark of mediocrity."

In my opinion, statistical modeling is an art form: difficult and beautiful.

**One goal of this course** is to improve interpretation and limitations of statistical models.
They are not magical turning data into truth. Firstly, the rule gargabe in, garbage out (GABA) applies.
Secondly, statistical models are based on data and their variability and have inherent limitations
one cannot overcome even with the most sophisticated models. This is expressed for instance 
in the so-called [bias-variance trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
You can't have it all.

### Explanatory vs. Predictive Models

I can recommend reading [this](https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full) 
article by Shmueli et al. (2010) on this topic.

Statistical models serve different purposes depending on the research question. Two primary goals are **explanation** 
and **prediction**, and each requires a different approach:

**Explanatory Models** focus on understanding causal relationships. 
These models aim to uncover mechanisms and answer **"why"** 
questions. For example:
   
   - Does smoking increase the risk of lung cancer? **Yes**. (If you want to see what a large effect-size looks like, check out [this study](https://bmjopen.bmj.com/content/bmjopen/8/10/e021611.full.pdf).)
   - How large is the "effect" of smoking on lung cancer? **Large**.
   - Does pain education and graded sensorimotor relearning improve disability (a question we ask in 
   our [Resolve Swiss project](https://data.snf.ch/grants/grant/220585))?

Explanatory models are **theory-driven**, designed to test hypotheses. Here, one wants to understand the underlying
mechanisms and the relationships between variables and hence often uses (parsimonious) models that are more interpretable, 
like linear regression.

**Predictive Models** prioritize forecasting future outcomes based on patterns in the data. 
These models aim to answer **"what will happen?"** For instance:
   
   - [Gait analysis](https://www.tandfonline.com/doi/abs/10.1080/03091902.2020.1822940) using Machine Learning (ML)?
   - [Skin cancer detection](https://jamanetwork.com/journals/jamadermatology/fullarticle/2756346) using neural networks?

Predictive models are **data-driven**, often using complex algorithms to achieve high accuracy. 
Their success is measured using metrics like [Root Means Square Error](https://computersciencewiki.org/index.php/Root-mean-square_error_(RMSE)) 
(RMSE), [Area Unter the Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#:~:text=The%20area%20under%20the%20curve,ranks%20higher%20than%20'negative') 
(AUC), or **prediction error on new, unseen data**.
Any amount of model complexity is allowed. One could for instance estimate a 
[neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) ("just" another statistical model) 
with many hidden layers and neurons in order to improve prediction quality. Interpretability of the model weights is not a priority here.

While explanatory and predictive goals often complement each other, 
their differences highlight the importance of clearly defining the purpose 
of your analysis. In applied health research, explanatory models help identify 
causal mechanisms, while predictive models can guide real-world decisions by 
providing actionable forecasts. Together, they enhance both our understanding 
of phenomena and our ability to make informed decisions in complex environments.

### Individual vs. Population Prediction

Another important distinction is between **individual vs. population** prediction.
In the smoking example above, we can be very sure about the mean effects that smoking has on lung cancer.
On an individual level, it is [harder to predict the outcome](https://www.liebertpub.com/doi/10.1089/rej.2019.2298). 
Nevertheless, individual predictions will be (notably) better than random guessing. We will discuss this in greater detail.

### Practical Use of Statistical Models

In my optinion, we should never be afraid to test our statistical models (as honestly as possible) against reality.
We could for instance ask ourselves: 

- "How much better does this model classify than the arithmetic mean? 
  (i.e., the linear model with just an intercept)"

- "How much better does this model classify than random guessing?"

- Is it worth the effort to collect data and estimate this model by using hundreds of hours of our time?

In some cases, these questions can be answered straightforwardly. 

- In advertising (Google, Facebook, ...), a couple of percentage points in prediction quality might make a difference of millions 
of dollars in revenue offsetting the statistitians salary. 

- Improved forecasts of a few percentage points in the stock market or just being [slightly better](https://en.wikipedia.org/wiki/Jim_Simons) 
than the average, will make you faboulously rich.

- Improved cancer forecasting might save lives, money and pain and is not only measured in money.

### Start at the beginning

What do we actually want to do in general? Very broadly speaking we want to:
**describe** the association of variables to each other that carry variability. 
Hence, the relationship is not deterministic like $$y = 2x + 3$$ but rather we need
to "loosen up" the relationship to account for variability (in $x$ and $y$).
So, the values $2$ and $3$ are not fixed but aflicted with uncertainty.
Depending on your philosophical view, you might say you want to find
the "true" but unknown relationship between variables.
This is what we do in simulation studies all the time: We know the true relationship, 
simulate data by adding variability and then try to estimate the true relationship we assumed in the first place.
For some practical applications, we can get a really nice and complete answer to our question
(for instance sample size for proportions).

So we are looking for a function $f$ such that

$$ Y = f(X) $$

where 

- $Y$ is the "outcome", "dependent variable" or "response". 
- $X$ are the "predictors". $X$ can be a single Variable $x$ or many 
variables $x_1, x_2, \ldots, x_p$.

It is important to be aware of the notation here:
"Predict" does **not necessarily** mean that we can predict the value in 
the future. It merely means we estimate the value (or mean) of $Y$ given $X$.

- This can be done at the same time points, known as **cross-sectional** analysis ("What is the maximum jumping height 
of a person given their age at a certain point in time, whereas both variables are measured at the same time?");
- or at different time points, known as **longitudinal analysis** ("What is the maximum jumping height of a person 10 years later ($t_2$)
 given their baseline health status at time $t_1$?").

The **simplest statistical model** would be the mean model where $Y$ is "predicted" by a 
constant: $Y = c$ which (at least in the classical linear regression) turns out to be $c = \bar{x}$. 
This simple model is often surprisingly good, or, to put it in other words, models with more complexity
are often not that much better with regards to multiple metrics.

## A (simple) model for adult body heights in the Bayesian framework

As repetition, read the parts about [Bayes statistics from QM1](https://jdegenfellner.github.io/Script_QM1_ZHAW/bayes_statistics.html) 
again to refresh your memory about the Bayesian framework.

It's recommendable to read the beginning of the book [Statistical rethinking](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf)
up until page 39 as well. We are not completely new to the topic of Bayes due 
to QM1.

We want to **start building our first model** right away.

Let's begin with the example in 
[Statistical rethinking](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf) 
using data from the [!Kung San](https://en.wikipedia.org/wiki/%C7%83Kung_people) starting on page 79.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
library(rethinking)
data("Howell1")
d <- Howell1
str(d)
d2 <- d[d$age >= 18, ] # only adults
```
We want to model the adult height of the !Kun San people.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
library(tidyverse)
d2 %>% ggplot(aes(x = height)) + geom_histogram()
```

Since we already have domain knowledge in this area, we can say that heights are usually normally distributed,
or at least a mixture of normal distrubutions (female/male).
We assume the following model:
$$h_i \sim \text{Normal}(\mu, \sigma)$$

As in QM1, we want to start with a Bayesian model and hence, we need some priors.
Since we are in Switzerland and just for fun, we use the [Swiss body heights](https://www.bfs.admin.ch/asset/de/30305714) 
as **prior for the mean**. According to the linke (Bundesamt für Statistik), 
the mean height of $n=7,182,252$ people in the Swiss sample is
$\bar{h_i}=171.1$ 

\[
h_i \sim \text{Normal}(\mu, \sigma)
\]
\[
\mu \sim \text{Normal}(171.1, 20)
\]
\[
\sigma \sim \text{Uniform}(0, 50)
\]

Of course we would not need the prior here due to the large sample size, but for enough for demonstration.
We are not completely uninformed about body heights and express our knowledge with the prior for $\mu$.$
The $20$ in the prior for the mean expresses our range of possible true mean values and aknowledge
that there are a variety of different subpopulations with different means.

Using the Swiss data in the link one could estimate that the standard deviation of the heights 
from $21,873$ Swiss people is around is $25.6553$ cm ([Exercise 1](#exercise1_Intro)).

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
curve(dnorm(x, 171.1, 20), from = 100, to = 250)
```

The **prior for $\sigma$** is uniform between $0$ and $50$ cm. This is a very wide prior and
just constrains the values to be positive and below $50$ cm. 
This could be stronger of course.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
curve(dunif(x, 0, 50), from = -10, to = 60)
```
Note, we didn’t specify a prior probability distribution of heights
directly, but once we’ve chosen priors for $\mu$ and $\sigma$, these imply a 
prior distribution of individual heights. 

**Without** even having seen the **new data**, we can check what our prior
(model) for heights would predict.
So, we simply draw $\mu$ and $\sigma$ from the priors and then draw heights 
from the normal distribution using the drawn parameters.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
sample_mu <- rnorm(10^4, 171.1, 20)
sample_sigma <- runif(10^4, 0, 50)
prior_h <- rnorm(10^4, sample_mu, sample_sigma)
length(prior_h)
dens(prior_h)
```

The prior is not itself a Gaussian distribution, but a distribution of
relative plausibilities of different heights, before seeing the data.

Now, there are a couple of different ways to estimtate the model incorporating 
the new data. For didactic reasons, grid approximation is often used (as in the book).
For many parameters, grid approximation becomes more and more infeasible (due to combinatorial explosion). 

We will skip that for now and use quadratic approximation instead which
works well for many common procedures in applied statistics (like linear regression).
Later, you'll probably use (or the software in the background) mostly Markov 
chain Monte Carlo (MCMC) sampling to get the posterior. 
[Pages 39 and the following](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf) 
explain the 3 concepts grida approximation, quadratic approximation and MCMC.

In short, quadratic approximation assumes that our posterior distribution 
of body heights can be approximated well by a normal distribution, 
at least near the peak.

Using the library `rethinking` we can estimate the model using quadratic approximation.

First, we define the model in the `rethinking` syntax (see R code 4.25 in the book).

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
library(rethinking)
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(171.1, 20),
  sigma ~ dunif(0, 50)
)
```
Then we estimate/fit the model using quadratic approximation.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
m_heights <- rethinking::map(flist, data = d2)
```

Now let's take a look at the fit *maximum a posteriori* model:

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
precis(m_heights)
```
Above, we see the mean of the posterior for $\mu$ **and** $\sigma$;
and a 89% credible interval for those parameters.

We can now plot the posterior distribution of the mean and the standard deviation.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
post <- extract.samples(m_heights, n = 10^4)
head(post)
dens(post$mu)
dens(post$sigma)
```

Note, that these samples come from a multi-dimensional distribution.
They are not necessarily independent from each other, but in this case they are.
We know this from the model definition above. $\mu$ and $\sigma$ are both
defined as normal respectively uniform distributions and by definition do not
influence each other.

Let's verify this:

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
vcov(m_heights)
```
gives you the variance-covariance matrix of the parameters of the posterior
distribution. In the diagonal you see the variance of the parameters.

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
diag(vcov(m_heights))
```
And we can compute the correlation matrix easily:

```{r, echo=TRUE, , message=FALSE, warning=FALSE}
cov2cor(vcov(m_heights))
```

Let's plot the posterior in 3D, because we **can**:

```{r posterior-3d-correct, echo=FALSE, message=FALSE, warning=FALSE}
library(plotly)
library(MASS)  # For kernel density estimation

# Extract posterior samples
posterior_samples <- extract.samples(m_heights)

# Calculate bivariate density using kde2d
density_est <- kde2d(posterior_samples$mu, posterior_samples$sigma, n = 100)

# Create a 3D surface plot
plot_ly(
  x = density_est$x,
  y = density_est$y,
  z = density_est$z,
  type = "surface",
  colorscale = "Viridis"
) %>%
  layout(
    title = "3D Posterior Density of μ and σ",
    scene = list(
      xaxis = list(title = "μ (mu)"),
      yaxis = list(title = "σ (sigma)"),
      zaxis = list(title = "Density")
    )
  )
```




## Exercises

### [E] Exercise 1 {#exercise1_Intro}

Use the [Swiss body heights](https://www.bfs.admin.ch/asset/de/30305714) data to 
determine 
- the 95% "Vertrauensintervall" for $\mu$ and
- calculate the standard deviation of the heights from $21,873$ Swiss people.
