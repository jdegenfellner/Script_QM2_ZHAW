# Introduction {#intro}

## What is statistical modeling and what do we need this for?

Typically, one simplifies the complex reality (and loses information) in order to make it better
understandable, mathematically treatable and to make predictions. 

Underlying our models, there are theories which should be [falsifiable](https://en.wikipedia.org/wiki/Falsifiability)
and testable.
For instance, I would be really surprised if I pull up my multimeter and measure the voltage (V) and
electric current (I) at a resistence (R) in a circuit and find that [Ohm's law](https://en.wikipedia.org/wiki/Ohm%27s_law) $V = IR$ is not true. 
This [**law**](https://en.wikipedia.org/wiki/Scientific_law)
can be tested over and over again and if one would find a single valid counterexample, 
the law would be falsified. It is also true that the law is probably not 100% accularate,
but an extremely good approximation of reality. Real-world measurements carry 
measurement errors and when plotting the data, one would see that the data points
might not lie exactly on a straight line. This is not a problem.

A [statistical model](https://en.wikipedia.org/wiki/Statistical_model) 
is a mathematical framework that represents the 
relationships between variables, helping us understand, infer, and
predict patterns in data. It acts as a bridge between observed data 
and the real-world processes that generated them. In health research, 
where variability and uncertainty are inherent, statistical models are 
valuable tools for making sense of complex phenomena. 
You can watch [this](https://www.youtube.com/watch?v=3d5ivs_8amQ&ab_channel=VeryNormal) as short intro.

Depending on the task at hand, we would use different models.
In any case, logical reasoning and critical thinking comes first, 
then comes the model. **It makes no sense to estimate statistical models just for the sake of it**.

**All models are wrong, but some are useful**. 
Or to quote [George Box](https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949):

> "Since all models are wrong the scientist cannot obtain
> a 'correct' one by excessive elaboration. On the contrary
> following William of Occam he should seek an economical
> description of natural phenomena. Just as the ability to
> devise simple but evocative models is the signature of the
> great scientist so overelaboration and overparameterization is often the mark of mediocrity."

### Explanatory vs. Predictive Models

I can recommend [this](https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full) 
article by Shmueli et al. (2010) on this topic.

Statistical models serve different purposes depending on the research question. Two primary goals are **explanation** 
and **prediction**, and each requires a different approach:

**Explanatory Models** focus on understanding causal relationships. 
These models aim to uncover mechanisms and answer **"why"** 
questions. For example:
   
   - Does smoking increase the risk of lung cancer? **Yes**. (If you want to see what a large effect-size looks like, check out [this study](https://bmjopen.bmj.com/content/bmjopen/8/10/e021611.full.pdf).)
   - Does pain education and graded sensorimotor relearning improve disability (a question we ask in 
   our [Resolve Swiss project](https://data.snf.ch/grants/grant/220585))?

Explanatory models are **theory-driven**, designed to test hypotheses. Here, one wants to understand the underlying
mechanisms and the relationships between variables and hence often uses models that are interpretable, like linear regression.

**Predictive Models** prioritize forecasting future outcomes based on patterns in the data. 
These models aim to answer **"what will happen?"** For instance:
   
   - Can we predict the likelihood of hospital readmission based on patient characteristics?
   - What is the expected recovery time for a patient given her therapy and baseline health status?

Predictive models are **data-driven**, often using complex algorithms to achieve high accuracy. 
Their success is measured using metrics like RMSE, AUC, or **prediction error on new, unseen data**.
Any amount of model complexity is allowed. One could for instance estimate a 
[neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)) ("just" another statistical model) 
with many hidden layers and neurons in order to improve prediction quality. Interpretability of the model weights is not a priority here.

While explanatory and predictive goals often complement each other, 
their differences highlight the importance of clearly defining the purpose 
of your analysis. In applied health research, explanatory models help identify 
causal mechanisms, while predictive models can guide real-world decisions by 
providing actionable forecasts. Together, they enhance both our understanding 
of phenomena and our ability to make informed decisions in complex environments.

Another important distinction is between **individual vs. population** prediction.
In the smoking example above, we can be very sure about the mean effects of smoking has on lung cancer.
On an individual level, it is [harder to predict the outcome](https://www.liebertpub.com/doi/10.1089/rej.2019.2298). 
Nevertheless, individual predictions will be (notably) better than random guessing. We will discuss this in greater detail.

### Using Statistical Models in Physiotherapy Research

Physiotherapy research often involves answering diverse and complex questions, from understanding treatment 
effectiveness to exploring patient perceptions. Statistical models are essential tools in this field because they enable researchers to systematically analyze data, uncover patterns, and make informed decisions grounded in evidence. 

In evidence-based physiotherapy, statistical models support several key areas:
- **Evaluating Treatment Effectiveness**: For instance, a randomised controlled trial (RCT) might use a statistical
 model to compare the effectiveness of two interventions, such as manual therapy versus exercise, for managing low back pain.
- **Understanding Prognostic Factors**: Cohort studies use models to identify variables that predict recovery times or functional 
improvements in patients with musculoskeletal conditions.
- **Diagnosing and Validating Tests**: Cross-sectional studies frequently apply statistical models to assess the diagnostic 
accuracy of tools like physical examination techniques or imaging modalities.
- **Exploring Patient Experiences**: Mixed-method studies may incorporate models to quantify trends while integrating 
qualitative insights for deeper understanding.

These models help bridge the gap between raw data and clinical practice, allowing physiotherapists to:
1. Test hypotheses (e.g., does adding manual therapy to standard care improve recovery rates?).
2. Explore relationships (e.g., is age or comorbidity a stronger predictor of delayed recovery?).
3. Make predictions (e.g., which patients are most likely to benefit from a specific intervention?).

By integrating statistical models into research, physiotherapists can better interpret study results and apply 
them in clinical decision-making. This aligns with the principles of evidence-based practice, ensuring that 
interventions are both effective and tailored to individual patient needs.