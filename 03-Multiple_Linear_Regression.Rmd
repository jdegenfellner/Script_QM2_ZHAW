# Multiple Linear Regression

So far, we have dealt with the simple mean model and the model with one predictor
in the Bayesian and Frequentist framework.
We will now add another predictor and subsequently an interaction term to the model.

## Linear Regression with 2 Predictors in the Baysian Framework

### Meaning of "linear"
What is a linear model? The term "linear" refers to the relationship of the predictors 
with the dependent variable (or outcome). The following model is also linear:

$$height_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2$$

The model is linear in the parameters $\beta_0, \beta_1, \beta_2$ but not in the predictors $x_i$.
The term $x_i^2$ is ok, since the heights are just sums of multiples of the predictors (which can be nonlinear).
This model is not a linear model anymore:

$$height_i = \beta_0 + \beta_1 x_i + e^{\beta_2 x_i^2}$$

$\beta_2$ is now is the exponent of $e$. It would also not be linear,
if the coefficients are in a square root or in the denominator of a fraction,
or in a sine or in a logarithm. You get the idea.

### Adding a transformed predictor to the model {#adding_transformed_predictor}

The world is not flat, although some people on YouTube might tell you otherwise.
In our context, not all regression is linear.

Around 4.5. in the book [Statistical Rethinking](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf)
there is are lineare regression using a quadratic term for weight. 
It is a principle, called the "**variable inclusion principle**", that we always include the lower order terms when fitting a model
with higher order terms. See [Westfall](https://vdoc.pub/documents/understanding-regression-analysis-a-conditional-distribution-approach-84oqjr8sqva0), 
p. 213. If we do not include the lower order terms, the coefficient does not measure what
we want it to meausure (curvature in our case). For instance, if we want to model a quadratic relationship (parabola) between
weight and height, we also have to include the linear term for weight ($x_i$).
Since we do not assume the relationship between weight and height to be linear but
quadratic (which is a polynomial of degree 2), we call this a 
[polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=In%20statistics%2C%20polynomial%20regression%20is,nth%20degree%20polynomial%20in%20x.).

This time, lets look at the whole age range of data from the !Kung San people.

```{r}
library(rethinking)
library(tidyverse)
data(Howell1)
d <- Howell1
d %>% ggplot(aes(x = weight, y = height)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_smooth(method = "loess", se = FALSE, color = "red")
```

It would not be a good idea to fit a linear trend through this data,
because we would not caupture the relationship adequately. 
The red line is a [loess smothing](https://en.wikipedia.org/wiki/Local_regression) line
which is often used to capture non-linear relationships.
The blue line is the usual line from classic linear regression (from the previous chapter).
Which one describes the data more accurately? 
In this case it is obvious, a non-linear relationship is present and it might be a good idea 
to model it. Modeling the relationshiop with a linear trend, leads to bad residuals with structure.
We will demonstrate this in the freuqentist setting.
Unfortunately, in more complex settings, with more predictors, it is not always so easy to see.

This time, we use the mean for the prior from the book ($178 cm$).
The model equations are (see [exercise 2](#exercise2_multiple_regression)):

\begin{eqnarray*}
h_i &\sim& \text{Normal}(\mu_i, \sigma) \\
\mu_i &=& \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha &\sim& \text{Normal}(178, 20) \\
\beta_1 &\sim& \text{Log-Normal}(0, 1) \\
\beta_2 &\sim& \text{Normal}(0, 1) \\
\sigma &\sim& \text{Uniform}(0, 50)
\end{eqnarray*}

The prior for $\beta_1$ is log-normal, because we can reasonably assume
the the overall linear trend is positive. The prior for $\beta_2$ is normal, because
we are not so sure. If we thought back to our school days to the topic of 
"curve discussion" or parabolas, we could probably also assume that $\beta_2$ is negative.
But, data will show.

How can we interpret the model equations? 
The model assumes that the **expected** height $\mu_i$ of a person $i$
depends non-linearly on the weight $x_i$ of the person.
We are in the business of mean-modeling. 
The prior for $\sigma$ is uniform as before.
The prior for $\alpha$ is normal with mean $178$ and standard deviation $20$
because this is what we can expect from body heights in our experience.

Let's **fit the model**:

We standardize the weight again and add the squared weights to the data set.
Standardizing the the predictors is a good idea, especially in polynomial regression
since squares and cubes of large numbers can get huge and cause numerical problems.

Let's fit the model with the quadratic term for weight:

```{r}
# Standardize weight
d$weight_s <- (d$weight - mean(d$weight)) / sd(d$weight)
# Square of standardized weight
d$weight_s2 <- d$weight_s^2
m4.1 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight_s + b2*weight_s^2,
    a ~ dnorm(178, 20),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data = d)
precis(m4.1)
```
$\beta_2$ is indeed negative. 
We get our **[joint distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)** 
of the **four model parameters**.
Let's look at the fit using the mean estimates of the posterior distribution:

```{r}
# Summarize the model parameters
model_summary <- precis(m4.1)
params <- as.data.frame(model_summary)

# Extract parameter values
a <- params["a", "mean"]       # Intercept
b1 <- params["b1", "mean"]     # Coefficient for standardized weight
b2 <- params["b2", "mean"]     # Coefficient for squared standardized weight

# Generate a sequence of standardized weights for the fitted curve
weight_fine <- seq(min(d$weight_s), max(d$weight_s), length.out = 200)

# Calculate the fitted values using the quadratic equation
height_fitted <- a + b1 * weight_fine + b2 * weight_fine^2

# Plot the scatterplot
plot(d$weight_s, d$height, pch = 16, col = "blue",
     xlab = "Standardized Weight", ylab = "Height (cm)",
     main = "Scatterplot with Fitted Curve (Standardized Weight)")

# Add the fitted curve
lines(weight_fine, height_fitted, col = "red", lwd = 2)

# Add a legend
legend("topright", legend = c("Observed data", "Fitted curve"),
       col = c("blue", "red"), pch = c(16, NA), lty = c(NA, 1), lwd = 2)
```

This fits much better than the linear model. In the book,
there is also a polynomial regression with a cubic term for weight.
Maybe this fits even better (see [exercise 1](#exercise1_multiple_regression)).

### Adding another predictor to the model {#adding_predictor_bayes}

Since the !Kung San data set has already such a high $R^2$ with the quadratic term 
(and possibly higher with the cubic term), we will use the created data set from 
[below](#adding_predictor_freq) in the frequentist setting
to estimate the coefficients of the model with two predictors.

We use rather uniformative priors and fit the model using `quap`:


```{r}
library(rethinking)
set.seed(123)
n <- 100
X1 <- rnorm(n, 0, 5)
X2 <- rnorm(n, 0, 5)
Y <- 10 + 0.5 * X1 + 1 * X2 + rnorm(n, 0, 2) # true model
df <- data.frame(X1 = X1, X2 = X2, Y = Y)

# fit model
m4.2 <- quap(
  alist(
    Y ~ dnorm(mu, sigma),
    mu <- a + b1*X1 + b2*X2,
    a ~ dnorm(10, 10),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data = df)
precis(m4.2)
```


#### Checking model assumptions {#check_model_bayes}
Andrew Gelman mentions in some of his talks (see [here](https://sites.stat.columbia.edu/gelman/research/published/philosophy_chapter.pdf) for more details) 
that many Bayesians he met do not check their models, since they reflect subjective probability.
As I said in the introduction, one should not be afraid to check model predictions against the observed
and probably new data. If a model for predicting BMI performs much worse on a new data set,
we can probably conclude that the model does not reflect the general relationship between the predictors
and the dependent variable. We **do not ask** the question if a model is true or false, but if it is useful or
how badly the model assumptions are violated.

For further, more detailed information on model checking, refer to chapter 6 of 
[Gelman's book](https://sites.stat.columbia.edu/gelman/book/BDA3.pdf).

Anyhow, we plot two posterior predictive checks here.
We test the model within the same data set. We create new observations
by drawing from the posterior distribution and compare these with the acutally 
observed values. This is called **posterior predictive checks**.

**First, we plot the observed $Y$ values against the predicted $Y$ values ($=\hat{Y}$)** 
from the model (as in Statistical rethinking, Chapter 5).
Although these practically never lie on the line $y=x$, they should be sufficiently 
close to it. We could also compare these two plots with the mean-model (see [exercise 6](#exercise6_multiple_regression)).

```{r}
# 1) Posterior predictive checks Y vs Y_hat
# see Statstical Rethinking p 138.
# call link without specifying new data
# so it uses the original data
mu <- link(m4.2)

# summarize samples accross cases
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI, prob = 0.89)

# simulate observations
# again, no new data, so uses original data
D_sim <- sim(m4.2, n = 1e4)
D_PI <- apply(D_sim, 2, PI, prob = 0.89)

plot(mu_mean ~ df$Y, col = rangi2, ylim = range(mu_PI), 
     xlab = "Observed Y", ylab = "Model-Predicted Y")
abline(a = 0, b = 1, lty = 2)
for(i in 1:nrow(df)) lines(rep(df$Y[i], 2), mu_PI[,i], col = rangi2)
```

As we can see, the model fits the data quite well. The points are close to the dashed line ($y=x$).
No under- or overestimation is visible. The model seems to capture the relationship between the predictors $X_1$ and $X_2$
and the dependent variable $Y$ quite well. If there were patches of data points above or below the dashed line,
we would probably have to reconsider the model definition and think about why these points are not captured by the model.

**Next, we plot the posterior predictive plots** analog to the upper left in the `check_model` output.

```{r}
library(scales)  # For the alpha function to adjust transparency

# 2) Posterior predictive densities
# Simulate observations using the posterior predictive distribution
D_sim <- sim(m4.2, n = 1e4)  # Generate 10,000 simulated datasets

# Calculate densities for all samples
densities <- apply(D_sim, 1, density)

# Find the maximum density value for setting the y-axis limits
max_density <- max(sapply(densities, function(d) max(d$y)))

# Create the density plot with predefined ylim
plot(NULL, xlim = range(df$Y), ylim = c(0, max_density),
     xlab = "Y", ylab = "Density",
     main = "Comparison of Observed and Predicted Densities")

# Add 100 posterior predictive density lines
set.seed(42)  # For reproducibility
n_lines <- 100
samples <- sample(1:1e4, n_lines)  # Randomly sample 100 posterior predictive datasets
for (s in samples) {
  lines(density(D_sim[s, ]), col = alpha("lightblue", 0.3), lwd = 1)
}

# Add the density line for the observed Y values
obs_density <- density(df$Y)
lines(obs_density$x, obs_density$y, col = "green", lwd = 2)

# Add legend
legend("topright", legend = c("Posterior Predictive Densities", "Observed Density"),
       col = c("lightblue", "green"), lty = 1, lwd = c(1, 2))
```

The light blue lines show distributions of model predicted $Y$ values. 
The green line shows the distribution of the observed $Y$ values.
As we can see, there seem to be no systematic differences between the observed and predicted values. 
The model seems to capture the relationship well.
If we see systematic deviations here, we need to reconsider the model definition. 

**Example**: If you want to predict pain ($Y$ variable) and you have a lot of zeros (pain-free participants)
you will probably see a discrepancy between the observed and predicted values in this plot.
What could you do? You could use a two step process (model the probability that a person is pain-free 
and then model the pain intensity for the people who have pain) or use a different model (like a zero-inflated model).

Note that we did not explicitely assume normally distributed errors in the model definition above,
so we won't check this here but in the Frequentist framework below.

## Linear regression with 2 Predictors in the Frequentist Framework
### Adding a transformed predictor to the model

No, let's fit the same model as above in the frequentist framework.

The model is:

$$height_i = \alpha + \beta_1 weight_i + \beta_2 weight_i^2 + \varepsilon_i$$
whereas
$$\varepsilon_i \sim N(0, \sigma)$$

We are looking for fixed, but unknown, parameters $\alpha$, $\beta_1$, $\beta_2$ and $\sigma$.
This is fit again using the `lm` function in R which uses least squares to estimate the parameters.
At this point I could torture you with [matrix algebra](https://en.wikipedia.org/wiki/Matrix_(mathematics)) 
and show you the [normal equations](https://en.wikipedia.org/wiki/Linear_least_squares) for linear regression,
but I will spare you for now.
Note that the least squares algorithm for fitting the curve works for all
kinds of functions. We could also fit an exponential curve using the same
technique.

```{r}
# scale weight
d$weight_s <- scale(d$weight)
# Fit the model
m4.2 <- lm(height ~ weight_s + I(weight_s^2), data = d)
summary(m4.2)
```

See `?I` in R. This command is used so that R knows that it should 
treat the "^2" as "square" and not as formula syntax.
We could also create a new variable as before. Whatever you prefer.

#### Interpretation of output and coefficients

- The intercept $\alpha$ is the **model-predicted height** of a person of **average weight**.
  Note that this is not the average height of the people in the data set, since the 
  mean model is also a model, but different from ours.
- The residuals have range from $-19.97$ to $19.51$. So, the model maximally
  overestimates the heights by $19.97$ cm and underestimates by $19.51$ cm.
  These numbers are plausible when you look at the scatterplot with the fitted
  curve.
- The coefficients $\beta_1$ and $\beta_2$ agree with the Bayes estimates.
  Specifically, $\beta_2$ is non-zero indicating curvature.
- If you like $p$-values: All the hypotheses that the coefficients are zero
  are rejected. The $p$-values are very small. The data can not be explained
  by chance alone. On the other hand, for at least $\beta_1$ and
  and the global test this is not a surprise when you look at the scatterplot.
- The $R^2$ is a whopping $0.96$ which could be a sign of overfitting, but
  in this case we conclude that the true relationship is caputured rather well.
  [Overfitting](https://en.wikipedia.org/wiki/Overfitting) would occur if 
  our curve would wiggle around the data points,
  so we would fit the data too much to the noise in the data than
  the underying trend.

#### Checking model assumptions

```{r, warnings=FALSE}
check_model(m4.2)
```

If we want to be perfectionists, we could remark that (upper right plot)
in the lower fitted values the residuals are more negative, 
meaning that the model overestimates the heights in this region.
In the middle region the model underestimates a bit and we can see
a positive tendency in the residuals. Apart from that,
the diagnostic plots look excellent.

### Adding another predictor to the model {#adding_predictor_freq}

Now, we add another predictor to the model. We use $X_1$ and $X_2$ 
**simultaneously** to predict $Y$. We are now in the lucky situation that
we can still visualize the situation in 3D. The regression line from simple
linear regression
becomes a [plane](https://stackoverflow.com/questions/47344850/scatterplot3d-regression-plane-with-residuals).
The vertical distances between the data points and the plane are the residuals.
See [here](https://rpubs.com/pjozefek/576206) or 
[here](https://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization)
at the end for examples.
Minimizing the sum of the squared errors gives again 
the estimates for the coefficients. 

For demonstration purposes, we can **create data ourselves** with known
coefficients. This is the same as [above](#adding_predictor_bayes).
This is the true model, which we usually do not know:

$$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i$$
$$ \varepsilon_i \sim N(0, \sigma^2)$$
$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$
$$ i = 1 \ldots n$$

for example:

$$ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + \varepsilon_i$$
$$ \varepsilon_i \sim N(0, 5)$$
$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2$$
$$ i = 1 \ldots n$$


According to the model, the conditional expected value of $Y_i$ given $X_1 = x_1$ and $X_2 = x_2$
is a linear function of $x_1$ and $x_2$. Note, that small letters are realized values
of random variables. Also note, that in the expectation the error term goes away, since
$\mathbb{E}(\varepsilon_i) = 0$.

- If $X_1$ increases by one unit, $Y$ increases by $0.5$ units on average (in expectation).
- If $X_2$ increases by one unit, $Y$ increases by $1$ unit on average (in expectation).
- If $X_1$ and $X_2$ are zero, $Y$ is $10$ on average (in expectation).

Why in expectation? Because there is still the error term which makes the whole thing random!
We can see that an increase in $X_1$ does not influence the relationship between $X_2$ and $Y$.
Hence, there is **no interaction** between $X_1$ and $X_2$ with respect to $Y$.

Now lets's draw 100 points from this model, fit the model and add the plane:

```{r}
library(plotly)

set.seed(123)
n <- 100
X1 <- rnorm(n, 0, 5)
X2 <- rnorm(n, 0, 5)
Y <- 10 + 0.5 * X1 + 1 * X2 + rnorm(n, 0, 2)
d <- data.frame(X1 = X1, X2 = X2, Y = Y)

# Fit the model
m4.3 <- lm(Y ~ X1 + X2, data = d)
summary(m4.3)

# Create a grid for the plane
X1_grid <- seq(min(d$X1), max(d$X1), length.out = 20)
X2_grid <- seq(min(d$X2), max(d$X2), length.out = 20)
grid <- expand.grid(X1 = X1_grid, X2 = X2_grid)

# Predict the values for the grid
grid$Y <- predict(m4.3, newdata = grid)

# Convert the grid into a matrix for the plane
plane_matrix <- matrix(grid$Y, nrow = length(X1_grid), ncol = length(X2_grid))

# Create the interactive 3D plot
plot_ly() %>%
  add_markers(
    x = d$X2, y = d$X1, z = d$Y,
    marker = list(color = "blue", size = 5),
    name = "Data Points"
  ) %>%
  add_surface(
    x = X1_grid, y = X2_grid, z = plane_matrix,
    colorscale = list(c(0, 1), c("red", "pink")),
    showscale = FALSE,
    opacity = 0.7,
    name = "Fitted Plane"
  ) %>%
  plotly::layout(
    scene = list(
      xaxis = list(title = "X1"),
      yaxis = list(title = "X2"),
      zaxis = list(title = "Y")
    ),
    title = "Interactive 3D Scatterplot with Fitted Plane"
  )
```

This is, of course, a very idealized situation. There is no curvature in the plane,
no interaction, no outliers, no heteroscadasticity. It's the simplest case of multiple regression
with 2 predictors. Reality is - usually - more complicated.

Let's look at summary output and check model assumptions:

```{r}
summary(m4.3)
check_model(m4.3)
```

We could repeat this simulation to get a feeling for the variability.
The posterior predictive checks look nice. In this case, we *know* that the model is true
and with this knowledge we can assess the diagnostic plots in front of us.

#### Adding variables to the model and why

This is a very complex question. We will go into it in later chapters and the next course (Methodenvertiefung).
At this point we can say this:
Depending on the goal at hand (prediction or explanation), we add variables to the model and probably use 
other models apart from linear regression.
Prediction seems to be easier than explanation. For instance, within linear models and 
just a handful of predictors, one can even brute force the problem by searching through
all subsets of predictors. If that is not possible, one could use clever algortithms, like 
[best subest selection](https://www.sthda.com/english/articles/37-model-selection-essentials-in-r/155-best-subsets-regression-essentials-in-r/). 

- What is **not** a good idea is to throw all variables into the model and hope for the best.
- What is also not a good idea is to select variables depending on the $p$-values of the coefficients.
- **Leaving variables out**, that are important, can lead to biased estimates of the coefficients 
  ([omitted variable bias](https://en.wikipedia.org/wiki/Omitted-variable_bias)).
- But also **adding variables** can hurt conclusions from the model (see Statistical Rethinking 6.2).


### Interaction Term $X_1 \times X_2$ {#interaction_term}

I recommend reading the excellent explanations about interactions
in John Kruschke's book [Doing Bayesian Data Analysis](https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf),
15.2.2 und 15.2.3. Peter Westfall also has a nice explanation in his [book](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4) 
in section 9.3.

Our statistical model is now:

$$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \mathbf{\beta_3 X_{1i} \times X_{2i}} + \varepsilon_i$$
$$ \varepsilon_i \sim N(0, \sigma^2)$$
$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1} \times x_{2}$$
$$ i = 1 \ldots n$$

for example:

$$ Y_i = 10 + 0.5 \cdot X_{1i} + 1 \cdot X_{2i} + 0.89 \cdot X_{1i} \times X_{2i} + \varepsilon_i$$
$$ \varepsilon_i \sim N(0, 5)$$
$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = x_2) = 10 + 0.5 x_1 + 1 x_2 + 0.89 x_1 \times x_2$$
$$ i = 1 \ldots n$$

The second equation states that the conditional expectation of $Y_i$ given $X_1=x_1$ and $X_2=x_2$ 
is a function of $x_1$ and $x_2$ and their interaction $x_1 \times x_2$. We are in a different situation now.
Set for instance $x_2$ to a certain value, say $x_2 = 7$. Then the relationship (in expectation) 
between $Y$ and $X_1$ is:

$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + 0.5 x_1 + 1 \cdot 7 + 0.89 x_1 \cdot 7$$
$$ \mathbb{E}(Y_i|X_1 = x_1; X_2 = 7) = 10 + (0.5 + 0.89 \cdot \mathbf{7}) \cdot x_1 + 1 \cdot 7$$

Depending on the value of $x_2$, the *effect* of $X_1$ on $Y$ changes. 
Hence, $X_2$ **modifies** the relationship between $X_1$ and $Y$, or stated otherwise,
$X_1$ and $X_2$ **interact** with respect to $Y$. Remember, the word *effect* is 
used in a strictly technical/statistical sense and **not in a causal** sense.
It does not mean that if we *do* change $X_1$ by one unit, 
$Y$ will also change in an experiment. We are purely describing the relationship
in an associative way. We will probably touch causality in a later chapter.
Bayesian statistics and causal inference are gaining popularity. Hence, we should try to keep up.

Let's draw 100 points from this model, fit the model and add the plane (see also [exercise 4](#exercise3_multiple_regression)):

```{r}
set.seed(123)
n <- 100
X1 <- rnorm(n, 0, 5)
X2 <- rnorm(n, 0, 5)
Y <- 10 + 0.5 * X1 + 1 * X2 + 0.89 * X1 * X2 + rnorm(n, 0, 5)
d <- data.frame(X1 = X1, X2 = X2, Y = Y)

# Fit the model
m4.4 <- lm(Y ~ X1 * X2, data = d)
summary(m4.4)

# Create a grid for the plane
X1_grid <- seq(min(d$X1), max(d$X1), length.out = 20)
X2_grid <- seq(min(d$X2), max(d$X2), length.out = 20)
grid <- expand.grid(X1 = X1_grid, X2 = X2_grid)

# Predict the values for the grid
grid$Y <- predict(m4.4, newdata = grid)

# Convert the grid into a matrix for the plane
plane_matrix <- matrix(grid$Y, nrow = length(X1_grid), ncol = length(X2_grid))

# Create the interactive 3D plot
plot_ly() %>%
  add_markers(
    x = d$X2, y = d$X1, z = d$Y,
    marker = list(color = "blue", size = 5),
    name = "Data Points"
  ) %>%
  add_surface(
    x = X1_grid, y = X2_grid, z = plane_matrix,
    colorscale = list(c(0, 1), c("red", "pink")),
    showscale = FALSE,
    opacity = 0.7,
    name = "Fitted Plane"
  ) %>%
  plotly::layout(
    scene = list(
      xaxis = list(title = "X1"),
      yaxis = list(title = "X2"),
      zaxis = list(title = "Y")
    ),
    title = "Interactive 3D Scatterplot with Fitted Plane"
  )
```

The term `X1 * X2` is a shortcut for `X1 + X2 + X1:X2` where `X1:X2` is the interaction term.
R automatically includes the main effects of the predictors when an interaction term is included.
The true but usually unknown $\beta$s are estimated quite precisely.

#### Formal test for interaction
We could apply a formal test for the interaction term by model comparison.
The command `anova(., .)` would compare the two models and test if the change in the
residual sum of squares is statistically interesting.

```{r}
m4.5 <- lm(Y ~ X1 + X2, data = d) # without interaction
anova(m4.5, m4.4)
```

One can show that the following test statistic is F distributed under the null hypothesis:

$$ F = \frac{\left(RSS_{\text{Model 1}} - RSS_{\text{Model 2}}\right) / \left(df_{\text{Model 1}} - df_{\text{Model 2}}\right)}{RSS_{\text{Model 2}} / df_{\text{Model 2}}}$$

where $RSS$ is the residual sum of squares, 
$df$ are the degrees of freedom of the residual sum of squares for both models.

The output of the `anova` command shows us the residual degress of freedom (`Res.Df`)
of both models, the residual sum of squares errors of both models (`RSS`), 
the sum of squared errors between model 1 and model 2 (`Sum of Sq`), the value of the 
F-statistic and the $p$-value for the hypothesis, that the coefficient for
the interaction term is zero ($\beta_3=0$). Model 1 RSS has 97 degrees of freedom, since we have 100 data points
and 3 parameters to estimate ($\beta_0, \beta_1, \beta_2$). Model 2 has 96 degrees of freedom, since
we have 100 data points and 4 parameters to estimate ($\beta_0, \beta_1, \beta_2, \beta_3$).

Let's verify the value of the F statistic:

```{r}
RSS_model1 <- sum(residuals(m4.5)^2)
RSS_model2 <- sum(residuals(m4.4)^2)
df_model1 <- n - length(coef(m4.5))
df_model2 <- n - length(coef(m4.4))
F <- ((RSS_model1 - RSS_model2) / (df_model1 - df_model2)) / (RSS_model2 / df_model2)
F
# Sum of Sq
RSS_model1 - RSS_model2
```

In the numerator of the F statistic, we have the change in the residual sum of squares 
(from the small (model 1) model to the larger one (model 2), `Sum of Sq`)
per additional parameter in the model (one additional parameter $\beta_3$). 

In the denominator, we have the residual sum of squares per residual degree of freedom of 
the larger model (model 2). Hence, in the numerator we have the information on how much
better we get with respect to the number of variables added, and in the denominator
we have information on how good the full model is with respect to its degrees of freedom.

The $p$-value is the probability of observing a value of the F statistic as extreme or more 
extreme than the one we observed, given that the null hypothesis is true. Here, 
the $p$-value is extremely small. So, statistically we would see an improvement in RSS
which is not explainable by chance alone.
But **let's be careful with $p$-values** and especially with fixed cutoff values for $\alpha$, 
which we will **never** use in this script.
Even for a rather small effect $\beta_3$, we would reject the null hypothesis, if only the sample
size is large enough. Since a very small effect relative to $\beta_1$ and $\beta_2$ would
probably not be of practical interest, one should be careful with looking at $p$-values alone.
For instance, in Richard McElreath's book [Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/),
there are no $p$-values at all. I like that.

If you again look at the comparison of the RSS between the two models, you would 
immediately see that the model with the interaction term is better (at least with respect to this metric). 
The difference is huge. We have already mentioned in the context of $R^2$ not to overinterpret 
such metric, because RSS is monotonically descreasing with number of variables added and reaches
zero when the number of variables equals the number of data points (see [exercise 3](#exercise3_multiple_regression)).

### Using an interaction plot to see a potential interaction {#interaction_plot}

Chronologically before we include an interaction term in the model, we can use an interaction plot
to see if there is a potential interaction between the predictors.
We can just create a catorical predictor out of the continuous predictors.
We just categorize the predictors into quartiles and plot the means of the dependent variable ($Y$).
If the lines are parallel, there is no interaction. If the lines are not parallel, 
there might be an interaction.

```{r}
n <- 100
X1 <- rnorm(n, 0, 5)
X2 <- rnorm(n, 0, 5)
Y <- 10 + 0.5 * X1 + 1 * X2 + 0.89 * X1 * X2 + rnorm(n, 0, 5)
d <- data.frame(X1 = X1, X2 = X2, Y = Y)

# Create categorical variables based on quartiles
d$X2_cat <- cut(d$X2, 
                breaks = quantile(d$X2, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE), 
                include.lowest = TRUE, 
                labels = c("Q1", "Q2", "Q3", "Q4"))

d$X1_cat <- cut(d$X1, 
                breaks = quantile(d$X1, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE), 
                include.lowest = TRUE, 
                labels = c("Q1", "Q2", "Q3", "Q4"))

# Create the interaction plot
interaction.plot(d$X2_cat, d$X1_cat, d$Y)
```

There seems to be an intercation of the predictors with respect to $Y$. The lines are not parallel.
If there was no interaction, the change in $Y$ with respect to $X_1$ would be the same for all levels of $X_2$.
This seems not to be the case here.
See [exercise 5](#exercise5_multiple_regression).

If we had one or both predictors already categorical, we would not have to discretize them before.

### Simpsons Paradox {#simpsons_paradox}

The [Simpsons paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) is a phenomenon, 
in which a trend appears in several different groups of data but disappears 
or reverses when these groups are combined. I agree with the criticism that this is not really 
a paradox but a failure to consider confounding variables adequately. Let's quickly invent an example.
We are interested in the relationship hours of muscle training and strength (not based on evidence)
in children vs. adults. Within both groups there will be an increasing relationship. The more training,
the more muscle strength. But if we combine the groups, we will see a decreasing relationship.

```{r}
library(tidyverse)
n <- 100
age <- c(rep("child", n/2), rep("adult", n/2))
training <- c(rnorm(n/2, 0, 5) + 30, rnorm(n/2, 0, 5)+ 10)
strength <- c(
  10 + 0.5 * training[1:(n/2)] + rnorm(n/2, 0, 2), # For children
  25 + 0.5 * training[(n/2 + 1):n] + rnorm(n/2, 0, 2) # For adults
)

d <- data.frame(age = age, training = training, strength = strength)

ggplot(d, aes(x = training, y = strength, color = age)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + # Group-specific regression lines
  geom_smooth(data = d, aes(x = training, y = strength), 
              method = "lm", se = FALSE, color = "black", linetype = "dashed", linewidth = 1.2) + # Overall regression line
  labs(title = "Regression Lines for Training and Strength",
       x = "Training",
       y = "Strength") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

- **Group-Specific Trends**:
   - In the group of **children** (blue line), strength **increases** with training, as indicated by the positive slope of the regression line.
   - Similarly, in the group of **adults** (red line), strength also **increases** with training.

- **Overall Trend**:
   - When both groups are combined, the overall regression line (black, dashed) shows a **negative slope**, suggesting that **strength decreases** with training. 
   - This overall trend is opposite to the trends observed within the individual groups.

- **Why Does This Happen?**
   - This paradox occurs because the relationship between the grouping variable (`age`) and the independent variable (`training`) creates a confounding effect.
   - In this case:
     - **Children** tend to have higher training values overall, while **adults** tend to have lower training values.
     - The overall regression line reflects the imbalance between these groups rather than the true within-group relationships.

This (fictitious) example illustrates the importance of considering group-level differences when analyzing data. 
In multiple regression this might not be so straightforward, since we cannot just plot the data and see the effect
as in our toy example. We will discuss this later.

Richard McElreath also has a nice example for this on [github](https://github.com/rmcelreath/causal_salad_2021/blob/main/1_causal_salad.r)
and explains this in a [video](https://www.youtube.com/watch?v=KNPYUVmY3NM&ab_channel=RichardMcElreath).

```{r}
library(rethinking)

# d-separation plots
a <- 0.7
cols <- c( col.alpha(1,a) , col.alpha(2,a) )

# pipe
# X -> Z -> Y
N <- 1000
X <- rnorm(N)
Z <- rbern(N,inv_logit(X))
Y <- rnorm(N,(2*Z-1))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)

# fork
# X <- Z -> Y
N <- 1000
Z <- rbern(N)
X <- rnorm(N,2*Z-1)
Y <- rnorm(N,(2*Z-1))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)

# collider
# X -> Z <- Y
N <- 1000
X <- rnorm(N)
Y <- rnorm(N)
Z <- rbern(N,inv_logit(2*X+2*Y-2))

plot( X , Y , col=cols[Z+1] , pch=16 )
abline(lm(Y[Z==1]~X[Z==1]),col=2,lwd=3)
abline(lm(Y[Z==0]~X[Z==0]),col=1,lwd=3)
abline(lm(Y~X),lwd=3,lty=3)
```

- Pipe: In this setting $X$ is associated with $Z$ and $Z$ is associated with $Y$. $X$ and $Y$ are also associated.
  But if we condition on $Z$, the association between $X$ and $Y$ disappears. This is the pipe.
  This means, if we **know** the value of $Z$, $X$ does not give us any information about $Y$.
  This can be seen in the first plot: Once we are within $Z=0$ or $Z=1$, $X$ does not give us any information about $Y$,
  i.e., the point cloud is horizontal and there is no correlation.

  Or in the framework of Simpsons paradox:

```{r}
# pipe
N <- 1000
X <- rnorm(N)
Z <- rbern(N,inv_logit(X))
Y <- rnorm(N,(2*Z-1))
mod1 <- lm(Y ~ X)
summary(mod1)
mod2 <- lm(Y ~ X + Z)
summary(mod2)
```

Adding $Z$ to the model makes the coefficient for $X$ disappear. 

See [exercise 7](#exercise7_multiple_regression).

## Exercises

### [M] Exercise 1 {#exercise1_multiple_regression}

- Fit a model with a cubic term for weight and height of the !Kung San people.
- Add the prediction bands as seen in the book.
- Come up with an explanation for the functional form of this relationship.
- Could there be reasons to for taking a less complicated model 
  ([1](https://en.wikipedia.org/wiki/Statistical_model_specification), [2](https://en.wikipedia.org/wiki/Occam%27s_razor))?

### [E] Exercise 2 {#exercise2_multiple_regression}

Consider the model equations from [above](#adding_transformed_predictor) 
where we used polynomial regression to model the relationship between 
weight and height:

- Draw the model hierarchy for the model.

### [H] Exercise 3 {#exercise3_multiple_regression}

Invent a data set (or use the first 3 lines of a previous data set) 
with 3 observations of $Y$ and $X_1, X_2$ and $X_3$. You have a data frame
with 3 rows and 4 columns. 

- Fit a model with $Y$ as the dependent variable and $X_1, X_2, X_3$ as predictors.
- How big is $R^2$?
- Could you have calulated this without `lm`and R?

### [E] Exercise 4 {#exercise4_multiple_regression}

Go back to the [section about the interaction term](#interaction_term) in the linear model.

- Use the code provided
- Standardise the predictors. How are the $\beta$s changing and what is their interpration now?
- Change the relative sizes of the true but usually unknown $\beta$s. 
  What happens to the estimates and the graph?
- What happens if you change the error term and increase or decrease its variance?

### [E] Exercise 5 {#exercise5_multiple_regression}

Draw the interaction plot from the [section about the interaction plot](#interaction_plot) 
for the case when there is not interaction, i.e. $\beta_3 = 0$.

### [M] Exercise 6 {#exercise6_multiple_regression}

Go back to the model assumptions checks [above](#check_model_bayes).

- Create the same two plots for the simple mean model without predictors, just with the intercept.
- Which model fits the data better according to these posterior predictive checks?

### [E] Exercise 7 {#exercise7_multiple_regression}

Go back to the Simpson's paradox [section](#simpsons_paradox).

- Verify that the coefficient for $X$ disappears when you add $Z$ to the model for the pipe.
- What happens in the case of the collider?

## TODOS

- Add more Bayesian model checks, chapter 6 Gelman.
- If you add a lot of variables to your regression model, you can get an arbitrarily large ($\le 1$)
  $R^2$. We will verify this when we have more than 2 explanatory variables. 
- maybe create animation of points wiggling to get a feeling for the variability
- Show by simulation what Gelman talks about with significant p values. So I scan the data
  for significant p values and then simulate data with the same effect size and see how often
  I get significant p values. Especially the next effect would be probably smaller,
  especially, if one did p-hacking! Calculate a priori probability for replication (def?).
- Variability of confidence interval borders (draw from X...)
- Logistic Regression
- Chapter: Sample size calculations for logistic and multivariate regression, Proportions, ICCs, t.test
- Chapter about ICCs, but maybe reduced
- clearer difference between prediction and explanation. why do we need causal inference and
  why do we have to be careful when we throw predictors into a model, how sure can we be that
  the model does what we want it to do?
- causal terror of richards book, fork, pipe, collider
- Angenommen man hat ein masking eines Effekts und der Model fit ist aber gut (keine Voraussetzung verletzt), 
  ist diese Situation möglich?
- What about 2 interactions?
- Which variables should I include in a model and why?
- Emphasize individual prediction (if not already) - cant get below $\sigma$ in prediction
- Include information of Kahnemans book, Thinking fast and slow. Specifically, regression
  is better than intuition. Intuition only works in a valid environment, where patterns
  can be obeserved. In non-valid environments, the wrong patterns are learned. Furthermore,
  regression using some simple scores is better than expert intuition. Also Meehl is mentioned 
  by Kahneman.
- What to do if regression assumptions are violated?
- What about a real life data set?
- What about papers?
- Simulate a cohort of Master thesis with small sample sizes and assume 
  that there is a true but unknown effect.
