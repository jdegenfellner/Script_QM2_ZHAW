# Multiple Linear Regression

So far, we have dealt with the simple mean model and the model with one predictor
in the Bayesian and Frequentist framework.
We will now add another predictor and subsequently an interaction term to the model.

## Linear Regression with Two Predictors in the Baysian Framework

### Meaning of "linear"
What is a linear model? The term "linear" refers to the relationship of the predictors 
with the dependent variable (or outcome). The following model is also linear:

$$height_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2$$

The model is linear in the parameters $\beta_0, \beta_1, \beta_2$ but not in the predictors $x_i$.
The term $x_i^2$ is ok, since the heights are just sums of multiples of the predictors (which can be nonlinear).
This model is not a linear model anymore:

$$height_i = \beta_0 + \beta_1 x_i + e^{\beta_2 x_i^2}$$

$\beta_2$ is now is the exponent of $e$. It would also not be linear,
if the coefficients are in a square root or in the denominator of a fraction,
or in a sine or in a logarithm. You get the idea.

### Adding a transformed predictor to the model {#adding_transformed_predictor}

The world is not flat, although some people on YouTube might tell you otherwise.
In our context, not all regression is linear.

Around 4.5. in the book [Statistical Rethinking](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf)
there is are lineare regression using a quadratic term for weight. 
It is a principle, called the "**variable inclusion principle**", that we always include the lower order terms when fitting a model
with higher order terms. See [Westfall](https://vdoc.pub/documents/understanding-regression-analysis-a-conditional-distribution-approach-84oqjr8sqva0), 
p. 213. If we do not include the lower order terms, the coefficient does not measure what
we want it to meausure (curvature in our case). For instance, if we want to model a quadratic relationship (parabola) between
weight and height, we also have to include the linear term for weight ($x_i$).
Since we do not assume the relationship between weight and height to be linear but
quadratic (which is a polynomial of degree 2), we call this a 
[polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression#:~:text=In%20statistics%2C%20polynomial%20regression%20is,nth%20degree%20polynomial%20in%20x.).

This time, lets look at the whole age range of data from the !Kung San people.

```{r}
library(rethinking)
library(tidyverse)
data(Howell1)
d <- Howell1
d %>% ggplot(aes(x = weight, y = height)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  geom_smooth(method = "loess", se = FALSE, color = "red")
```

It would not be a good idea to fit a linear trend through this data,
because we would not caupture the relationship adequately. 
The red line is a [loess smothing](https://en.wikipedia.org/wiki/Local_regression) line
which is often used to capture non-linear relationships.
The blue line is the usual line from classic linear regression (from the previous chapter).
Which one describes the data more accurately? 
In this case it is obvious, a non-linear relationship is present and it might be a good idea 
to model it. Modeling the relationshiop with a linear trend, leads to bad residuals with structure.
We will demonstrate this in the freuqentist setting.
Unfortunately, in more complex settings, with more predictors, it is not always so easy to see.

This time, we use the mean for the prior from the book ($178 cm$).
The model equations are (see [exercise 2](#exercise2_multiple_regression)):

\begin{eqnarray*}
h_i &\sim& \text{Normal}(\mu_i, \sigma) \\
\mu_i &=& \alpha + \beta_1 x_i + \beta_2 x_i^2 \\
\alpha &\sim& \text{Normal}(178, 20) \\
\beta_1 &\sim& \text{Log-Normal}(0, 1) \\
\beta_2 &\sim& \text{Normal}(0, 1) \\
\sigma &\sim& \text{Uniform}(0, 50)
\end{eqnarray*}

The prior for $\beta_1$ is log-normal, because we can reasonably assume
the the overall linear trend is positive. The prior for $\beta_2$ is normal, because
we are not so sure. If we thought back to our school days to the topic of 
"curve discussion" or parabolas, we could probably also assume that $\beta_2$ is negative.
But, data will show.

How can we interpret the model equations? 
The model assumes that the **expected** height $\mu_i$ of a person $i$
depends non-linearly on the weight $x_i$ of the person.
We are in the business of mean-modeling. 
The prior for $\sigma$ is uniform as before.
The prior for $\alpha$ is normal with mean $178$ and standard deviation $20$
because this is what we can expect from body heights in our experience.

Let's **fit the model**:

We standardize the weight again and add the squared weights to the data set.
Standardizing the the predictors is a good idea, especially in polynomial regression
since squares and cubes of large numbers can get huge and cause numerical problems.

Let's fit the model with the quadratic term for weight:

```{r}
# Standardize weight
d$weight_s <- (d$weight - mean(d$weight)) / sd(d$weight)
# Square of standardized weight
d$weight_s2 <- d$weight_s^2
m4.1 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1*weight_s + b2*weight_s^2,
    a ~ dnorm(178, 20),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data = d)
precis(m4.1)
```
$\beta_2$ is indeed negative. 
We get our **[join distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution)** 
of the **four model parameters**.
Let's look at the fit using the mean estimates of the posterior distribution:

```{r}
# Summarize the model parameters
model_summary <- precis(m4.1)
params <- as.data.frame(model_summary)

# Extract parameter values
a <- params["a", "mean"]       # Intercept
b1 <- params["b1", "mean"]     # Coefficient for standardized weight
b2 <- params["b2", "mean"]     # Coefficient for squared standardized weight

# Generate a sequence of standardized weights for the fitted curve
weight_fine <- seq(min(d$weight_s), max(d$weight_s), length.out = 200)

# Calculate the fitted values using the quadratic equation
height_fitted <- a + b1 * weight_fine + b2 * weight_fine^2

# Plot the scatterplot
plot(d$weight_s, d$height, pch = 16, col = "blue",
     xlab = "Standardized Weight", ylab = "Height (cm)",
     main = "Scatterplot with Fitted Curve (Standardized Weight)")

# Add the fitted curve
lines(weight_fine, height_fitted, col = "red", lwd = 2)

# Add a legend
legend("topright", legend = c("Observed data", "Fitted curve"),
       col = c("blue", "red"), pch = c(16, NA), lty = c(NA, 1), lwd = 2)
```

This fits much better than the linear model. In the book,
there is also a polynomial regression with a cubic term for weight.
Maybe this fits even better (see [exercise 1](#exercise1_multiple_regression)).

..........

### Adding another predictor to the model
....


## Linear Regression with Two Predictors in the Frequentist Framework
### Adding a transformed predictor to the model

No, let's fit the same model as above in the frequentist framework.

The model is:

$$height_i = \alpha + \beta_1 weight_i + \beta_2 weight_i^2 + \varepsilon_i$$
whereas
$$\varepsilon_i \sim N(0, \sigma)$$

We are looking for fixed, but unknown, parameters $\alpha$, $\beta_1$, $\beta_2$ and $\sigma$.
This is fit again using the `lm` function in R which uses least squares to estimate the parameters.
At this point I could torture you with [matrix algebra](https://en.wikipedia.org/wiki/Matrix_(mathematics)) 
and show you the [normal equations](https://en.wikipedia.org/wiki/Linear_least_squares) for linear regression,
but I will spare you for now.
Note that the least squares algorithm for fitting the curve works for all
kinds of functions. We could also fit an exponential curve using the same
technique.

```{r}
# scale weight
d$weight_s <- scale(d$weight)
# Fit the model
m4.2 <- lm(height ~ weight_s + I(weight_s^2), data = d)
summary(m4.2)
```

See `?I` in R. This command is used so that R knows that it should 
treat the "^2" as "square" and not as formula syntax.
We could also create a new variable as before. Whatever you prefer.

#### Interpretation of output and coefficients

- The intercept $\alpha$ is the **model-predicted height** of a person of **average weight**.
  Note that this is not the average height of the people in the data set, since the 
  mean model is also a model, but different from ours.
- The residuals have range from $-19.97$ to $19.51$. So, the model maximally
  overestimates the heights by $19.97$ cm and underestimates by $19.51$ cm.
  These numbers are plausible when you look at the scatterplot with the fitted
  curve.
- The coefficients $\beta_1$ and $\beta_2$ agree with the Bayes estimates.
  Specifically, $\beta_2$ is non-zero indicating curvature.
- If you like $p$-values: All the hypotheses that the coefficients are zero
  are rejected. The $p$-values are very small. The data can not be explained
  by chance alone. On the other hand, for at least $\beta_1$ and
  and the global test this is not a surprise when you look at the scatterplot.
- The $R^2$ is a whopping $0.96$ which could be a sign of overfitting, but
  in this case we conclude that the true relationship is caputured rather well.
  [Overfitting](https://en.wikipedia.org/wiki/Overfitting) would occur if 
  our curve would wiggle around the data points,
  so we would fit the data too much to the noise in the data than
  the underying trend.

#### Checking model assumptions

```{r, warnings=FALSE}
check_model(m4.2)
```

If we want to be perfectionists, we could remark that (upper right plot)
in the lower fitted values the residuals are more negative, 
meaning that the model overestimates the heights in this region.
In the middle region the model underestimates a bit and we can see
a positive tendency in the residuals. Apart from that,
the diagnostic plots look excellent.

### Adding another predictor to the model

Now, we add another predictor to the model. So we use $X_1$ and $X_2$ 
**simultaneously** to predict $Y$. We are now in the lucky situation that
we can still visualize the situation in 3D. The regression line from simple
linear regression
becomes a [plane](https://stackoverflow.com/questions/47344850/scatterplot3d-regression-plane-with-residuals).
The vertical distances between the data points and the plane are the residuals.
Minimizing the sum of the squared residuals gives again 
the estimates for the coefficients. 

For demonstration purposes, we can create data ourselves with known
coefficients.
This is the true model, which we usually do not know:

$$ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \varepsilon_i$$
$$ Y_i = 10 + 0.5 X_{1i} + 1 X_{2i} + \varepsilon_i$$
$$ \mathbb{E}(Y_i) = 10 + 0.5 X_1 + 1 X_2$$
$$ i = 1 \ldots n$$
$$ \varepsilon_i \sim N(0, 5)$$

According to the model, the expected value of $Y$ is a linear function of $X_1$ and $X_2$.

- If $X_1$ increases by one unit, $Y$ increases by $0.5$ unit on average (in expectation).
- If $X_2$ increases by one unit, $Y$ increases by $1$ units on average (in expectation).
- If $X_1$ and $X_2$ are zero, $Y$ is $10$ on average (in expectation).

Why in expectation? Because there is still the error term which makes the whole thing random!
We can see that an increase in $X_1$ does not influence the relationship between $X_2$ and $Y$.
Hence, there is no interaction between $X_1$ and $X_2$ with respect to $Y$.

Now lets's draw 100 points from this model, fit the model and add the plane:

```{r}
library(plotly)

#set.seed(123)
n <- 100
X1 <- rnorm(n, 0, 5)
X2 <- rnorm(n, 0, 5)
Y <- 10 + 0.5 * X1 + 1 * X2 + rnorm(n, 0, 2)
d <- data.frame(X1 = X1, X2 = X2, Y = Y)

# Fit the model
m4.3 <- lm(Y ~ X1 + X2, data = d)
summary(m4.3)

# Create a grid for the plane
X1_grid <- seq(min(d$X1), max(d$X1), length.out = 20)
X2_grid <- seq(min(d$X2), max(d$X2), length.out = 20)
grid <- expand.grid(X1 = X1_grid, X2 = X2_grid)

# Predict the values for the grid
grid$Y <- predict(m4.3, newdata = grid)

# Convert the grid into a matrix for the plane
plane_matrix <- matrix(grid$Y, nrow = length(X1_grid), ncol = length(X2_grid))

# Create the interactive 3D plot
plot_ly() %>%
  add_markers(
    x = d$X2, y = d$X1, z = d$Y,
    marker = list(color = "blue", size = 5),
    name = "Data Points"
  ) %>%
  add_surface(
    x = X1_grid, y = X2_grid, z = plane_matrix,
    colorscale = list(c(0, 1), c("red", "pink")),
    showscale = FALSE,
    opacity = 0.7,
    name = "Fitted Plane"
  ) %>%
  plotly::layout(
    scene = list(
      xaxis = list(title = "X1"),
      yaxis = list(title = "X2"),
      zaxis = list(title = "Y")
    ),
    title = "Interactive 3D Scatterplot with Fitted Plane"
  )
```

This is, of course, a very idealized situation. There is no curvature in the plane,
no interaction, not outliers, no homoscadasticity. It's the simplest case of multiple regression
with 2 predictors. Reality is - usually - more complicated.

Let's look at summary output and check model assumptions:

```{r}
summary(m4.3)
check_model(m4.3)
```

We could repeat this simulation to get a feeling for the variability.
The posterior predictive checks look nice. In this case, we *know* that the model is true
and with this knowledge we can assess the diagnostic plots in front of us.

### Interaction term $X_1 \times X_2$

I recommend reading the excellent explanations about interactions
in John Kruschke's book [Doing Bayesian Data Analysis](https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf),
15.2.2 und 15.2.3.





## Exercises

### [M] Exercise 1 {#exercise1_multiple_regression}

- Fit a model with a cubic term for weight and height of the !Kung San people.
- Add the prediction bands as seen in the book.
- Come up with an explanation for the functional form of this relationship.
- Could there be reasons to for taking a less complicated model 
  ([1](https://en.wikipedia.org/wiki/Statistical_model_specification), [2](https://en.wikipedia.org/wiki/Occam%27s_razor))?

### [E] Exercise 2 {#exercise2_multiple_regression}

Consider the model equations from [above](#adding_transformed_predictor) 
where we used polynomial regression to model the relationship between 
weight and height:

- Draw the model hierarchy for the model.


## TODOS

- If you add a lot of variables to your regression model, you can get an arbitrarily large ($\le 1$)
  $R^2$. We will verify this when we have more than 2 explanatory variables. 
- maybe create animation of points wiggling to get a feeling for the variability
- Show by simulation what Gelman talks about with significant p values. So I scan the data
  for significant p values and then simulate data with the same effect size and see how often
  I get significant p values. Especially the next effect would be probably smaller,
  especially, if one did p-hacking! Calculate a priori probability for replication (def?).
- For multiple regression:
  We could also look at the simple regression problem as fitting a plane to the data,
  as is done [here](https://rpubs.com/pjozefek/576206) or 
  [here](https://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization)
  at the end. As
- Variability of confidence interval borders
- Next Chapter: Multiple regression
- Chapter: Sample size calculations for logistic and multivariate regression, Proportions, ICCs, t.test
- Chapter about ICCs, but maybe reduced
