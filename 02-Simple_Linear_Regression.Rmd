# Simple Linear Regression

## Simple Linear Regression in the Bayesian Framework

We will now add one covariate/explanatory variable to the model. 
Refer to [Statistical Rethinking](https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf) 
"4.4 Linear prediction" or "4.4 Adding a predictor" as it's called in the online version of the book.

So far, our "regression" did not do much to be honest. The mean of a list of values
was already calculated in the [descriptive statistics section](https://jdegenfellner.github.io/Script_QM1_ZHAW/descriptive_stats.html) 
before and we have mentioned how great this statistic is as measure of location and where its weaknesses are.

Now, we want to model **how** body height and weight are **related**.
Formally, one wants to *predict* body heights from body weights.

Here and in the frequentist framework, we will see that it is **not the same**
problem (and therefore results in a different statistical model) 
**to predict body weights from body heights or vice versa**.

The word "predictor" is important here. It is a technical term
and describes a variable that we know (in our case weight) and with 
which we want to "guess as good as possible" the value of the 
dependent variable (in our case height). "As good as possible"
means that we put a penalty on an error. The farer our prediction
is aways from the true value ($y_i$), the higher the penalty.
And not only that, but if you are twice as far away from the true value,
you should be penalized four times as much. This is the idea behind
the squared error [loss function](https://en.wikipedia.org/wiki/Loss_function) 
and the core of the least squares method.
What if we would punish differently you ask?
There are many loss functions one could use, maybe we will see some
later. For now, we punish quadratically.

We **always** visualize the data first to improve our understanding.

```{r}
plot(d2$height ~ d2$weight)
```

It's not often, that you see such a clean plot.
The scatterplot indicates a linear relationship between the two variables.
The higher the weight, the higher the height; with some deviations of course and
we decide that normally distributed errors are a good idea.
This relationsip is neither causal, not deterministic.

- It is not causal since an increase in weight does not 
  necessarily lead to an increase in height, especially in grown-ups.
- It is not deterministic since there are deviations from the line. 
  It if was deterministic, we would not need statistical modeling. 

For simpler notation, we will call `d2$weight` $x$. $\bar{x}$
is the mean of $x$.

### Model definition

Let's write down our **model** (again with the Swiss population prior mean):

\begin{eqnarray*}
h_i &\sim& \text{Normal}(\mu_i, \sigma)\\
\mu_i &\sim& \alpha + \beta (x_i - \bar{x})\\
\alpha &\sim& \text{Normal}(171.1, 20)\\
\beta &\sim& \text{Normal}(0, 10)\\
\sigma &\sim& \text{Uniform}(0, 50)
\end{eqnarray*}

Visualization of the **model structure**:

```{r, echo=FALSE}
library(DiagrammeR)

grViz("
digraph model {
  graph [layout = dot, rankdir = TB]

  # Nodes for variables
  node [shape = ellipse, style = filled, fillcolor = lightblue]
  alpha [label = 'α ~ Normal(171.1, 20)']
  beta [label = 'β ~ Normal(0, 10)']
  sigma [label = 'σ ~ Uniform(0, 50)']
  mu [label = 'μ_i = α + β(x_i - x̄)']
  h_i [label = 'h[i] ~ Normal(μ_i, σ)']

  # Connections
  alpha -> mu
  beta -> mu
  mu -> h_i
  sigma -> h_i
}
")
```

There are now additional lines for the priors of $\alpha$ and $\beta$.
The model structure also shows the way to simulate from the prior.
One starts at the top and ends up with the heights.

- $h_i$ is the height of the $i$-th person and we assume it is normally distributed.
- $\mu_i$ is the mean of the height of the $i$-th person and we 
  assume it is linearly dependent on the difference $x_i-\bar{x}$. 
  Compared to the intercept model, a different mean is assumed for each person
  depending on his/her weight. 
- $\alpha$ is the intercept and we use the same prior as before.
- $\beta$ is the slope of the line and we use the normal distribution as prior for it,
  hence it can be positive or negative and how plausible each value is, is
  determined by that specific normal distribution. Note, that we could 
  easily adapt the distribtion to any distribution we like.
- The prior for $\sigma$ is unchanged.
- $x_i - \bar{x}$ is the deviation of the weight from the mean weight, thereby **we
  center** the weight variable. This is a common practice in regression analysis.

The linear model is quite popular in applied statistics and one
reason is probably the rather straightforward interpretation of the coefficients.

### Priors

We want to plot our priors to get a feeling **what 
the model would predict without seeing the data**.
This is a kind of "sanity check" to see if the priors are reasonable.
Again, we just draw from the assumed distributions for $\alpha$ and $\beta$
100 times and draw the corresponding lines. Just as the model 
definition says.

```{r}
set.seed(2971)
N <- 100  # 100 lines
a <- rnorm(N, 171.1, 20)
b <- rnorm(N, 0, 10)

# Assume d2$weight is defined, e.g., using some dataset or simulation
xbar <- mean(d2$weight)

plot(NULL, xlim = range(d2$weight), ylim = c(-100, 400),
     xlab = "weight", ylab = "height")
abline(h = 0, lty = 2)  # horizontal line at 0
abline(h = 272, lty = 1, lwd = 0.5)  # horizontal line at 272
mtext("b ~ dnorm(0, 10)")

# Overlay the 100 lines
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
        from = min(d2$weight), to = max(d2$weight),
        add = TRUE, col = col.alpha("black", 0.2))
}
```

This linear relationship defined with the chosen priors seems rather 
non-restrictive. According to our priors,
one could see very steeply rising or falling lines. We could at least make 
the priors for the slope ($\beta$) non-negative. One possibility to do this 
is to use a [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution) 
for the prior of $\beta$ which can only take non-negative values.

$$ \beta \sim \text{Log-Normal}(0, 1) $$

Lets plot the priors again.

```{r}
set.seed(2971)
N <- 100  # 100 lines
a <- rnorm(N, 171.1, 20)
b <- rlnorm(N, 0, 1)

# Assume d2$weight is defined, e.g., using some dataset or simulation
xbar <- mean(d2$weight)

plot(NULL, xlim = range(d2$weight), ylim = c(-100, 400),
     xlab = "weight", ylab = "height")
abline(h = 0, lty = 2)  # horizontal line at 0
abline(h = 272, lty = 1, lwd = 0.5)  # horizontal line at 272
mtext("b ~ dlnorm(0, 1)")

# Overlay the 100 lines
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
        from = min(d2$weight), to = max(d2$weight),
        add = TRUE, col = col.alpha("black", 0.2))
}
```

This seems definitely more realistic.

### Fit model

Now, let's **estimate the posterior/fit the model** as before:

```{r}
# load data again, since it's a long way back
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]
xbar <- mean(d2$weight)
# fit model
mod <- quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu <- a + b * (weight - xbar),
        a ~ dnorm(171.1, 100),
        b ~ dnorm(0, 10),
        sigma ~ dunif(0, 50)
    ) ,
data = d2)
```

Let's look at the **marginal distributions** of the parameters:

```{r}
precis(mod)
```

The analysis yields estimates for all our parameters of the model: $\alpha$,
$\beta$ and $\sigma$. The estimates are the mean of the posterior distribution.

See [exercise 2](#exercise2_simpl_lin_reg).

**Interpretation of $\beta$**:
The mean of the posterior distribution of $\beta$ is 0.9. A person with a weight
of 1 kg more weight can be expected to be 0.9 cm taller. A 89% credible interval
for this estimate is $[0.83, 0.97]$. We can be quite sure that the slope is 
positive (of course we designed it that way too via the prior). 

It might also be interesting to inspect the variance-covariance matrix, 
respectively the correlation between the parameters as we did before
in the intercept model.

```{r}
diag(vcov(mod))
round(cov2cor(vcov(mod)),2)
```

As we can see the correlations are (near) zero. Compare to the graphical
display of the model structure. There is no connection.

### Result

**Graphical end result** of fitting the model:

```{r}
plot(d2$height ~ d2$weight, col = rangi2)
post <- extract.samples(mod)
a_quap <- mean(post$a)
b_quap <- mean(post$b)
curve(a_quap + b_quap * (x - xbar), add = TRUE)
```

### Credible bands

We could draw again and again from the posterior distribution
and calculate the means like above. Plotting the regression lines
with the respective parameters $\alpha$, $\beta$ would indicate
the variability of the estimates.

```{r}
# Define a sequence of weights for predictions
weight.seq <- seq(from = 25, to = 70, by = 1)

# Use the model to compute mu for each weight
mu <- link(mod, data = data.frame(weight = weight.seq))
str(mu)

# Visualize the distribution of mu values
plot(height ~ weight, d2, type = "n")  # Hide raw data with type = "n"

# Loop over samples and plot each mu value
for (i in 1:100) {
  points(weight.seq, mu[i, ], pch = 16, col = col.alpha(rangi2, 0.1))
}
```

The `link` function fixes the weight at the values in `weight.seq` and
draws samples from the posterior distribution of the parameters. We will do 
the analog thing in the frequentist framework.

We can also draw a nice shade for the regression line:

```{r}
# Summarize the distribution of mu
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))
lines(weight.seq, mu.mean)
shade(mu.PI, weight.seq)
```

As we can see, we are pretty sure about the mean of height which 
we wanted to model in the first place.
Mean modeling is one thing, individual prediction is another.
Given a certain weight of a person, what is the height of the same person?
The first line in the model definition ($height_i \sim Normal(\mu_i, \sigma)$)
tells us that a person's weight is distributed *around* the mean
(which linearly depends on weight) and is not necessary the mean itself.

To get to an **individual prediction**, we need to consider the uncertainty 
of the parameter estimation *and* the uncertainty from the Gaussian distribution
around the mean (at a certain weight). We do this with `sim`.

```{r}
# Simulate heights from the posterior
sim.height <- sim(mod, data = list(weight = weight.seq))
str(sim.height)

# Compute the 89% prediction interval for simulated heights
height.PI <- apply(sim.height, 2, PI, prob = 0.89)

# Plot the raw data
plot(height ~ weight, d2, col = col.alpha(rangi2, 0.5))

# Draw MAP (mean a posteriori) line
lines(weight.seq, mu.mean)

# Draw HPDI (highest posterior density interval) region for mu
shade(mu.PI, weight.seq)

# Draw PI (prediction interval) region for simulated heights
shade(height.PI, weight.seq)
```

The lighter and wider shaded region is where the model expects to find 89%
of the heights of a person with a certain weight. 

This part is **sometimes a bit desillusioning** when seen for the first time:
Draw a horizontal line at 150 cm and see how many weights (according to 
the individual prediction) are compatible with this height. Weights 
from 30 to 50 kg are compatible with this height according to the 
89% prediction interval. The higher the credibility, the wider the interval,
the wider the range of compatible weights (more than 60% of the weight-range).

```{r}
(50 - 30) / (range(d2$weight)[2] - range(d2$weight)[1])
```

On the other hand: We did not model the relationship this ways.
We modeled height depending on weight and not the other way around. 
In the next chapter, we will regress weight on height (yes, this is the correct order)
and see what changes.

### Summary

- We have added a covariate (weight) to the simple mean model to predict height.
- We have centered the weight variable.
- We have defined and refined priors for the intercept and slope.
- We have estimated the posterior distribution of the parameters using quadratic approximation with `quap`.
- We have visualized the result.
- We have created credible bands for mean and individual predictions.


## Simple Linear Regression in the Frequentist Framework

We will now do the same analysis in the frequentist framework while introducing
some foundational theory along the way.
I recommend reading the first couple of chapters from [Westfall](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4).

### Model definition

Our linear model is defined as:

$$ h_i = \beta_0 + \beta_1 x_i + \varepsilon_i $$

where

- $\varepsilon_i$ is the error term with $\varepsilon_i \sim N(0, \sigma), \forall i$
- $\beta_0$ is the unknown but fixed intercept
- $\beta_1$ is the unknown but fixed slope

#### Model Assumptions of the Classical Regression Model ([Westfall](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4), 1.7): {#_model_assumptions}

The first and **most important assumption** is that the data are produced  
probabilistically, which is specifically stated as
$$ Y|X = x \sim p(y|x)$$

What does this mean? 

- $Y|X = x$ is the random variable Y **conditional** on X being equal to x, i.e. the 
   distribution of $Y$ if we know the value of $X$ (in our example the weight in kg).
   [This](https://blogs.sas.com/content/iml/files/2015/09/GLM_normal_identity.png) is a nice image of what is meant here.
- $p(y|x)$ is the distribution of potentially observable $Y$ given $X = x$. 
  In our case above this was the normal distribution with mean $\mu_i$ and variance $\sigma$.

You can play with [this shiny app](https://psychmeth.shinyapps.io/Regression-NVFehler/) to improve your understanding.
It offers the option "Bedingte Verteilung anzeigen".

One always thinks about the so-called
[data generating process](https://en.wikipedia.org/wiki/Data_generating_process) 
([Westfall](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4), 1.2).
How did the data come about? There is a process behind it and this process
is attempted to be modeled.

**Further assumptions**:

- Correct functional specification: The conditional mean function $f(x) = \mathbb{E}(Y|X=x)$.
  In the case of the linear model, the assumption is $\mathbb{E}(Y|X=x) = \alpha + \beta x$. 
  The **expectation** of $Y$ (height) depends linearly on $x$ (weight).
  This assumption is violated when the true relationship is not linear or the
  data at least suggest that it is not linear, like [here](https://www.alexanderdemos.org/Class5_files/figure-html/unnamed-chunk-2-1.png).

- The errors are homoscedastic (constant variance $\sigma$). This means the
  variances of all conditional distributions $p(y|x)$ are constant ($=\sigma^2$).
  This assumption is violated if points are [spreading out more and more
  around the regression line](https://www.investopedia.com/thmb/n9S9lWMv6X9-sKC2DtAOtUTPSik=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/Heteroskedasticity22-ce5acc2acef6494d91935588b0599579.png), 
  indicating that the errors are getting larger.

- Normality. For the classical linear regression model all the conditional
  distributions $p(y|x)$ are normal distributions. It could well be, that
  the errors are not nicely normally distributed around the regression line,
  for instance if we have a lot of outliers upwards and the distribution 
  is skewed, like [here](https://www.bookdown.org/rwnahhas/RMPH/mlr-normality.html).

- The errors are independent of each other. 
  The potentially observable $\varepsilon_i = Y_i - f(\mathbf{x_i}, \mathbf{\beta})$
  is uncorrelated with $\varepsilon_j = Y_j - f(\mathbf{x_j}, \mathbf{\beta})$ for 
  $i \neq j$. This assumption is violated if the errors are correlated,
  here is an example: The true data comes from a sine curve and we estimate a 
  linear model (green), which does not fit the data well (left plot).
  The residuals plot shows clear patterns (right plot) and indicates
  that the errors are correlated. Specifically, the errors around $x=2$ and $x=4$ 
  are negatively correlated (see [exercise 6](#exercise6_simpl_lin_reg)).
```{r, echo=FALSE, warning=FALSE}

# Set seed for reproducibility
set.seed(123)

# Generate data from a sine function
x <- seq(0, 2 * pi, length.out = 100)       # Predictor variable
true_y <- sin(x)                            # True values based on sine function

# Add noise to simulate observed data
noise <- rnorm(length(x), mean = 0, sd = 0.2)
observed_y <- true_y + noise                # Observed response variable

# Perform a simple linear regression with intercept
linear_model <- lm(observed_y ~ x)

# Display summary of the model
#summary(linear_model)

# Load necessary libraries
library(ggplot2)
library(patchwork)

# Create the first plot: Observed data and linear regression fit
plot1 <- ggplot(data = data.frame(x, observed_y, true_y), aes(x = x)) +
  geom_point(aes(y = observed_y), color = "blue", size = 2) +
  geom_line(aes(y = true_y), color = "red",
            linetype = "dashed", linewidth = 1) +
  geom_abline(intercept = coef(linear_model)[1], slope = coef(linear_model)[2],
              color = "green", linewidth = 1) +
  labs(title = "Sine Data with Linear Regression", x = "x", y = "Observed y") +
  theme_minimal()

# Create the second plot: Residuals
plot2 <- ggplot(data = data.frame(x, residuals = residuals(linear_model)),
                aes(x = x, y = residuals)) +
  geom_point(color = "blue", size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals vs. x", x = "x", y = "Residuals") +
  theme_minimal()

# Arrange the plots side by side using patchwork
plot1 + plot2
```

In the case above, the errors are **not conditionally independent**.
If we condition on $X=2$ and $X=4.5$, the errors are correlated ($r \sim -0.3$),
which they should not be.

These assumptions become clearer as we go along and should be checked 
for every model we fit. They are not connected, they can all be true or false.
The question is not "Are the assumptions met?" since they never are exactly met.
The question is **how** "badly" the assumptions are violated?

Remember, **all models are wrong, but some are useful**.

In full, the classical linear regression model can be written as:

$$ Y_i|X_i = x_i \sim_{independent} N(\beta_0 + \beta_1 x_{i1} + \dots \beta_k x_{ik},\sigma^2)$$
for $i = 1, \dots, n$.

### Fit the model {#fit_model_simple_lin_reg_classic}

Again, we fit the model using the least squares method. For a neat animated explanation, 
visit [this video](https://www.youtube.com/watch?v=jEEJNz0RK4Q&ab_channel=COCCmath).
There are literally hundreds of videos on the topic. Choose wisely. Not all are good.
If in doubt, use our recommended books as reading materials. This is the most reliable source.
A hint along the way: Be very sceptical if you ask GPT about information, 
although for this special case one has a good chance of getting a good answer due to 
the vast amount of training data. 

One has to minimize the sum of squared differences between the true heights and 
the model-predicted heights in order to find $\beta_0$ and $\beta_1$.

$$ SSE(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1 x_i))^2 $$

We omit the technical details (set derivative to zero and solve the system) and give the results for $\beta_0$ and $\beta_1$:

\[
\hat{\beta_0} = \bar{y} - (\hat{\beta_1} \bar{x}),
\]
\[
\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} =
 \frac{s_{x,y}}{s_x^2} = r_{xy} \frac{s_y}{s_x}.
\]

where:

- \(r_{xy}\) is the [sample correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between \(x\) and \(y\)
- \(s_x\) and \(s_y\) are the [uncorrected sample standard deviations](https://en.wikipedia.org/wiki/Standard_deviation) of \(x\) and \(y\)
- \(s_x^2\) and \(s_{xy}\) are the [sample variance](https://en.wikipedia.org/wiki/Variance) and [sample covariance](https://en.wikipedia.org/wiki/Covariance), respectively

Interpretation of $\hat{\beta}_0$ and $\hat{\beta}_1$: see [exercise 3](#exercise3_simpl_lin_reg).

Let's **use R again to solve the problem**:

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]
mod <- lm(height ~ weight, data = d2)
summary(mod)
```

**Interpretation of R-output**:

- `Call`: The model that was fitted.
- `Residuals`: $r_i = height_i - \widehat{height}_i$. 
   Differences between true heights and model-predicted heights.
- `Coefficients`: Estimated for $\beta_0$ and $\beta_1$. We call them $\hat{\beta}_0$ and $\hat{\beta}_1$.
   - `Estimate`: The (least squares) estimated value of the coefficient.
   - `Std. Error`: The standard error of the estimate.
   - `t value`: The value of the $t$-statistic for the (Wald-) hypothesis test
        $H_0: \beta_i = 0$.
   - `Pr(>|t|)`: The $p$-value of the hypothesis test. 
- `Residual standard error`: The estimate of $\sigma$ 
   which is also a model parameter (as in the Bayesian framework).
- `Multiple R-squared`: The proportion of the variance explained by the 
   model (we will explain this below).
- `Adjusted R-squared`: A corrected version of the $R^2$ which takes into account
   the number of predictors in the model.
- `F-statistic`: The value of the $F$-statistic for the hypothesis test: 
    $H_0: \beta_1 = \beta_2 = \dots = \beta_k = 0$. Note, the alternative 
    hypotheses to this test is that *any* of the $\beta_i$ is not zero. If that is
    the case, the model explains more than the mean model with just $\beta_0$.

We could also **solve the least squares problem graphically**: We want to find the 
values of $\beta_0$ and $\beta_1$ that minimize the sum of squared differences
which can be plotted as 3D function. All we have to do is to ask R which of 
the coordinates minimizes the sum of squared errors. The result confirmes
the results from the `lm` function.The dot in red marks the spot (Code is in the git repository):

**---COMPILE CODE AT DEOPLOYMENT---**

```{r, echo=FALSE, eval=FALSE}
library(pacman)
p_load(plotly, tidyverse, rethinking)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

d2$x <- d2$weight
d2$y <- d2$height

mod <- lm(y ~ x, data = d2)
#summary(mod)
cat("Intercept beta_0: ", coef(mod)[1], "\n")
cat("Slope beta_1: ", coef(mod)[2], "\n")

sse <- function(beta_0, beta_1, data) {
  predicted <- beta_0 + beta_1 * data$x
  sum((data$y - predicted)^2)
}

beta_0_values <- seq(100, 126, length.out = 1000) # adapt to your data
beta_1_values <- seq(-1, 3, length.out = 1000) # adapt to your data
sse_values <- outer(beta_0_values, beta_1_values, Vectorize(function(a, b) sse(a, b, d2)))
fig <- plot_ly(x = ~beta_0_values, y = ~beta_1_values, z = ~sse_values, type = "surface") %>%
  layout(title = "Sum of squared errors depending on beta_0 and beta_1")
#fig

# Highlight min of this paraboloid (fig)----
min_sse_index <- which(sse_values == min(sse_values), arr.ind = TRUE) # Find indices where error is smallest.
min_beta_0 <- beta_0_values[min_sse_index[1]]
min_beta_1 <- beta_1_values[min_sse_index[2]]
min_sse <- min(sse_values)

# check
#min_beta_0 # (= Intercept)
#min_beta_1
#coef(mod) # match

fig <- fig %>% add_trace(
  x = min_beta_0, 
  y = min_beta_1, 
  z = min_sse, 
  type = 'scatter3d', 
  mode = 'markers',
  marker = list(color = 'red', size = 7)
)

fig

```


### ANOVA (Analysis of Variance) {#analysis_of_variance}

A non-obvious and very useful finding is that the total variability in the data (our heights) can be
**decomposed** (or [analysed](https://en.wiktionary.org/wiki/analysis)) 
into two parts:

- The variability explained by the model (the regression line)
- The variability not explained by the model (the residuals)

$$ \text{Sum of Squares in Total} = \text{Sum of Squares from Regression} + \text{Sum of Squared Errors} $$

$$ SST = SSR + SSE $$

$$ \sum_{i=1}^{n} (y_i - \bar{y})^2 = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 + \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

If you are interested in the details, check out [this](https://en.wikipedia.org/wiki/Explained_sum_of_squares#Partitioning_in_the_general_ordinary_least_squares_model).

[This video](https://www.youtube.com/watch?v=NxRTs7sXKAQ&ab_channel=365DataScience) 
explains the concept nicely.

Let's visualize our regression result:

```{r, echo=FALSE}
library(rethinking)
library(tidyverse)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

# Fit linear model
mod <- lm(height ~ weight, data = d2)

# Calculate predicted heights for the regression line
d2 <- d2 %>%
  mutate(predicted_height = predict(mod, newdata = d2),
         mean_height = mean(height))

# Select one point to highlight the differences
highlight_point <- d2[10, ]

# Create the plot
ggplot(d2, aes(x = weight, y = height)) +
  # Scatterplot of all points
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue", linewidth = 1) +
  geom_point(data = highlight_point, aes(x = weight, y = height),
            color = "red", size = 3) +
  
  # Vertical difference: Point and mean height (offset slightly left)
  geom_segment(data = highlight_point,
               aes(x = weight - 0.5, xend = weight - 0.5,
                   y = height, yend = mean_height, color = "Point-to-Mean"),
               linetype = "dashed") +
  
  # Vertical difference: Point and regression line (no offset)
  geom_segment(data = highlight_point,
               aes(x = weight, xend = weight,
                   y = height, yend = predicted_height,
                   color = "Point-to-Line"),
               linetype = "dashed") +
  
  # Vertical difference: Regression line and mean height (offset slightly right)
  geom_segment(data = highlight_point,
               aes(x = weight + 0.5, xend = weight + 0.5,
                   y = predicted_height,
                   yend = mean_height, color = "Line-to-Mean"),
               linetype = "dashed") +
  
  # Add horizontal line for the mean height
  geom_hline(yintercept = mean(d2$height), 
             color = "darkgreen", linetype = "dashed") +
  # Labels
  labs(title = "Scatterplot with Regression Line and Differences",
       x = "Weight",
       y = "Height",
       color = "Distance Type") +
  # Style
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5,
                              size = 16),  # Center and adjust title size
    legend.position = "top"  # Place legend at the top
  )
```

The plot is rather self-explanatory. The blue dotted line is the distance from
the mean to the point (total variance), the red dotted line is the distance from
the mean to the regression line (explained variance) and the green dotted line
is the distance from the regression line to the point (unexplained variance).
One sees that it adds up. I find this fact quite fascinating.
One finds additivity by considering not determinisic values, but variances.
Thank you [Ronald Fisher](https://en.wikipedia.org/wiki/Analysis_of_variance#:~:text=ANOVA%20was%20developed%20by%20the,to%20different%20sources%20of%20variation.).

### $R^2$ - Coefficient of Determination
[$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination)
is the **amount of variance explained by the model**. 
You can also read [Westfall](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4) 8.1.

As you can see 
[above](#analysis_of_variance), the total variance (SST) of our outcome (height) 
can be decomposed into two parts: the variance explained by the model (SSR)
and the variance not explained by the model (SSE).

Maybe the most intuitive definition of $R^2$ is:

$$ R^2 = \frac{SSR}{SST} = \frac{SST - SSE}{SST} = 1 - \frac{SSE}{SST}$$

The value is between 0 and 1. The higher the value, the more variance is explained.
But be cautious. Depending on the context, a really high $R^2$ is not 
necessarily a good thing. With the data we are working with,
it could easily hint towards an error. If we are near 1,
all points in the simple linear regression model are on the line.
If we are near 0, the model does not explain much of the variance 
and we would see "noise with no slope" in the scatterplot ([exercise 4](#exercise4_simpl_lin_reg)).
The normal $R^2$ can be found in the R output under `Multiple R-squared`.

If you add a lot of variables to your regression model, you can get an arbitrarily large ($\le 1$)
$R^2$. We will verify this when we have more than 2 explanatory variables. 
As a non-formal explanation for this: In the Sum of Squares Errors (SSE),
if you add more covariates ($\beta_2, \beta_3$), you have more freedom
to choose values that minimize the number that will be squared. Simple regression
is just a special case of multiple (more than one predictor) regression with $\beta_2=\beta_3=\dots=0$.
Hence, you will definitely not be worse off with regards to SSE when using more covariates.
A smaller SSE implies a larger SSR (sum constraint) and hence a larger $R^2$. SST remains constant.

Although not perfect, one way to mitigate the influence of "too many" variables
on $R^2$ is to use the adjusted $R^2$, which an also be found in the R output (`Adjusted R-squared`).

#### Seperating property of regression due to $R^2$:
[Peter Westfall](https://www.routledge.com/Understanding-Regression-Analysis-A-Conditional-Distribution-Approach/Westfall-Arias/p/book/9780367493516?srsltid=AfmBOore3O_Ciecl0TTkr9AjPIY1d6OmbQa7o7IAdKpTSkD8s9HkwzD4)
explains (in Figure 8.1 of the book) how $R^2$ influences the separation of distributions in our simple regression model.

In our regression of height on weight (order is correct, that's how you say it), 
the $R^2$ is $0.5696$. The following plot shows how well one can predict
height if we use the 10% and 90% quantile of the weights (x_low and x_high).
In both, you see the conditional distribution of height given the weight $X = x_{low}$ or $X = x_{high}$.
Scenario 1 is the original model, scenario 2 is the same data with added noise (in Y-direction),
which reduces $R^2$ to $0.13$, much lower. In the right plot, the distributions have a large
overlap and it is hard to distinguish between weights when seeing the height.
With a very low $R^2$, the height prediction does not really change and we could just as well
use the mean model.

```{r, echo=FALSE}
library(pacman)
p_load(plotly, tidyverse, rethinking, patchwork)
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

mod <- lm(height ~ weight, data = d2)
#summary(mod)

#plot(d2$weight, d2$height)

# lets use the 10% and the 90% quantile 
# of height

x_low_high <- quantile(d2$weight, probs = c(0.1, 0.9))

# Scenario 1
# original
#coef(mod)
# Conditional distribution of height is: N(113.8793936  + 0.9050291 *weight, 5.086)

# Draw to distributions for weights = x_low_high into one diagram
# 1) N(113.8793936  + 0.9050291 *x_low_high[1], 5.086)
# 2) N(113.8793936  + 0.9050291 *x_low_high[2], 5.086)

# Parameters for the two normal distributions
mean1 <- 113.8793936 + 0.9050291 * x_low_high[1]
mean2 <- 113.8793936 + 0.9050291 * x_low_high[2]
sd <- 5.086  # Standard deviation for both distributions

# Create a data frame for plotting
x_seq <- seq(mean1 - 4 * sd, mean2 + 4 * sd, length.out = 1000)
data <- data.frame(
  x = rep(x_seq, 2),
  density = c(dnorm(x_seq, mean = mean1, sd = sd),
              dnorm(x_seq, mean = mean2, sd = sd)),
  group = rep(c(paste0("x_low = ", x_low_high[1]),
                paste0("x_high = ", x_low_high[2])), each = length(x_seq))
)

# Scenario 2
# more variability to get a worse R^2
set.seed(123)
d2$height_added_noise <- d2$height + rnorm(nrow(d2), mean = 0, sd = 15)

mod_noise <- lm(height_added_noise ~ weight, data = d2)
#summary(mod_noise)

# Extract the new coefficients
coef_noise <- coef(mod_noise)

# Conditional distribution of height with noise
# N(coef_noise[1] + coef_noise[2] * weight, Residual SD)
residual_sd <- sigma(mod_noise)

# Parameters for the two normal distributions
mean1_noise <- coef_noise[1] + coef_noise[2] * x_low_high[1]
mean2_noise <- coef_noise[1] + coef_noise[2] * x_low_high[2]

# Create a data frame for plotting (with added noise)
x_seq_noise <- seq(mean1_noise - 4 * residual_sd, mean2_noise + 4 * residual_sd, length.out = 1000)
data_noise <- data.frame(
  x = rep(x_seq_noise, 2),
  density = c(dnorm(x_seq_noise, mean = mean1_noise, sd = residual_sd),
              dnorm(x_seq_noise, mean = mean2_noise, sd = residual_sd)),
  group = rep(c(paste0("x_low = ", x_low_high[1]),
                paste0("x_high = ", x_low_high[2])), each = length(x_seq_noise))
)

# Plot for Scenario 1
plot1 <- ggplot(data, aes(x = x, y = density, color = group)) +
  geom_line(linewidth = 1.2) +
  labs(
    title = "Scenario 1",
    x = "Height",
    y = "Density",
    color = "Weight"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    legend.position = "bottom"
  )

# Plot for Scenario 2
plot2 <- ggplot(data_noise, aes(x = x, y = density, color = group)) +
  geom_line(linewidth = 1.2) +
  labs(
    title = "Scenario 2",
    x = "Height",
    y = "Density",
    color = "Weight"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red")) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    legend.position = "bottom"
  )

# Combine the two plots side by side with a shared legend
combined_plot <- plot1 + plot2 +
  plot_layout(guides = "collect") &  # Collect the legend
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, size = 16)
  )

# Add a single title to the combined plot
combined_plot <- combined_plot +
  plot_annotation(
    title = "Normal Distributions for Two Scenarios",
    theme = theme(plot.title = element_text(size = 18, hjust = 0.5))
  )

# Display the combined plot
combined_plot
```

In the left plot, given $X = x_{low}$ gives as a rather shifted normal distribution of
potentially observable heights for this weight compared to $X=x_{high}$.
We would have a lower missclassification error when trying to distinguish weights
of very light and very heavy people in the sample just by seeing the height. 
You can think of an even more extreme separation of these distributions,
which would happen, when $R^2$ is very high or the true slope is much higher.
Then it would be trivial to distinguish between the two groups (light versus heavy people).
We could then just introduce a cutoff value and classify people accordingly.
Every person above the cutoff value for height (maybe 160 cm) would be classified 
as "heavy" (top 10% weight) and every person below as "light" (bottom 10% weight).


See also [exercise 5](#exercise5_simpl_lin_reg).

### Check regression assumptions
..........



....... **TODOS**

- random X vs fixed X
- If you add a lot of variables to your regression model, you can get an arbitrarily large ($\le 1$)
$R^2$. We will verify this when we have more than 2 explanatory variables. 
- Regress weight on height and study changes
- Regression towards the mean
- One could think of the least squares method of minimizing the sum of 
  Work that has to be done in order to adjust a long stick through the points.
  Every point is attached to the stick with a srping. the work needed to change the
  spring by $\delta x$ is $k \delta x^2$. The stick is adjusted in such a way that
  the sum of the work is minimized. This is the least squares method.


## Exercises

### [E] Exercise 1 {#exercise1_simpl_lin_reg}

In the model from above:

\begin{eqnarray*}
h_i &\sim& \text{Normal}(\mu_i, \sigma)\\
\mu_i &\sim& \alpha + \beta (x_i - \bar{x})\\
\alpha &\sim& \text{Normal}(171.1, 20)\\
\beta &\sim& \text{Normal}(0, 10)\\
\sigma &\sim& \text{Uniform}(0, 50)
\end{eqnarray*}

- What ist the expected height when $x_i = \bar{x}$?
- What is the expected height when $x_i$ changes by 1 unit?

### [E] Exercise 2 {#exercise2_simpl_lin_reg}

Look at the marginal distrubutions of the parameters in the Bayesian model.

- Plot the posterior distribution of all 3 parameters.
- Include in the plot a 99% credible interval (HDI).

### [M] Exercise 3 {#exercise3_simpl_lin_reg}

Go to the coefficient estimates in the simple linear regression setting 
above ([Fit the model](#fit_model_simple_lin_reg_classic))
in the classical framework.

- Create an R file to simulate the simple linear regression model.
- Change your input parameters and see how the estimates change.
- Does this make sense with respect to the estimates given, specifically with respect
to $\beta_1$?

### [M] Exercise 4 {#exercise4_simpl_lin_reg}

Verify the statement above in the text for high and low values of $R^2$.

### [M] Exercise 5 {#exercise5_simpl_lin_reg}

Verify with simulation in R that the separation of the distributions in the simple linear regression model
improves if the true (but usually unknown) slope increases.

### [H] #Exercise 6 {#exercise6_simpl_lin_reg}

Go to the model assumptions in the classical regression model ([Model Assumptions](#_model_assumptions)).
- Use the code from [github](https://github.com/jdegenfellner/Script_QM2_ZHAW/blob/main/02-Simple_Linear_Regression.Rmd) 
  to recreate the regression model with the sine-curve.
- Check the independence assumption as described. Look at the residuals, when $X=2$ and $X=4.5$. 
  You can get those by filtering residuals that are $>0.5$ and $<0.5$.