# Reliability and Validity

For this chapter we refer to the book 
[Measurement in Medicine](https://www.cambridge.org/core/books/measurement-in-medicine/8BD913A1DA0ECCBA951AC4C1F719BCC5).

I invite you to read the introductory chapters 1 and 2 about concepts,
theories and models, and types of measurement.

In general, when conducting a measurement of any sort 
(laboratory measurements, scores from questionnaires, etc.), 
we want to be reasonably sure

- that we actually **measure what we intend to measure**; 
  ([validity](https://en.wikipedia.org/wiki/Validity_(statistics)); 
  chapter 6 in the book);
- that the measurement does **not change too much** if the 
  underlying **conditions are the same**
  ([reliability](https://en.wikipedia.org/wiki/Reliability_(statistics)); 
  chapter 5 in the book); and
- that we are able to detect a **change** if the underlying conditions change 
  ([responsiveness](https://tinyurl.com/3vdcxy49); chapter 7 in the book); and
- that we understand the meaning of a change in the measurement
  (interpretability; chapter 8 in the book).

In this [video](https://www.youtube.com/watch?v=KuT2n1w0Ixc&ab_channel=Physiotutors), 
Kai jump starts you on reliability and validity.


## Reliability
You can watch this [video](https://www.youtube.com/watch?v=9HSoWaRpcys&ab_channel=Physiotutors) 
to get started.

Imagine, you measure a patient (pick your favorite measurement), for example,
the range of motion (ROM) of the shoulder. 

- If you are interested in how
  similar your measurements are in comparison to your colleagues, you are 
  trying to determine the so-called **inter-rater reliability**.
- If you are interested in how similar your measurements are when you measure
  the same patient twice, you are trying to determine the so-called 
  **intra-rater reliability**.

Assuming there is a true (but unknown) underlying value (of ROM), 
it is clear that measurements will not be *exactly* the same. 
Possible influences (potentially) causing different results are:

- the measurement instrument itself (e.g., the goniometer),
- the patient (e.g., mood/motivation),
- the examiner (e.g., mood, influence on patient),
- the environment (e.g., the room temperature).

Note that the **true score** is defined in our context as the average of all measurements
if we would measure repeat it an infinite number of times.

### Peter and Mary's ROM measurements

The data can be found [here](http://www.clinimetrics.nl/answers-to-the-assignments-in-textbook_22_0.html).
We randomly select 50 measurements from Peter and Mary in 50 different patients,
plot their measurements and annotate the absolutely largest one.

```{r}
library(pacman)
p_load(tidyverse, readxl)

# Read file
url <- "https://raw.githubusercontent.com/jdegenfellner/Script_QM2_ZHAW/main/data/chapter%205_assignment%201_2_wide.xls"
temp_file <- tempfile(fileext = ".xls")
download.file(url, temp_file, mode = "wb")  # mode="wb" is important for binary files
df <- read_excel(temp_file)

head(df)
dim(df)

# As in the book, let's randomly select 50 patients.
set.seed(123)
df <- df %>% sample_n(50)
dim(df)

# "as" = affected shoulder
# "nas" = not affected shoulder

df <- df %>%
  mutate(diff = abs(ROMnas.Peter - ROMnas.Mary))  # Compute absolute difference

max_diff_point <- df %>%
  dplyr::filter(diff == max(diff, na.rm = TRUE))  # Find the row with the max difference

df %>%
ggplot(aes(x = ROMnas.Peter, y = ROMnas.Mary)) +
  geom_point() + 
  geom_point(data = max_diff_point, aes(x = ROMnas.Peter, y = ROMnas.Mary), 
             color = "blue", size = 4) +  # Highlight max difference point
  geom_abline(intercept = 0, slope = 1, color = "red") +
  theme_minimal() +
  ggtitle("ROMnas.Peter vs. ROMnas.Mary") +
  theme(plot.title = element_text(hjust = 0.5)) +
  annotate("text", x = max_diff_point$ROMnas.Peter, 
           y = max_diff_point$ROMnas.Mary, 
           label = paste0("Max Diff: ", round(max_diff_point$diff, 2)), 
           vjust = -1, color = "blue", size = 4)

# average abs. difference:
mean(df$diff, na.rm = TRUE) # 7.2

cor(df$ROMnas.Peter, df$ROMnas.Mary, use = "complete.obs") 
```

The red line represents the line of equality ($y=x$). If the measurements are 
exactly the same,
all points would lie on this line. The blue point represents the largest 
difference in measured Range of Motion (ROM) values between Peter and Mary
from the randomly chosen 50 people. Note that the maximum difference in 
all 155 patients is 35 degrees.

The first simple measure of agreement we could use is the correlation, which
measures the strength and direction of a linear relationship between two variables.
But correlation does not exactly measure what we want. If there was a bias 
(e.g., Mary systematically measures 5 degrees more than Peter), correlation would not 
notice this. (-> exercise later...). It actually is too optimistic about 
the agreement since it only cares about the linearity and not about a potential bias.
$r=0.2403213$ which indicates a weak positive correlation. Higher values of Peter's
are associated with higher values of Mary's measurements.
But: Knowing Peter's measurement does not help us to *predict* Mary's measurement
at such a low correlation (-> exercise later).
So, on the *not affected shoulder* (nas), the agreement is really bad.

What about the affected shoulder (as)?

```{r}
library(ggExtra)
df <- df %>%
  mutate(diff = abs(ROMas.Peter - ROMas.Mary))  # Compute absolute difference

max_diff_point <- df %>%
  dplyr::filter(diff == max(diff, na.rm = TRUE))  # Find the row with the max difference

p <- df %>%
ggplot(aes(x = ROMas.Peter, y = ROMas.Mary)) +
  geom_point() + 
  geom_point(data = max_diff_point, aes(x = ROMas.Peter, y = ROMas.Mary), 
             color = "blue", size = 4) +  # Highlight max difference point
  geom_abline(intercept = 0, slope = 1, color = "red") +
  theme_minimal() +
  ggtitle("ROMas.Peter vs. ROMas.Mary") +
  theme(plot.title = element_text(hjust = 0.5)) +
  annotate("text", x = max_diff_point$ROMas.Peter, 
           y = max_diff_point$ROMas.Mary, 
           label = paste0("Max Diff: ", round(max_diff_point$diff, 2)), 
           vjust = -1, color = "blue", size = 4)
# Add marginal histograms
ggMarginal(p, type = "density", fill = "gray", color = "black")

# average abs. difference:
mean(df$diff, na.rm = TRUE) # 7.2
cor(df$ROMas.Peter, df$ROMas.Mary, use = "complete.obs") 

# mean difference
mean(df$ROMas.Peter - df$ROMas.Mary, na.rm = TRUE) 
```

In the affected side, the average absolute difference is even larger ($7.78$)
with a maximum absolute difference of 37 degrees,
but the correlation is much higher ($r=0.8516653$). See Figure 5.2 in the book.

Btw, this is an an example for using the correlation coefficient even though
the marginal distributions are not normal: There are much more measurements in the higher
values around 80 than below, say, 60. But the correlation coefficient makes sense
for descriptive purposes.

In this case, knowing Peter's measurement *does* help us to predict 
Mary's measurement (-> exercise later).

### Intraclass Correlation Coefficient (ICC)

One way to measure reliability is to use the intraclass correlation coefficient (ICC).

This measure is based on the idea that observed score $Y_i$ consists of the true score 
(ROM) and a measurement error (for each person).
The proportion of the true score variability to the total variability is the ICC.

In the background one thinks of a statistical model from the 
[Classical Test Theory (CTT)](https://en.wikipedia.org/wiki/Classical_test_theory).
There is 

- a true underlying score $\eta_i$ (for each patient i) and 
- an error term $\varepsilon \sim N(0, \sigma_i)$ which is the difference between the true
score and 
- the observed score $Y_i$.

$$ Y_i = \eta_i + \varepsilon_i $$

It is assumed that $\eta_i$ and $\varepsilon_i$ are independent: ($\mathbb{C}ov(\eta_i, \varepsilon_i)=0$).
This is a nice assumption because now we know (see [here](https://en.wikipedia.org/wiki/Variance#Addition_and_multiplication_by_a_constant)) 
that the variability
of the observed score $Y_i$ is just the sum of the variability of the true score $\eta_i$
and the variability of the error term $\varepsilon_i$:

$$ \mathbb{V}ar(Y_i) = \mathbb{V}ar(\eta_i) + \mathbb{V}ar(\varepsilon_i) $$
$$ \sigma_{Y_i}^2 = \sigma_{\eta_i}^2 + \sigma_{\varepsilon_i}^2 $$

We want most of the variability in our observed scores $Y_i$ to be explained by the 
true but unobservable scores $\eta_i$. The measurement error $\varepsilon_i$ should be
be comparatively small. If it is large, we are mostly measuring noise or at least not
what we want to measure. 

If you either pull two people with the same true but unobservable score $\eta$ out of the population
or measure the same person twice and the score does not change in between, we can 
**define reliability as correlation between these two measurements**:

$$Y_1 = \eta + \varepsilon_1$$
$$Y_2 = \eta + \varepsilon_2$$

$$cor(Y_1, Y_2) = cor(\eta + \varepsilon_1, \eta + \varepsilon_2) = 
\frac{Cov(\eta + \varepsilon_1, \eta + \varepsilon_2)}{\sigma_{Y_1}\sigma_{Y_2}}  = $$

If we use the [properties of the covariance](https://en.wikipedia.org/wiki/Covariance#Properties), 
and the fact that the errors $\varepsilon_1$ and $\varepsilon_2$ are independent, we get:

$$ \frac{Cov(\eta, \eta) + Cov(\eta, \varepsilon_2) + Cov(\varepsilon_1, \eta) + Cov(\varepsilon_1, \varepsilon_2)}{\sigma_{Y_1} \sigma_{Y_2}}  = $$
$$ \frac{\sigma_{\eta}^2 + 0 + 0 + 0}{\sigma_{Y_1} \sigma_{Y_2}}$$

Since $\eta$ is a random variable (we draw a person randomly from the population),
it is well defined to talk about the variance of $\eta$ (i.e., $\sigma_{\eta}^2$).
I think this aspect may not come across in the book quite so clearly.

Furthermore, it does not matter if I call the measurement $Y_1$, $Y_2$ or more gerneral
$Y$, since they have the same variance and true score: 

$$\sigma_{Y} = \sigma_{Y_1} = \sigma_{Y_2}$$

Hence, it follows that:

$$cor(Y_1, Y_2) =  \frac{\sigma_{\eta}^2}{\sigma_{Y}^2} = \frac{\sigma_{\eta}^2}{\sigma_{\eta}^2 + \sigma_{\varepsilon}^2}$$

This is the **intraclass correlation coefficient (ICC)**. It is the proportion of the
true score variability to the total variability. The ICC is a number between 0 and 1 (think about why!).

Depending on how much deviation from the true but unknown $\eta$ we throw into the error term $\varepsilon$,
you get different versions of the ICC. We will probably stick with the simple versions $ICC_{agreement}$
and $ICC_{consistency}$ here and make sure we understand those. 

Let's look again at the term for the ICC above and divide the numerator and the denominator by
$\sigma_{\eta}^2$, which we can do, since it is a positive number:

$$ \frac{\sigma_{\eta}^2}{\sigma_{\eta}^2 + \sigma_{\varepsilon}^2} = 
\frac{1}{1 + \frac{\sigma_{\varepsilon}^2}{\sigma_{\eta}^2}}$$

We could call the term $\frac{\sigma_{\varepsilon}^2}{\sigma_{\eta}^2}$ the noise-to-signal ratio.
The higher this ratio, the lower the ICC. The lower the ratio, the higher the ICC.

- If you increase the noise (measurement error $\sigma_{\varepsilon}^2$) for fixed 
  true score variability $\sigma_{\eta}^2$, the ICC decreases, because the denominator
  increases.
- If you increase the true score variability $\sigma_{\eta}^2$ for fixed noise$\sigma_{\varepsilon}^2$, 
  the ICC increases, since the denominator decreases.

Btw, we could also divide by $\sigma_{\varepsilon}^2$ and get the signal-to-noise ratio.

At first glance, the following statement seems wrong:

In a very **homogeneous population** (patients have very similar scores/measurements),
the **ICC might be very low**. The reason is that the patient variability $\sigma_{\eta}^2$ is low
and you probably have some measurement error $\sigma_{\varepsilon}^2$.
Hence, if you look at the formula, ICC must be low (for a given measurement error).

On the other hand, if you have a very **heterogeneous population** (patients have rather different 
scores/measurements), the **ICC might be very high**.
The reason is that the patient variability $\sigma_{\eta}^2$ is high and you probably 
have some measurement error $\sigma_{\varepsilon}^2$.

**What matters is the ratio of the two**, as can be seen from the formula above.

Let's try to calculate the ICC for our data using a statistical model. There are a couple of different
R packages to do this. We will use the `irr` package.

```{r}
library(irr)
irr::icc(as.matrix(df[, c("ROMas.Peter", "ROMas.Mary")]), 
    model = "oneway", type = "consistency")
```

We get the result: $ICC(1) = 0.851$.

Since we are regression model experts, we would like to see if we can get the result using the Bayesian 
framework. 

Below is the model structure. Explanation of the model:

- $Y_i$ is the observed score (ROM) for patient $ID$.
- $\mu_i$ is the expected value of the observed score for patient $ID$.
- $\sigma_{\varepsilon}$ is the standard deviation of the measurement error.
- $\alpha[ID]$ is the patient-specific intercept. 
  Since every patient has a different intercept, and they
  come from a normal distribution, we have a **random intercepts model**.
- $\alpha_{mean}$ is mean of the prior for the patient-specific intercepts.
- $\sigma_{\alpha}$ is the standard deviation of the patient-specific intercepts.
  This is the patient variability! 
- The ICC is then calculated as 
  $\frac{\sigma_{\alpha}^2}{\sigma_{\alpha}^2 + \sigma_{\varepsilon}^2}$.


\[
\begin{eqnarray*}
Y_i &\sim& N(\mu_i, \sigma_{\varepsilon}) \\
\mu_i &=& \alpha[ID] \\
\alpha[ID] &\sim& \text{Normal}(\alpha_{ID mean}, \sigma_{\alpha}) \\
\alpha_{IDmean} &\sim& \text{Normal}(66, 20) \\
\sigma_{\alpha} &\sim& \text{Uniform}(0,20) \\
\sigma_{\varepsilon} &\sim& \text{Uniform}(0,20)
\end{eqnarray*}
\]

We did not even notice it, but this was our first **multilevel regression model**.
It is multilevel due to the extra layer of patient-specific intercepts.
The observations are obviously clustered within patients, since observations
from the same patient are more similar than observations from different patients.

Draw model structure ... exercise..

This time we fire up the `rethinking` package and use the `ulam` function 
to fit the model.
This uses Markov Chain Monte Carlo (MCMC) to sample from the posterior 
distribution of the parameters.
The `chains` argument specifies how many chains we want to run, and the `cores` argument
specifies how many cores we want to use.

```{r}
library(rethinking)
library(tictoc)

data_ <- df %>% 
  mutate(ID = row_number()) %>%
  dplyr::select(ID,ROMas.Peter, ROMas.Mary) %>% 
  pivot_longer(cols = c(ROMas.Peter, ROMas.Mary), 
               names_to = "Rater", values_to = "ROM") %>% 
  mutate(Rater = factor(Rater))

tic()
m5.1 <- ulam(
  alist(
    # Likelihood
    ROM ~ dnorm(mu, sigma),
    
    # Patient-specific intercepts (random effects)
    mu <- a[ID],  
    a[ID] ~ dnorm(a_bar, sigma_ID),  # Hierarchical structure for patients
    
    # Priors for hyperparameters
    a_bar ~ dnorm(66, 20),  # Population-level mean
    sigma_ID ~ dunif(0,20),  # Between-patient standard deviation
    sigma ~ dunif(0,20)  # Residual standard deviation
  ), 
  data = data_, 
  chains = 8, cores = 4
)
toc() # 7s

precis(m5.1, depth = 2)

post <- extract.samples(m5.1)
var_patients <- mean(post$sigma_ID^2)  # Between-patient variance
var_residual <- mean(post$sigma^2)     # Residual variance
var_patients / (var_patients + var_residual) # ICC
# 0.846
# not too bad; very close to the result from the irr package
```

The trick to do these calculations by "hand" is to get the 
variance decomposition correct.
We stumbled upon variance decomposition in the context of ANOVA,
where we decomposed the total variance into the regression variance
and the residual variance. Here, we decompose the total variance
into the between-patient variance and the residual variance.

We can also estimate a random intercept model with the `lme4` package using
the command `lmer`in the Frequentist framework. No priors.

```{r}
library(lme4)
m5.2 <- lmer(ROM ~ (1|ID), data = data_)
summary(m5.2)
print(VarCorr(m5.2), comp = "Variance")

# ICC = 
270.99 / (270.99 + 47.35) # 
# 0.8512597
# -> exactly the same result as the irr package
```

So far, we have only looked at the **$ICC_{consistency}$** (see also page 106 in the book). 
There, we have not yet explicitely considered a bias (=systematic difference 
between the raters) that the raters could introduce. In the book,
they introduce a bias of 5 degrees (Mary measures 5 degrees more than Peter on average).

There is also the **$ICC_{agreement}$**, which explicitely considers this difference
that could occur between the raters. 

We will now introduce the 5 degree bias and use our Bayesian 
framework to estimate the ICC. By introducing the bias, we should see
a lower ICC (agreement). Note, that the prediction quality of Mary's scores
given Peter's scores should not change, since we would only shift Mary's scores
down by 5 degrees, which would not disturb the linear regression model. We can
always move around the points to where we want them to be. We do that for instance
when we scale or standardize the data.

Anyhow, let's try to give the model equations for the new model considering
the introduced bias:

\[
\begin{eqnarray*}
Y_i &\sim& N(\mu_i, \sigma_{\varepsilon}) \\
\mu_i &=& \alpha[ID] + \beta[Rater] \\
\alpha[ID] &\sim& \text{Normal}(\alpha_{IDmean}, \sigma_{\alpha}) \\
\beta[Rater] &\sim& \text{Normal}(0, \sigma_{\beta}) \\
\alpha_{IDmean} &\sim& \text{Normal}(66, 20) \\
\sigma_{\alpha} &\sim& \text{Uniform}(0,20) \\
\sigma_{\beta} &\sim& \text{Uniform}(0,10) \\
\sigma_{\varepsilon} &\sim& \text{Uniform}(0,20)
\end{eqnarray*}
\]


...


### Difference between correlation and ICC

If we do not introduce a bias in the data, the correlation coefficient
is the same as the ICC (as seen above). On page 110, Figure 5.3, they show nicely 
what the difference is between the correlation coefficient and the ICC. We note:

- The ICC (agreement) measures how tightly the two measurements are 
  clustered around the line of equality ($y=x$)........
  ........

### Standard Error of Measurement (SEM)
...

### Bland-Altman Plot
...

## Validity
...

## TODOS

- mention missing values, missingness mechanisms -> Methodenvertiefung
- Logistic Regression, Poisson, -> Methodenvertiefung
- Exercise: Show by simulation what Gelman talks about with significant p values. So I scan the data
  for significant p values and then simulate data with the same effect size and see how often
  I get significant p values. Especially the next effect would be probably smaller,
  especially, if one did p-hacking! Calculate a priori probability for replication (def?).
- Chapter: Sample size calculations for multivariate regression, Proportions, ICCs, t.test
- Chapter about Reliability, Validity and ICCs (incl. simulation of what an ICC of 0.9 or so means), but maybe reduced
- Angenommen man hat ein masking eines Effekts und der Model fit ist aber gut (keine Voraussetzung verletzt), 
  ist diese Situation möglich?
- What about papers? -> eLearning
- AIC, BIC, cross-validation, Model selection (best subset, leaps....), Variable selection
- More on bias variance tradeoff, show for polynomial regression?
- include eLearning tasks in script.
