# Reliability and Validity

For this chapter we refer to the book 
[Measurement in Medicine](https://www.cambridge.org/core/books/measurement-in-medicine/8BD913A1DA0ECCBA951AC4C1F719BCC5).

I invite you to read the introductory chapters 1 and 2 about concepts,
theories and models, and types of measurement.

In general, when conducting a measurement of any sort 
(laboratory measurements, scores from questionnaires, etc.), 
we want to be reasonably sure

- that we actually **measure what we intend to measure**; 
  ([validity](https://en.wikipedia.org/wiki/Validity_(statistics)); 
  chapter 6 in the book);
- that the measurement does **not change too much** if the 
  underlying **conditions are the same**
  ([reliability](https://en.wikipedia.org/wiki/Reliability_(statistics)); 
  chapter 5 in the book); and
- that we are able to detect a **change** if the underlying conditions change 
  ([responsiveness](https://tinyurl.com/3vdcxy49); chapter 7 in the book); and
- that we understand the meaning of a change in the measurement
  (interpretability; chapter 8 in the book).

In this [video](https://www.youtube.com/watch?v=KuT2n1w0Ixc&ab_channel=Physiotutors), 
Kai jump starts you on reliability and validity.


## Reliability
You can watch this [video](https://www.youtube.com/watch?v=9HSoWaRpcys&ab_channel=Physiotutors) 
to get started.

Imagine, you measure a patient (pick your favorite measurement), for example,
the range of motion (ROM) of the shoulder. 

- If you are interested in how
  similar your measurements are in comparison to your colleagues, you are 
  trying to determine the so-called **inter-rater reliability**.
- If you are interested in how similar your measurements are when you measure
  the same patient twice, you are trying to determine the so-called 
  **intra-rater reliability**.

Assuming there is a true (but unknown) underlying value (of ROM), 
it is clear that measurements will not be *exactly* the same. 
Possible influences (potentially) causing different results are:

- the measurement instrument itself (e.g., the goniometer),
- the patient (e.g., mood/motivation),
- the examiner (e.g., mood, influence on patient),
- the environment (e.g., the room temperature).

Note that the **true score** is defined in our context as the average of all measurements
if we would measure repeat it an infinite number of times.

### Peter and Mary's ROM measurements

The data can be found [here](http://www.clinimetrics.nl/answers-to-the-assignments-in-textbook_22_0.html).
We randomly select 50 measurements from Peter and Mary in 50 different patients,
plot their measurements and annotate the absolutely largest one.

```{r}
library(pacman)
p_load(tidyverse, readxl)

# Read file
url <- "https://raw.githubusercontent.com/jdegenfellner/Script_QM2_ZHAW/main/data/chapter%205_assignment%201_2_wide.xls"
temp_file <- tempfile(fileext = ".xls")
download.file(url, temp_file, mode = "wb")  # mode="wb" is important for binary files
df <- read_excel(temp_file)

head(df)
dim(df)

# As in the book, let's randomly select 50 patients.
set.seed(123)
df <- df %>% sample_n(50)
dim(df)

# "as" = affected shoulder
# "nas" = not affected shoulder

df <- df %>%
  mutate(diff = abs(ROMnas.Peter - ROMnas.Mary))  # Compute absolute difference

max_diff_point <- df %>%
  dplyr::filter(diff == max(diff, na.rm = TRUE))  # Find the row with the max difference

df %>%
ggplot(aes(x = ROMnas.Peter, y = ROMnas.Mary)) +
  geom_point() + 
  geom_point(data = max_diff_point, aes(x = ROMnas.Peter, y = ROMnas.Mary), 
             color = "blue", size = 4) +  # Highlight max difference point
  geom_abline(intercept = 0, slope = 1, color = "red") +
  theme_minimal() +
  ggtitle("ROMnas.Peter vs. ROMnas.Mary") +
  theme(plot.title = element_text(hjust = 0.5)) +
  annotate("text", x = max_diff_point$ROMnas.Peter, 
           y = max_diff_point$ROMnas.Mary, 
           label = paste0("Max Diff: ", round(max_diff_point$diff, 2)), 
           vjust = -1, color = "blue", size = 4)

# average abs. difference:
mean(df$diff, na.rm = TRUE) # 7.2

cor(df$ROMnas.Peter, df$ROMnas.Mary, use = "complete.obs") 
```

The red line represents the line of equality ($y=x$). If the measurements are 
exactly the same,
all points would lie on this line. The blue point represents the largest difference
from the randomly chosen 50 people. Note that in the maximum difference in 
all 155 patients is 35 degrees.

The first simple meause of agreement we could use is the correlation, which
measures the strength and direction of a linear relationship between two variables.
But correlation does not exacly what we want. If there was a bias 
(mary systematically measures 5 degrees more than Peter), correlation would not 
notice this. (-> exercise later...). It actually is too optimistic about 
the agreement since it only cares about the linearity and not about a potential bias.
$r=0.2403213$ which indicates a weak positive correlation. Higher values of Peter's
are associated with higher values of Mary's measurements.
But: Knowing Peter's measurement does not help us to *predict* Mary's measurement
at such a low correlation (-> exercise later).
So, on the *not affected shoulder* (nas), the agreement is really bad.

What about the affected shoulder (as)?

```{r}
library(ggExtra)
df <- df %>%
  mutate(diff = abs(ROMas.Peter - ROMas.Mary))  # Compute absolute difference

max_diff_point <- df %>%
  dplyr::filter(diff == max(diff, na.rm = TRUE))  # Find the row with the max difference

p <- df %>%
ggplot(aes(x = ROMas.Peter, y = ROMas.Mary)) +
  geom_point() + 
  geom_point(data = max_diff_point, aes(x = ROMas.Peter, y = ROMas.Mary), 
             color = "blue", size = 4) +  # Highlight max difference point
  geom_abline(intercept = 0, slope = 1, color = "red") +
  theme_minimal() +
  ggtitle("ROMas.Peter vs. ROMas.Mary") +
  theme(plot.title = element_text(hjust = 0.5)) +
  annotate("text", x = max_diff_point$ROMas.Peter, 
           y = max_diff_point$ROMas.Mary, 
           label = paste0("Max Diff: ", round(max_diff_point$diff, 2)), 
           vjust = -1, color = "blue", size = 4)
# Add marginal histograms
ggMarginal(p, type = "density", fill = "gray", color = "black")

# average abs. difference:
mean(df$diff, na.rm = TRUE) # 7.2
cor(df$ROMas.Peter, df$ROMas.Mary, use = "complete.obs") 

# mean difference
mean(df$ROMas.Peter - df$ROMas.Mary, na.rm = TRUE) 
```

In the affected side, the average absolute difference is even larger ($7.78$)
with a maximum absolute difference of 37 degrees,
but the correlation is much higher ($r=0.8516653$). See Figure 5.2 in the book.

Btw, this is an an example for using the correlation coefficient even though
the marginal distributions are not normal: There are much more measurements in the higher
values around 80 than below, say, 60. But the correlation coefficient makes sense
for descriptive purposes.

In this case, knowing Peter's measurement *does* help us to (peter) predict 
Mary's measurement (-> exercise later).

### Intraclass Correlation Coefficient (ICC)

One way to measure reliability is to use the intraclass correlation coefficient (ICC).

This measure is based on the idea that the true score (ROM) has some variability
in our data set, gut there are also measurement errors with variability.
The proportion of the true score variability to the total variability is the ICC.

In the background one thinks of a statistical model from the 
[Classical Test Theory (CTT)](https://en.wikipedia.org/wiki/Classical_test_theory).
There is 

- a true underlying score $\eta_i$ (for each patient i) and 
- an error term $\varepsilon \sim N(0, \sigma)$ which is the difference between the true
score and 
- the observed score $Y_i$.

$$ Y_i = \eta_i + \varepsilon_i $$

It is assumed that $\eta_i$ and $\varepsilon_i$ are independent.
This is a nice assumption because now we know (see [here](https://en.wikipedia.org/wiki/Variance#Addition_and_multiplication_by_a_constant)) 
that the variability
of the observed score $Y_i$ is just the sum of the variability of the true score $\eta_i$
and the variability of the error term $\varepsilon_i$ ($\mathbb{C}ov(\eta_i, \varepsilon_i)=0$):

$$ \mathbb{V}ar(Y_i) = \mathbb{V}ar(\eta_i) + \mathbb{V}ar(\varepsilon_i) $$

We want most of the variability in our observed scores $Y_i$ to be explained by the 
true but unobservable scores $\eta_i$. The measurement error $\varepsilon_i$ should be
be comparatively small. If it is large, we are mostly measuring noise. 

If you either pull two people with the same true but unobservable score $\eta$ out of the population
or measure the same person twice and the score does not change in between, we can 
**define reliability as correlation between these two measurements**:

$$Y_1 = \eta + \varepsilon_1$$
$$Y_2 = \eta + \varepsilon_2$$

$$cor(Y_1, Y_2) = cor(\eta + \varepsilon_1, \eta + \varepsilon_2) = 
\frac{Cov(\eta + \varepsilon_1, \eta + \varepsilon_2)}{\sigma_{Y_1}\sigma_{Y_2}}  = $$

If we use the [properties of the covariance](https://en.wikipedia.org/wiki/Covariance#Properties), 
and the fact that the errors $\varepsilon_1$ and $\varepsilon_2$ are independent, we get:

$$ \frac{Cov(\eta, \eta) + Cov(\eta, \varepsilon_2) + Cov(\varepsilon_1, \eta) + Cov(\varepsilon_1, \varepsilon_2)}{\sigma_{Y_1} \sigma_{Y_2}}  = $$
$$ \frac{\sigma_{\eta}^2 + 0 + 0 + 0}{\sigma_{Y_1} \sigma_{Y_2}}$$

Since $\eta$ is a random variable (we draw a person randomly from the population),
it is well defined to talk about the variance of $\eta$ ($=\sigma_{\eta}^2$).
Also: $$\sigma_{Y} = \sigma_{Y_1} = \sigma_{Y_2}$$

Hence, it follows that:

$$cor(Y_1, Y_2) =  \frac{\sigma_{\eta}^2}{\sigma_{Y}^2} = \frac{\sigma_{\eta}^2}{\sigma_{\eta}^2 + \sigma_{\varepsilon}^2}$$

This is the **intraclass correlation coefficient (ICC)**. It is the proportion of the
true score variability to the total variability. The ICC is a number between 0 and 1.

Depending on how much deviation from the true but unknown $\eta$ we throw into the error term $\varepsilon$,
you get different versions of the ICC. We will probably stick with the simple versions $ICC_{agreement}$
and $ICC_{consistency}$ here and make sure we understand those. 

Let's look again at the term for the ICC above and divide the numerator and the denominator by
$\sigma_{\eta}^2$, which we can do, since it is a positive number:

$$ \frac{\sigma_{\eta}^2}{\sigma_{\eta}^2 + \sigma_{\varepsilon}^2} = 
\frac{1}{1 + \frac{\sigma_{\varepsilon}^2}{\sigma_{\eta}^2}}$$

We could call the term $\frac{\sigma_{\varepsilon}^2}{\sigma_{\eta}^2}$ the noise-to-signal ratio.
The higher this ratio, the lower the ICC. The lower the ratio, the higher the ICC.

- If you increase the noise (measurement error $\sigma_{\varepsilon}^2$) for fixed 
  true score variability $\sigma_{\eta}^2$, the ICC decreases, because the denominator
  increases.
- If you increase the true score variability $\sigma_{\eta}^2$ for fixed noise$\sigma_{\varepsilon}^2$, 
  the ICC increases, since the denominator decreases.

At first glance, the following statement seems wrong:

In a very **homogeneous population** (patients have very similar scores/measurements),
the **ICC might be very low**. The reason is that the patient variability $\sigma_{\eta}^2$ is low
and you probably have some measurement error $\sigma_{\varepsilon}^2$.
Hence, if you look at the formula, ICC must be low (for a given measurement error).

On the other hand, if you have a very **heterogeneous population** (patients have rather different 
scores/measurements), the **ICC might be very high**.
The reason is that the patient variability $\sigma_{\eta}^2$ is high and you probably 
have some measurement error $\sigma_{\varepsilon}^2$.

**What matters is the ratio of the two**, as can be seen from the formula above.


## Validity
...

## TODOS

- Bayes ICC
- mention missing values, missingness mechanisms -> Methodenvertiefung
- Logistic Regression, Poisson, -> Methodenvertiefung
- Exercise: Show by simulation what Gelman talks about with significant p values. So I scan the data
  for significant p values and then simulate data with the same effect size and see how often
  I get significant p values. Especially the next effect would be probably smaller,
  especially, if one did p-hacking! Calculate a priori probability for replication (def?).
- Chapter: Sample size calculations for multivariate regression, Proportions, ICCs, t.test
- Chapter about Reliability, Validity and ICCs (incl. simulation of what an ICC of 0.9 or so means), but maybe reduced
- Angenommen man hat ein masking eines Effekts und der Model fit ist aber gut (keine Voraussetzung verletzt), 
  ist diese Situation möglich?
- What about papers? -> eLearning
- AIC, BIC, cross-validation, Model selection (best subset, leaps....), Variable selection
- More on bias variance tradeoff, show for polynomial regression?
- include eLearning tasks in script.
